import os


### AMR Analysis ###

def get_database(wildcards):
    if wildcards.geneset == 'vf':
        return 'data/input/databases/'+config['db_vf']
    elif wildcards.geneset == 'amr':
        return 'data/input/databases/'+config['db_card']
    else:
        print(wildcards.geneset + ' invalid')
        assert(False)

rule minimap_gene:
    input:
        fastq = ancient('data/auxiliary/samples/{samplename}/reads.filtered.2.fastq'),
        db_gene = lambda wildcards: get_database(wildcards)
    output:
        'data/auxiliary/mapping/{samplename}/{geneset}/minimapAlignment.bam'
    conda:
        '../envs/minimap2.yaml'
    benchmark:
        'benchmarks/mapping/{samplename}_{geneset}/minimap.txt'
    resources:
        #cluster execution
        cpus = '4',
        gpus = '0',
        mem = '8G',
        walltime = '16:00:00'
    shell:
        'minimap2 -t {resources.cpus} -ax map-ont {input.db_gene} {input.fastq} | samtools view -b -T {input.db_gene} > {output}'


rule gene_detection:
    input:
        alignment = 'data/auxiliary/mapping/{samplename}/{geneset}/minimapAlignment.bam',
        db_gene = lambda wildcards: get_database(wildcards)
    output:
        ide = 'data/auxiliary/mapping/{samplename}/{geneset}/identifiedElements.txt',
        sus = 'data/auxiliary/mapping/{samplename}/{geneset}/surroundingSequences.txt',
        stats = 'data/auxiliary/mapping/{samplename}/{geneset}/stats.txt'
    conda:
        '../envs/gene_detection.yaml'
    benchmark:
        'benchmarks/mapping/{samplename}_{geneset}/detection.txt'
    resources:
        #cluster execution
        cpus = '8',
        gpus = '0',
        mem = '8G',
        walltime = '2:00:00'
    script:
        '../scripts/gene_detection.py'
 
rule gene_coverage:
    input:
        alignment = 'data/auxiliary/mapping/{samplename}/{geneset}/minimapAlignment.bam'
    output:
        coverage = 'data/auxiliary/mapping/{samplename}/{geneset}/coverage.txt'
    conda:
        '../envs/samtools.yaml'
    benchmark:
        'benchmarks/mapping/{samplename}_{geneset}/gene_coverage.txt'
    resources:
        #cluster execution
        cpus = '1',
        gpus = '0',
        mem = '8G',
        walltime = '4:00:00'
    shell:
        'samtools sort {input.alignment} | samtools coverage - > {output.coverage}'
        

rule remap_surrounding_sequences:
    input:
        mmdb = 'data/input/databases/'+config['db_kraken_flat'],
        fastq = 'data/auxiliary/mapping/{samplename}/{geneset}/surroundingSequences.txt'
    output:
        'data/auxiliary/mapping/{samplename}/{geneset}/sus_minimap.sam'
    log:
        'logs/mapping/{samplename}/{geneset}/sus_minimap.txt'
    conda:
        '../envs/minimap2.yaml'
    benchmark:
        'benchmarks/mapping/{samplename}/{geneset}/minimap_sus_mapping.txt'
    resources:
        #cluster execution
        cpus = '8',
        gpus = '0',
        mem = '128G',
        walltime = '16:00:00'
    shell:
        'minimap2 -t {resources.cpus} -ax map-ont {input.mmdb} {input.fastq} -o {output} 2> {log}'



#Returns all patient samples (their respective gene_detection outputs)
def getAllPatientSamples(wildcards):
    ret = []
    for row in samples.itertuples():

        if (row.samplename) in missingSamples:
            continue

        ret.append('data/auxiliary/mapping/'+row.samplename+'/{geneset}/identifiedElements.txt')
        ret.append('data/auxiliary/mapping/'+row.samplename+'/{geneset}/sus_minimap.sam')


    return ret
    
rule gene_dump:
    input:
        getAllPatientSamples
    output:
        'data/output/{geneset}/'+config['project_prefix']+'_fulldump_{geneset}.csv'
    log:
        'logs/mapping/{geneset}/full_dump.txt'
    conda:
        '../envs/pysam.yaml'
    benchmark:
        'benchmarks/mapping/{geneset}/dump.txt'
    resources:
        #cluster execution
        cpus = '1',
        gpus = '0',
        mem = '8G',
        walltime = '2:00:00'
    script:
        '../scripts/gene_aggregate.py'

### Kraken2 ###


rule kraken2:
    input:
        kr2db = ancient(os.path.join('data/input/databases/',config['db_kraken'])),
        fastq = ancient('data/auxiliary/samples/{samplename}/reads.fastq')
    output:
        report = 'data/auxiliary/mapping/{samplename}/kraken2/report.txt',
        stream = 'data/auxiliary/mapping/{samplename}/kraken2/stream.txt'
    conda:
        '../envs/kraken2.yaml'
    benchmark:
        'benchmarks/mapping/{samplename}/kraken_mapping.txt'
    log:
        'logs/mapping/{samplename}/kraken_mapping.txt'
    resources:
        #cluster execution
        cpus = '8',
        gpus = '0',
        mem = '256G',
        walltime = '16:00:00'
    shell:
        '{{ if [ $(stat -c %s {input.fastq}) -ne 0 ] ; then kraken2 --db {input.kr2db} --threads {resources.cpus} --report {output.report} --output {output.stream} {input.fastq}  ; else exit 1 ; fi ; }} 2> {log}'


def getAllKrakens():
    allKrakens = []
    for row in samples.itertuples():
        if (row.samplename) in missingSamples:
            continue
        allKrakens.append('data/auxiliary/mapping/'+row.samplename +'/kraken2/report.txt')
    return allKrakens

rule assembleKrakenDump:
    input:
        getAllKrakens()
    output:
        'data/output/mapping/'+config['project_prefix']+'_KrakenFullDump.csv'
    conda:
        '../envs/biopanda.yaml'
    resources:
        cpus = '1',
        gpus = '0',
        mem = '1G',
        walltime = '1:30:00'
    script:
        "../scripts/combineKrakens.py"


rule buildBrackenDB:
    input:
        kr2db = ancient('data/auxiliary/databases/'+config['db_kraken']+'/updatedDB'),
        dbBuildFlag = 'data/auxiliary/databases/'+config['db_kraken']+'/DB.complete'
    output:
        kmerDistr = 'data/auxiliary/databases/'+config['db_kraken']+'/kmerDistribution.txt'
    conda:
        '../envs/bracken.yaml'
    benchmark:
        'benchmarks/databases/buildBrackenDB.txt'
    log:
        'logs/databases/buildBrackenDB.txt'
    params:
        #cluster execution
        cpus = '8',
        gpus = '0',
        mem = '8G',
        walltime = '4:00:00'
    shell:
            "{{ bracken-build -d {input.kr2db} -t {params.cpus}  -l {config[bracken_read_length]} && \
        mv {input.kr2db}/database{config[bracken_read_length]}mers.kmer_distrib {output.kmerDistr} ; }} 2>{log}"

rule bracken:
    input:
        report = 'data/auxiliary/mapping/{samplename}/kraken2/report.txt',
        kmerDistr = ancient('data/auxiliary/databases/'+config['db_kraken']+'/kmerDistribution.txt')
    output:
        rks='data/output/mapping/{samplename}/bracken/report_kraken_style_{level}.txt',
        rbs='data/output/mapping/{samplename}/bracken/report_bracken_style_{level}.txt'
    conda:
        '../envs/bracken.yaml'
    benchmark:
        'benchmarks/mapping/{samplename}/bracken_mapping_{level}.txt'
    params:
        level = lambda wildcards: wildcards.level 
    log:
        'logs/mapping/{samplename}/bracken_mapping_{level}.txt'
    resources:
        #cluster execution
        cpus = '1',
        gpus = '0',
        mem = '8G',
        walltime = '03:59:00'
    shell:
        'est_abundance.py -i {input.report} -k {input.kmerDistr} -o {output.rbs} -l {params.level} --out-report {output.rks}  2> {log}'


def getAllBrackens(wildcards):
    allBrackens = []
    for row in samples.itertuples():
        if row.samplename in missingSamples:
            continue
        allBrackens.append('data/output/mapping/'+row.samplename+ '/bracken/report_kraken_style_'+wildcards.level+'.txt')
    return allBrackens

rule assembleBrackenDump:
    input:
        getAllBrackens
    output:
        'data/output/mapping/'+config['project_prefix']+'_BrackenFullDump_{level}.csv'
    conda:
        '../envs/biopanda.yaml'
    resources:
        cpus = '1',
        gpus = '0',
        mem = '1G',
        walltime = '1:30:00'
    script:
        "../scripts/combineKrakens.py"

rule blast:
    input:
        ancient('data/auxiliary/samples/{samplename}_{gvhd}/unclassified_reads.fastq')
    output:
        'data/auxiliary/unmappedReads/{samplename}_{gvhd}/blast.txt'
#    singularity:
#        "docker://"+selectedWorkflow.blast_image
    benchmark:
        'benchmarks/blast/{samplename}_{gvhd}.txt'
    log:
        'logs/blast/{samplename}_{gvhd}.txt'
    params:
        #cluster execution
        cpus = '16',
        gpus = '0',
        mem = '256G',
        walltime = '24:00:00'
    shell:
        'blastn '+( '-db data/input/databases/{config[db_blast]}'if config['db_blast_use_remote'] == False else '-remote -db nt' )+' -query {input} -outfmt 6 -out {output} -num_threads {params.cpus} -max_target_seqs 1 2> {log}'

rule createMinimapIndexForMarkerGenes:
    input:
        mmdb = 'data/input/databases/markerGenes.fa'
    output:
        mmdb = 'data/input/databases/markerGenes.mmi'
#    singularity:
#        "docker://" + selectedWorkflow.amr_image
    params:
        # cluster execution
        cpus = '1',
        gpus = '0',
        mem = '196G',
        walltime = '2:00:00'
    shell:
        'minimap2 -d {output} {input}'

rule alignIlluminaSequencedSamplesToMarkerGenes:
    input:
        mmdb = 'data/input/databases/markerGenes.mmi',
        fastq = 'data/auxiliary/samples/{samplename}/reads.filtered.fastq'
    output:
        alignment  = temp('data/auxiliary/mapping/{samplename}/markerGenes/{samplename}.sam'),
        tmp = temp(directory('data/auxiliary/mapping/{samplename}/markerGenes/tmp'))
    log:
        'logs/mapping/{samplename}/markerGenes/alignment.txt'
#    singularity:
#        "docker://"+selectedWorkflow.amr_image
    benchmark:
        'benchmarks/mapping/{samplename}/alignmentVsMarkerGenes.txt'
    resources:
        #cluster execution
        cpus = '8',
        gpus = '0',
        mem = '196G',
        walltime = '16:00:00'
    shell:
        'mkdir {output.tmp} && minimap2 -t {params.cpus} --split-prefix {output.tmp} -ax map-ont {input.mmdb} {input.fastq} -o {output.alignment} 2> {log}'

rule convertMarkerGeneAlignmentToBAM:
    input:
        'data/auxiliary/mapping/{samplename}/markerGenes/{samplename}.sam'
    output:
        'data/auxiliary/mapping/{samplename}/markerGenes/{samplename}.bam'
    log:
        'logs/mapping/{samplename}/markerGenes/alignmentConversion.txt'
    benchmark:
        'benchmarks/mapping/{samplename}/alignmentVsMarkerGenes.txt'
    resources:
        # cluster execution
        cpus = '8',
        gpus = '0',
        mem = '64G',
        walltime = '8:00:00'
    shell:
        'module load sambamba/0.6.6 && sambamba view -f bam -t 8 -S -l 0 -o {output} {input}'
        
        
        
# Additional Verification

rule align_to_crassphage:
    input:
        fastq = 'data/auxiliary/samples/{samplename}/reads.fastq',
        database = 'data/input/databases/'+config['db_crassphage']
    output:
        alignment = 'data/auxiliary/mapping/crassphage/{samplename}.sam'
    conda:
        '../envs/minimap2.yaml'
    log:
        'logs/mapping/crassphage/{samplename}.txt'
    benchmark:
        'benchmarks/mapping/crassphage/{samplename}.txt'
    resources:
        # cluster execution
        cpus = '8',
        gpus = '0',
        mem = '8G',
        walltime = '8:00:00'
    shell:
        'minimap2 -t {resources.cpus} -ax map-ont --secondary=no {input.database} {input.fastq} -o {output.alignment}'
     


        
rule zymo_check:
    input:
        fastq = 'data/auxiliary/samples/{samplename}/reads.fastq',
        database = 'data/input/databases/'+config['db_zymo']
    output:
        alignment = 'data/auxiliary/mapping/zymo/{samplename}.sam'
    conda:
        '../envs/minimap2.yaml'
    log:
        'logs/mapping/zymo/{samplename}.txt'
    benchmark:
        'benchmarks/mapping/zymo/{samplename}.txt'
    resources:
        # cluster execution
        cpus = '8',
        gpus = '0',
        mem = '8G',
        walltime = '8:00:00'
    shell:
        'minimap2 -t {resources.cpus} -ax map-ont --secondary=no {input.database} {input.fastq} -o {output.alignment}'    
        
        

rule process_crassphage_alignment:
    input:
        alignment = 'data/auxiliary/mapping/crassphage/{samplename}.sam',
        stream = 'data/auxiliary/mapping/{samplename}/kraken2/stream.txt'
    output:
        stats = 'data/auxiliary/crassphage/summary_{samplename}.csv',
        migration = 'data/auxiliary/crassphage/migration_{samplename}.csv'
    conda:
        '../envs/pysam.yaml'
    log:
        'logs/eukaryota/crassphage/{samplename}_aggregation.log'
    resources:
        # cluster execution
        cpus = '1',
        gpus = '0',
        mem = lambda wc,input,attempt : str(
            max(
                round(
                    input.size/1e9
                )+1,
                2
            )
        )+'G',
        walltime = '0:30:00'
    script:
        '../scripts/analyze_crassphage_alignments.py'      
        
        
def get_all_crassphage_analyses(wildcards):
    all = []
    for row in samples.itertuples():
        if row.samplename in missingSamples:
            continue
        all.append('data/auxiliary/crassphage/summary_'+row.samplename+ '.csv')
    return all

rule assemble_crassphage_analyses:
    input:
        get_all_crassphage_analyses
    output:
        'data/output/crassphage/'+config['project_prefix']+'_summary.csv'
    conda:
        '../envs/biopanda.yaml'
    resources:
        cpus = '1',
        gpus = '0',
        mem = '8G',
        walltime = '0:30:00'
    script:
        "../scripts/concat_pandas_tables.py"
        
def get_all_crassphage_migrations(wildcards):
    all = []
    for row in samples.itertuples():
        if row.samplename in missingSamples:
            continue
        all.append('data/auxiliary/crassphage/migration_'+row.samplename+ '.csv')
    return all

rule assemble_crassphage_migrations:
    input:
        get_all_crassphage_migrations
    output:
        'data/output/crassphage/migration.csv'
    conda:
        '../envs/biopanda.yaml'
    resources:
        cpus = '1',
        gpus = '0',
        mem = '8G',
        walltime = '0:30:00'
    script:
        "../scripts/concat_pandas_tables.py"
        
def get_all_crassphage_alignments(wildcards):
    all = []
    for row in samples.itertuples():
        if row.samplename in missingSamples:
            continue
        all.append('data/auxiliary/mapping/crassphage/'+row.samplename+ '.sam')
    return all
            
rule assemble_crassphage_mapping_qualities:
    input:
        get_all_crassphage_alignments
    output:
        'data/output/crassphage/mapping.csv'
    log:
        'logs/crassphage/assemble_mapping_quals.log'
    conda:
        '../envs/pysam.yaml'
    resources:
        cpus = '1',
        gpus = '0',
        #mem = lambda wc, input, attempt : str(1+round(max(x.size for x in input)/1e9))+'G',
        mem = '32G',
        walltime = '1:30:00'
    script:
        "../scripts/assemble_mapping_qualities.py"    
        
rule filter_human:
    input:
        reads = 'data/auxiliary/samples/{samplename}/reads.fastq',
        database = 'data/input/databases/'+config['db_human']
    output:
        reads = temp('data/auxiliary/samples/{samplename}/reads.filtered.fastq'),
        stats = 'data/auxiliary/samples/{samplename}/filtering.stats'
    conda:
        '../envs/minimap2.yaml'
    log:
        'logs/mapping/filter_human/{samplename}.txt'
    benchmark:
        'benchmarks/filter_human/{samplename}.txt'
    resources:
        # cluster execution
        cpus = '1',
        gpus = '0',
        mem = '32G',
        walltime = '48:00:00'
    script:
        '../scripts/filter_human.py'   
        
rule filter_human_kraken:
    input:
        reads = 'data/auxiliary/samples/{samplename}/reads.filtered.fastq',
        stream = 'data/auxiliary/mapping/{samplename}/kraken2/stream.txt'
    output:
        reads = 'data/auxiliary/samples/{samplename}/reads.filtered.2.fastq',
        stats = 'data/auxiliary/samples/{samplename}/filtering.2.stats'
    conda:
        '../envs/biopanda.yaml'
    log:
        'logs/mapping/filter_human_2/{samplename}.txt'
    benchmark:
        'benchmarks/filter_human_2/{samplename}.txt'
    resources:
        # cluster execution
        cpus = '1',
        gpus = '0',
        mem = '32G',
        walltime = '48:00:00'
    script:
        '../scripts/filter_human_kraken.py'
