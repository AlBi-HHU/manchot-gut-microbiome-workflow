def get_signal_file(wildcards):
    ret = signals.loc[(wildcards.signalid)]
    return ret

def get_sample_file(wildcards): #This is only for signal files
    ret = samples.loc[wildcards.samplename]
    #If the sample is in fastq mode (not using a signal file) we will return an invalid file so that this path will not get evaluated by snamekae
    if ret['mode'] == 'fastq':
        return 'thisfiledoesnotexist'
    #If no barcode is given we can assume that all content from the signal file belongs to the given sample
    elif ret.barcode == 'None' or ret.barcode == 'none':
        return 'data/auxiliary/signals/'+ret.file+'/combined.fastq'
    else:
        return 'data/auxiliary/signals/'+ret.file+'/barcoding/'+ret.barcode+'.fastq'

def get_barcode(wildcards):
    ret = samples.loc[wildcards.samplename]
    return ret.barcode


checkpoint basecalling:
    input:
        ancient('data/auxiliary/signals/{signalid}/extracted')
    output:
        completionFlag = 'data/auxiliary/signals/{signalid}/basecalling/completionFlag.txt'
    log:
        'logs/signals/{signalid}/basecalling.txt'
    benchmark:
        'benchmarks/signals/{signalid}/basecalling.txt'
    params:
        flowcell = lambda wildcards : get_signal_file(wildcards).flowcell,
        kit = lambda wildcards : get_signal_file(wildcards).kit,
        folder = 'data/auxiliary/signals/{signalid}/basecalling',
        gpu = '1' if config['guppy_use_gpu'] else 0,
        guppy_executable =  './'+config['guppy_bin_folder']+'/guppy_basecaller' if config['guppy_module'] == 'None' else 'guppy_basecaller',
        module_load = 'module load {}'.format(config['guppy_module']) if config['guppy_module'] != 'None' else '' 
    resources:
        cpus = '1' if config['guppy_use_gpu'] else '8' ,
        gpus = '1' if config['guppy_use_gpu'] else '0',
        mem = '64G',
        walltime = '47:00:00'
    shell: #A bit tricky: .zip files are not always setup correctly
        '{{ {params.module_load} && sh scripts/guppyWrapper.sh -r {input} -o {params.folder} -e {params.guppy_executable} -f {params.flowcell} -k {params.kit} -c {resources.cpus} -g {params.gpu} && touch {output.completionFlag} ; }} 2> {log}'



#Combines multiple fastq files produced by basecalling from one signal file into one large fastq file //TODO: Skip this and just work on folder
rule combineFASTQFilesPerSignal:
    input:
        #aggregate_fastq_files
        ancient('data/auxiliary/signals/{signalid}/basecalling/completionFlag.txt')
    output:
        temp('data/auxiliary/signals/{signalid}/combined.fastq')
    #singularity:
    params:
        folder = 'data/auxiliary/signals/{signalid}/basecalling'
    resources:
        cpus = '1',
        mem = '4G',
        gpus = '0',
        walltime = '2:00:00'
    shell:
        'find {params.folder} -name "*.fastq" | xargs -n 100 cat > {output}'

rule combineFastqFilesPerBarcode:
    input:
        #aggregate_barcode_files,
        'data/auxiliary/signals/{signalid}/barcoding/out'
    output:
        'data/auxiliary/signals/{signalid}/barcoding/{barcode}.fastq'
    params:
        barcodestring = lambda wildcards: str(wildcards.barcode).zfill(2)  # Fill with padding to ensure that barcode 1 is represented as 01 to match Guppy output
    resources:
        cpus = '1',
        mem = '4G',
        gpus = '0',
        walltime = '0:30:00',
    shell:
        'find {input}/barcode{params.barcodestring} -name "*.fastq" | xargs -n 100 cat > {output}'

checkpoint demultiplex: #TODO: if problems with incomplete files on cluster persist attempt flag solution
    input:
        ancient('data/auxiliary/signals/{signalid}/basecalling/completionFlag.txt')
    output:
        folder = directory('data/auxiliary/signals/{signalid}/barcoding/out')
    log:
        'logs/demux/demultiplex_{signalid}.txt'
    benchmark:
        'benchmarks/demux/demultiplex_{signalid}.txt'
    params:
        indir = 'data/auxiliary/signals/{signalid}/basecalling/pass',
        barcodekit = lambda wildcards: get_signal_file(wildcards)['barcodekit'],
        guppy_executable = './'+config['guppy_bin_folder']+'/guppy_barcoder' if config['guppy_module'] == 'None' else 'guppy_barcoder',
        module_load = 'module load {}'.format(config['guppy_module']) if config['guppy_module'] != 'None' else ''
    resources:
        #cluster execution
        cpus = '4',
        gpus = '0',
        mem = '128G',
        walltime = '72:00:00'
    shell:
        "{{ {params.module_load} && {params.guppy_executable} -i {params.indir} -s {output.folder} -t {resources.cpus} --barcode_kits {params.barcodekit} --verbose_logs  ; }} 2> {log}"

rule linkReads: #For signal pathway
    input:
        get_sample_file
    output:
        temp('data/auxiliary/samples/{samplename}/reads_with_adapter.fastq')
    resources:
        cpus = '1',
        gpus = '0',
        mem = '128M',
        walltime = '0:00:10'
    shell:
        "ln {input} {output}"
        
def get_read_folder_fastq_mode(wildcards):
    return barcode_notation_mode[wildcards.samplename]
        
#Aggregation for already basecalled files (fastq mode)
rule get_read_folder_fastq:
    input:
         get_read_folder_fastq_mode
    output:
        temp('data/auxiliary/samples/{samplename}/reads_with_adapter.fastq')
    resources:
        cpus = '1',
        gpus = '0',
        mem = '4G',
        walltime = '1:30:00'
    shell:
        'zcat -f {input} > {output}'
        
#Adapter Trimming
rule porechop:
    input:
        'data/auxiliary/samples/{samplename}/reads_with_adapter.fastq'
    output:
        'data/auxiliary/samples/{samplename}/reads.fastq'
    conda:
        '../envs/porechop.yaml'
    benchmark:
        'benchmarks/porechop/{samplename}.txt'
    log:
        'logs/porechop/{samplename}.txt'
    resources:
        #cluster execution
        cpus = '8',
        gpus = '0',
        mem = lambda wc,input,attempt : str(
            round(
                input.size/1e9
            )*8+8 #excessive memory consumption by porechop  see https://github.com/rrwick/Porechop/issues/77         
                )+'G',
        walltime = '24:00:00'        
    shell:
        'porechop --threads {resources.cpus} -i {input} -o {output}'
