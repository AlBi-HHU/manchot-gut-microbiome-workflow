{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages / imports\n",
    "\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import math\n",
    "import altair as alt\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as colors\n",
    "import glob\n",
    "from skbio.stats import ordination\n",
    "from scipy.stats import mannwhitneyu\n",
    "from functools import lru_cache,reduce\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "os.makedirs('Output',exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Definitions / Helper Functions](#defines)\n",
    "* [Load Data](#loaddata)\n",
    "* [Normalization Function](#normalize)\n",
    "* Analysis\n",
    "    * [Top Level Statistics](#toplevel)\n",
    "    * [P-Values Top Level Statistics](#pvalues)\n",
    "    * [Overview Top Taxa per Level](#toptaxa)\n",
    "    * [Unmapped and Human Fraction](#human)\n",
    "    * [AMR Gene Counts](#amr)\n",
    "    * [Taxa Counts (Diversity)](#taxacount)\n",
    "    * [Barplots Compositions](#barplots)\n",
    "    * [Patient Journeys](#barplots_time)\n",
    "    * [Bray-Curtis Based PCoA](#pcoa)\n",
    "    * [Sampling Times Overview](#sampletimes)\n",
    "    * [Zymo Std Analysis](#zymo)\n",
    "    * [Eukaryotes Analysis](#eukaryotes)\n",
    "    * [Crassphage Analysis](#crassphage)\n",
    "    * [Population Level Analysis / Illumina](#strains)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions / Helper Functions <a class=\"anchor\" id=\"defines\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePrePostPairs(table : pd.DataFrame, patientcolumn: str = 'patientid' , timecolumn: str ='time' ):\n",
    "    result = []\n",
    "    counts_of_common_occurences = table.groupby([patientcolumn,timecolumn]).size().reset_index()\n",
    "    for patient in counts_of_common_occurences[patientcolumn].unique():\n",
    "        subtable = counts_of_common_occurences[counts_of_common_occurences[patientcolumn] == patient]\n",
    "        for time1,time2 in it.combinations(subtable[timecolumn].unique(),2):\n",
    "            if time1 < 0 and time2 > 0:\n",
    "                result.append((patient,time1,time2))\n",
    "            elif time1 > 0 and time2 < 0:\n",
    "                result.append((patient,time2,time1))\n",
    "    return result\n",
    "\n",
    "\n",
    "#Helper Function that returns a preceding timepoint (if one exists) for each row in a table\n",
    "def find_previous_sample(table,row):\n",
    "    #print (row ['PatID'])\n",
    "    timepoints_patient = list(table[table['PatID']==row['PatID']]['time'].unique())\n",
    "    timepoints_filtered = [timepoint for timepoint in timepoints_patient if timepoint < row['time']]\n",
    "    if len(timepoints_filtered) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return sorted(timepoints_filtered)[-1]\n",
    "\n",
    "#Helper function that recalculates some relative days and relabels some patients\n",
    "def adjust_table(df):\n",
    "\n",
    "    df.loc[(df['patientid'] == '999') & (df['time'] == 0), 'patientid'] = 'G1'\n",
    "    df.loc[df['patientid'] == 'G1', 'time'] = -100\n",
    "    df.loc[df['patientid'] == '132', 'patientid'] = '45'\n",
    "    df.loc[df['patientid'] == '133', 'patientid'] = '14'\n",
    "    df.loc[df['patientid'] == '134', 'patientid'] = '46'\n",
    "\n",
    "def sort_samples(samples):\n",
    "    return sorted(\n",
    "        samples,\n",
    "        key=lambda x : ('G' in x,\n",
    "                        int(x.split('/')[0].replace('G','').split('.')[0]),\n",
    "                        int(x.split('/')[0].replace('G','').split('.')[-1]),\n",
    "                        int(x.split('/')[1]))\n",
    "    )\n",
    "\n",
    "#Constants, frequently used\n",
    "CHORDATA = '7711'\n",
    "FUNGI = '4751'\n",
    "VIRUSES = '10239'\n",
    "BACTERIA = '2'\n",
    "ARCHAEA = '2157'\n",
    "EUKARYOTA = '2759'\n",
    "PROTOZOA = '1891100'\n",
    "OFFSET_PAT_18 = 161\n",
    "\n",
    "PAPER_SAMPLES = ['G1/-100','G2/-100','G3/-100','G4/-100','G5/-100','G6/-100',\n",
    "             'G7/-100','G8/-100','G9/-100','G10/-100','G11/-100',\n",
    "             '4/-7','7/-6','11/-2','12/-6','14/-7','15/-6','16/-5','17/-7','19/-2',\n",
    "             '21/-9','24/-10','28/-5','29/-5','32/-5','34/-6','38/-6','39/-6','42/-8',\n",
    "             '13/-7','18/-3','22/-4','23/-9','25/-62','37/-5','40/-1','41/-6','20/-4',\n",
    "             '26/-3','31/-6','33/-2','36/-6',\n",
    "             '4/1','11/0','15/6','16/2','17/9','17/13','24/0','24/9','28/1','29/1','34/10','39/1','42/6',\n",
    "             '18/13','22/0','23/12','25/1','37/1','41/14','26/12','31/6','33/7',\n",
    "             '15/16','28/21','29/15','34/18','38/15','39/15','42/18','42/30',\n",
    "             '25/15','37/17','37/22','40/17','26/27',\n",
    "             '12/34','24/49','28/63','34/41',\n",
    "             '18/91','21/50','22/91','23/61','25/31','31/58','33/47','36/38','36/63',\n",
    "             '11/182','12/342','14/558','15/104','15/189','16/163','16/171','17/154',\n",
    "             '19/153','24/127','28/237','29/186','39/115',\n",
    "             '13/298','18.2/-9','18.2/16','18.2/71','21/120','23/130','37/138','26/185','31/178']\n",
    "\n",
    "\n",
    "HEALTHY_SAMPLES = ['G1/-100','G2/-100','G3/-100','G4/-100','G5/-100','G6/-100',\n",
    "             'G7/-100','G8/-100','G9/-100','G10/-100','G11/-100']\n",
    "\n",
    "ZYMO_SAMPLES_NEW = ['Zymo_Zymo/-100','Zymo_EZ1/-100','Zymo_Power/-100','Zymo_Pro/-100']+['Stool_Power_11/-100',\n",
    " 'Stool_Power_21/-100',\n",
    " 'Stool_Pro_11/-100',\n",
    " 'Stool_Pro_21/-100',\n",
    " 'Stool_Zymo_11/-100',\n",
    " 'Stool_Zymo_21/-100',\n",
    " 'Stool_EZ1_12/-100',\n",
    " 'Stool_EZ1_21/-100']\n",
    "\n",
    "LIFELINES = ['LL81_E07_7627/-777', 'LL91_A04_8564/-777',\n",
    "       'LL93_H07_8785/-777', 'LL87_A09_8215/-777', 'LL80_A10_7551/-777',\n",
    "       'LL47_D03_4330/-777', 'LL63_B04_5872/-777', 'LL83_F03_7788/-777',\n",
    "       'LL66_C11_6217/-777', 'LL80_D01_7482/-777', 'LL92_D01_8637/-777',\n",
    "       'LL59_F11_5548/-777', 'LL83_G11_7853/-777', 'LL72_C02_6721/-777',\n",
    "       'LL84_D03_7882/-777', 'LL45_F06_4164/-777', 'LL89_E11_8427/-777',\n",
    "       'LL60_B04_5584/-777', 'LL45_F11_4204/-777', 'LL70_E06_6539/-777'\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "MARKER_SPECIES = {\n",
    "\n",
    "    'Mucindegrading Bacteria':[\n",
    "        'Akkermansia muciniphila',\n",
    "        'Prevotella spp.'\n",
    "    ],\n",
    "    'Mucosaprotective':[\n",
    "        'Akkermansia muciniphila',\n",
    "        'Faecalibacterium prausnitzii'        \n",
    "    ],\n",
    "\n",
    "    'LPS associated':[\n",
    "        'Citrobacter spp.',\n",
    "        'Enterobacter spp.',\n",
    "        'Escherichia spp.',\n",
    "        'Klebsiella spp.',\n",
    "        'Providencia spp.',\n",
    "        'Pseudomonas spp.',\n",
    "        'Serratia spp.',\n",
    "        'Sutterella spp.'     \n",
    "    ],\n",
    "\n",
    "    'Immunomodulation':[\n",
    "        'Escherichia spp.',\n",
    "        'Enterococcus spp.'        \n",
    "    ],\n",
    "\n",
    "    'Clostridiaceae':[\n",
    "        'Clostridium spp.'      \n",
    "    ],\n",
    "\n",
    "    'Fungi':[\n",
    "        'Candida',\n",
    "        'Geotrichum',\n",
    "        'Saccharomyces cerevisiae'        \n",
    "    ],\n",
    "    \n",
    "    'Blautia':[\n",
    "        'Blautia'\n",
    "    ]\n",
    "\n",
    "}\n",
    "\n",
    "MARKER_SPECIES_REDUCED = [\n",
    "    'Faecalibacterium prausnitzii',\n",
    "    'Akkermansia muciniphila',\n",
    "    'Alistipes finegoldii',\n",
    "    'Saccharomyces cerevisiae'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data <a class=\"anchor\" id=\"loaddata\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kraken2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This data can be collected from the snakemake workflow\n",
    "kraken_dataframe = pd.read_csv('Input/KrakenFullDump.csv',usecols=[1,2,3,4,5,6],dtype={'patientid' : str,'taxonid':str})\n",
    "adjust_table(kraken_dataframe)\n",
    "kraken_dataframe = pd.concat([kraken_dataframe,\n",
    "pd.read_csv('Input/KrakenLifelines.csv',usecols=[1,2,3,4,5,6],dtype={'patientid' : str,'taxonid':str}\n",
    "            )]\n",
    "                            \n",
    "                            )\n",
    "kraken_dataframe['sample'] = kraken_dataframe['patientid']+'/'+kraken_dataframe['time'].astype(str) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMR Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can be extracted from the snakemake workflow, used to normalize the AMR read counts\n",
    "sequencing_stats = pd.read_csv('Input/sampleStats.filtered.csv',dtype={'patientid' : str})\n",
    "\n",
    "sequencing_stats['sample'] = sequencing_stats['patientid']+'/'+sequencing_stats['time'].astype(str)\n",
    "\n",
    "adjust_table(sequencing_stats)\n",
    "sequencing_stats = sequencing_stats[sequencing_stats['sample'].isin(PAPER_SAMPLES)]\n",
    "\n",
    "readcount_min = sequencing_stats['nReads'].min()\n",
    "\n",
    "sequencing_stats = sequencing_stats.set_index(['sample'])\n",
    "\n",
    "def calc_fraction(x):\n",
    "    if x['sample'] in sequencing_stats.index:\n",
    "        return x['readcount'] / sequencing_stats.loc[x['sample']]['nReads']\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "#can be extracted from the snakemake workflow\n",
    "amr = pd.read_csv('Input/fulldump_amr_170322.csv',dtype={'Patient' : str}).rename(\n",
    "columns={'Patient' : 'patientid','Time' : 'time'})\n",
    "\n",
    "amr['sample'] = amr['patientid'].astype(str)+'/'+amr['time'].astype(str)\n",
    "amr['readcount'] = amr.apply(\n",
    "    lambda x : 1/x['Ambiguous Assignments'] if x['Ambiguous Assignments'] != 0 else 1,axis=1\n",
    ")\n",
    "adjust_table(amr)\n",
    "amr = amr[amr['sample'].isin(PAPER_SAMPLES)]\n",
    "\n",
    "amr['Gene Pro Read'] =amr.apply(lambda x : calc_fraction(x) ,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kraken2/Metamaps Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namesToIDs = {\n",
    "    'unclassified' : -1,\n",
    "    'Unassigned at Level' : '-1337'\n",
    "}\n",
    "\n",
    "with open('Input/Taxonomy/names.dmp','r') as f:\n",
    "    for l in f.read().splitlines():\n",
    "        d=[x.strip() for x in l.split('|')]\n",
    "        if d[3] == 'scientific name':\n",
    "            namesToIDs[d[1]] = d[0]\n",
    "with open('Input/Taxonomy/names_additional.dmp','r') as f:\n",
    "    for l in f.read().splitlines():\n",
    "        d=[x.strip() for x in l.split('|')]\n",
    "        if d[3] == 'scientific name':\n",
    "            namesToIDs[d[1]] = d[0]\n",
    "\n",
    "taxonomy = {}\n",
    "\n",
    "\n",
    "levels = {}\n",
    "\n",
    "with open('Input/Taxonomy/nodes.dmp','r') as f:\n",
    "    for l in f.read().splitlines():\n",
    "        d=[x.strip() for x in l.split('|')]\n",
    "        taxonomy[d[0]] = d[1]\n",
    "        levels[d[0]] = d[2]\n",
    "        \n",
    "with open('Input/Taxonomy/nodes_additional.dmp','r') as f:\n",
    "    for l in f.read().splitlines():\n",
    "        d=[x.strip() for x in l.split('|')]\n",
    "        taxonomy[d[0]] = d[1]        \n",
    "        levels[d[0]] = d[2]\n",
    "\n",
    "# ID to names\n",
    "idtonames = {\n",
    "        v : k \n",
    "        for k, v in namesToIDs.items()\n",
    "}\n",
    "\n",
    "idtonames['46506']  = 'Bacteroides stercoris'\n",
    "idtonames['820']  = 'Bacteroides uniformis'\n",
    "idtonames['4335590'] = 'Bacteroides Vulgatus'\n",
    "idtonames['1118061'] = 'Alistipes communis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zymo Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zymo_theory = pd.read_csv('Input/zymo_theory.csv',\n",
    "                          header=None,\n",
    "                          names=['Taxon','Read Fraction']\n",
    "                         )\n",
    "\n",
    "zymo_theory_taxa = zymo_theory['Taxon'].tolist()\n",
    "zymo_theory['Read Fraction (%)'] = zymo_theory['Read Fraction']/100\n",
    "zymo_theory['Sample ID'] = 'Zymo Theoretical Composition'\n",
    "\n",
    "zymo_minimap = pd.read_csv('Input/zymo_minimap_verification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metamaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metamaps_dataframe = pd.read_csv(\n",
    "    'Input/fulldumpMetamaps_2021-08-19.csv',\n",
    "    usecols=[1,2,3,4,5],\n",
    "    dtype={'patientid' : str}\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sample Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PatID is specified as String (Text) so IDs like 18.2 don't get confused as decimal numbers\n",
    "\n",
    "#Both tables contain annotations regarding the individual samples\n",
    "sample_characteristics = pd.read_excel('Input/Annotations/Probencharakteristiken_030122.xlsx',dtype={'PatID' : str})\n",
    "sample_statistics = pd.read_excel('Input/Annotations/Probenstatistiken_010322.xlsx',dtype={'PatID' : str})\n",
    "\n",
    "#Subset of clinical data containing the leukocyte values\n",
    "leukocytes = pd.read_csv('Input/Annotations/KI_Patienten_Nur_Leukos.csv',sep=';')\n",
    "leukocytes.loc[\n",
    "    ((leukocytes['KI ID']==18)&(leukocytes['relatives_datum']>92)),'KI ID'\n",
    "] = '18.2'\n",
    "leukocytes.loc[(leukocytes['KI ID']=='18.2'),'relatives_datum'] -= OFFSET_PAT_18\n",
    "\n",
    "#Outcomes\n",
    "outcomes = pd.read_excel('Input/Annotations/Patient_Statistics (2).xlsx',dtype={'Pat ID' : str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization Function <a class=\"anchor\" id=\"normalize\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    level, #No default value here to avoid accidental mistakes!\n",
    "    samples = None,\n",
    "    random_seed = (4+8+15+16+23+42),\n",
    "    normalize=True,\n",
    "    excluded_taxa_filter = None, #Can be a list of taxa\n",
    "    included_taxa_filter = None, #Only one taxa, becomes new root\n",
    "):\n",
    "    \n",
    "    ### Helper Functions for Filtering\n",
    "    \n",
    "    not_found_taxa = set()\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def is_not_below(x,y):\n",
    "        parent_node = taxonomy[x]\n",
    "        if parent_node == y: #Parent was the node we looked for\n",
    "            return False\n",
    "        elif parent_node == x: #This is only the case at the root node\n",
    "            return True\n",
    "        else: #We need to keep looking\n",
    "            return is_not_below(parent_node,y)\n",
    "\n",
    "\n",
    "    def filter_function(row,taxon):\n",
    "        if row['taxonid'] == '0': #Root Node\n",
    "            return False\n",
    "        if row['taxonid'] not in taxonomy:\n",
    "            #print('Warning: TaxID {} is not in the taxonomy, this is (potentially) bad!'.format(row['taxonid']))\n",
    "            return False\n",
    "        if row['taxonid'] == taxon: #This is the taxon itself, remove\n",
    "            return False\n",
    "        #Otherwise we will check if the taxon is below our target taxon in the taxonomy\n",
    "        return is_not_below(row['taxonid'],taxon)\n",
    "\n",
    "\n",
    "    ###\n",
    "    \n",
    "    #We begin by assuming the raw kraken dataframe as input\n",
    "    working_table = kraken_dataframe\n",
    "    \n",
    "    #####################\n",
    "    #   SELECT SAMPLES\n",
    "    #\n",
    "    #####################    \n",
    "    \n",
    "    #Filter to target samples\n",
    "    if samples != None:\n",
    "        working_table = working_table[working_table['sample'].isin(samples)]    \n",
    "    \n",
    "\n",
    "    #####################\n",
    "    #   CALCULATE UNASSIGNED READS\n",
    "    #\n",
    "    #####################    \n",
    "    \n",
    "    #Determine root readcounts\n",
    "    root_readcounts_kr = None\n",
    "    \n",
    "    if included_taxa_filter != None:\n",
    "        root_readcounts_kr = working_table[\n",
    "            working_table['taxonid']==included_taxa_filter\n",
    "        ].groupby(\n",
    "            ['sample']\n",
    "        )['readcount'].sum()    \n",
    "    else: #If we don't filter we take all reads -> R\n",
    "        root_readcounts_kr = working_table[\n",
    "            working_table['level']=='R'\n",
    "        ].groupby(\n",
    "            ['sample']\n",
    "        )['readcount'].sum()\n",
    "        \n",
    "    readcounts_at_level=None\n",
    "    \n",
    "    if included_taxa_filter != None:\n",
    "        include = working_table[\n",
    "            working_table['level'] == level\n",
    "        ].apply(lambda x : filter_function(x,included_taxa_filter),axis=1)\n",
    "        \n",
    "        readcounts_at_level = working_table[\n",
    "            working_table['level'] == level\n",
    "        ][~include]\n",
    "        \n",
    "        readcounts_at_level = readcounts_at_level.groupby(\n",
    "            ['sample']\n",
    "        )['readcount'].sum()\n",
    "    else:\n",
    "        #Determine total read counts at level\n",
    "        readcounts_at_level = working_table[\n",
    "            working_table['level'] == level\n",
    "        ].groupby(\n",
    "            ['sample']\n",
    "        )['readcount'].sum()\n",
    "    \n",
    "    #Add unassigned at level\n",
    "    unassigned_entries = []\n",
    "    for sample in readcounts_at_level.keys():\n",
    "        \n",
    "        patientid,time = sample.split('/')\n",
    "        time = int(time)\n",
    "\n",
    "        difference = root_readcounts_kr[sample]-readcounts_at_level[sample]\n",
    "        \n",
    "        addon_table = pd.DataFrame([\n",
    "            (difference,patientid,time,'Unassigned at Level','-1337',level,sample)\n",
    "        ],columns=[\n",
    "            'readcount','patientid','time','taxon','taxonid','level','sample'\n",
    "        ])\n",
    "        \n",
    "        unassigned_entries.append(addon_table)\n",
    "                \n",
    "    #Combine into one table\n",
    "    if len(unassigned_entries) != 0:\n",
    "        unassigned_table = pd.concat(unassigned_entries)\n",
    "        working_table = pd.concat([unassigned_table,working_table])\n",
    "    \n",
    "    #####################\n",
    "    #   DOWNSAMPLING\n",
    "    #\n",
    "    #####################\n",
    "    \n",
    "    #Filter to target level\n",
    "    downsampled_table = working_table[working_table['level'] == level]  \n",
    "    \n",
    "    #Filter for taxon if required\n",
    "    if included_taxa_filter != None:\n",
    "        print('Reducing composition to subtree below taxon: {}'.format(idtonames[included_taxa_filter]))\n",
    "        include = downsampled_table.apply(lambda x : filter_function(x,included_taxa_filter),axis=1)\n",
    "        downsampled_table = downsampled_table[~include]\n",
    "           \n",
    "    if normalize:\n",
    "    \n",
    "        # Identify lowest read count\n",
    "        readAnzahlen = downsampled_table.groupby('sample')['readcount'].sum()\n",
    "        minimaleReadAnzahl = readAnzahlen.min()\n",
    "        print('The minimal read count across all samples is [Taxonomic Level {}]: {}'.format(level,minimaleReadAnzahl))\n",
    "\n",
    "        frames = []\n",
    "\n",
    "        # Draw new counts for each sample\n",
    "        for probe in downsampled_table['sample'].unique():\n",
    "\n",
    "            sample_table = downsampled_table[downsampled_table['sample'] == probe]\n",
    "            sample = sample_table.sample(\n",
    "                n=round(minimaleReadAnzahl),\n",
    "                random_state=random_seed,\n",
    "                weights='readcount',\n",
    "                replace=True\n",
    "            )\n",
    "\n",
    "            sample = sample.groupby([\n",
    "                'taxon',\n",
    "                'taxonid',\n",
    "                'sample',\n",
    "                'patientid',\n",
    "                'time',\n",
    "                'level'\n",
    "            ],as_index=False).count()\n",
    "\n",
    "            frames.append(sample)\n",
    "\n",
    "        #Overwrite table with downsampled entries\n",
    "        downsampled_table = pd.concat(frames)   \n",
    "    \n",
    "    #####################\n",
    "    #   FILTERING II\n",
    "    #\n",
    "    #####################\n",
    "    \n",
    "    unassigned_downsampled_table = downsampled_table[downsampled_table['taxonid'] == '-1337']\n",
    "    assigned_downsampled_table = downsampled_table[downsampled_table['taxonid'] != '-1337']\n",
    "    \n",
    "    \n",
    "\n",
    "    if excluded_taxa_filter != None:\n",
    "        for taxon in excluded_taxa_filter:\n",
    "            include = assigned_downsampled_table.apply(lambda x : filter_function(x,taxon),axis=1)\n",
    "            assigned_downsampled_table = assigned_downsampled_table[include]\n",
    "\n",
    "        \n",
    "    downsampled_table = pd.concat([unassigned_downsampled_table,assigned_downsampled_table])\n",
    "        \n",
    "        \n",
    "\n",
    "    return downsampled_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Level Statistics Visualization <a class=\"anchor\" id=\"toplevel\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Gruppen=['Bacteria', 'Fungi','Other eukaryota','Viruses','Archaea']\n",
    "z = sample_statistics[['leukocytephase_cluster_2_kurz','Bacteria_Prozent pro classified ohne human','Fungi_Prozent pro classified ohne human','Other eukaryota_Prozent pro classified ohne human','Viruses_Prozent pro classified ohne human','Archaea_Prozent pro classified ohne human','Probe']]\n",
    "z = z.melt(id_vars=['Probe','leukocytephase_cluster_2_kurz'])\n",
    "z[['domain','variable']] = z['variable'].str.split('_',expand=True)\n",
    "z = z.drop(columns=['variable'])\n",
    "z = z.pivot(index=['Probe','leukocytephase_cluster_2_kurz'],columns='domain',values='value')\n",
    "z = z.reset_index()\n",
    "z = z.melt(id_vars=['Probe','leukocytephase_cluster_2_kurz'])\n",
    "z['domain'].unique()\n",
    "Tabelle_pre=sample_statistics[sample_statistics['leukocytephase_cluster_2_kurz'].isin(['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'])]\n",
    "c1 = alt.Chart(sample_statistics,width=100,height=100).mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'leukocytephase_cluster_2_kurz:O',\n",
    "        header=alt.Header(labels=False),\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None\n",
    "    ),\n",
    "    y=alt.Y('Mikrogramm DNA pro g Stuhl ', axis=alt.Axis(grid=False,minExtent=40), title='DNA [µg] Per g Stool'),\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "c2 = alt.Chart(sample_statistics,width=100,height=100).mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'leukocytephase_cluster_2_kurz:O',\n",
    "        header=alt.Header(labels=False),\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None\n",
    "    ),\n",
    "     y=alt.Y('Total reads pro DNA Library:Q',scale=alt.Scale(type='log'), axis=alt.Axis(grid=False,tickCount=10, format='.1e',minExtent=40),title='Total Reads')\n",
    "    )\n",
    "\n",
    "c2_2 = alt.Chart(sample_statistics,width=100,height=100).mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'leukocytephase_cluster_2_kurz:O',\n",
    "        header=alt.Header(labels=False),\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None\n",
    "    ),\n",
    "     y=alt.Y('median length alle reads:Q',scale=alt.Scale(type='log'), axis=alt.Axis(grid=False,tickCount=10, format='.1e',minExtent=40),title='Median Readlength')\n",
    "    )\n",
    "\n",
    "\n",
    "c3 = alt.Chart(sample_statistics,width=100,height=100).mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'leukocytephase_cluster_2_kurz:O',\n",
    "        header=alt.Header(labels=False),\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None\n",
    "    ),\n",
    "    y=alt.Y('Unclassified_Prozent pro total reads', axis=alt.Axis(grid=False,minExtent=40), title='Unclassified [% Reads]')\n",
    "    ) \n",
    "\n",
    "c4 = alt.Chart(sample_statistics,width=100,height=100).mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'leukocytephase_cluster_2_kurz:O',\n",
    "        header=alt.Header(labels=False),\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None\n",
    "    ),\n",
    "    y=alt.Y('Human_Prozent pro total reads', axis=alt.Axis(grid=False,minExtent=40), title='Human [% Reads]')\n",
    "    )\n",
    "\n",
    "c5 = alt.Chart(sample_statistics,width=100,height=100).mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'leukocytephase_cluster_2_kurz:O',\n",
    "        header=alt.Header(labels=False),\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None\n",
    "    ),\n",
    "    y=alt.Y('G_False', axis=alt.Axis(grid=False,minExtent=40), title='No. Genera')\n",
    "    )\n",
    "\n",
    "c6 =alt.Chart(Tabelle_pre,width=100,height=100).mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'leukocytephase_cluster_2_kurz:O',\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None,\n",
    "        header=alt.Header(labels=False)\n",
    "    ),\n",
    "    y=alt.Y('ARG Reads pro Eingabe Reads *10000:Q', title=['ARG-carrying Reads','Per 10,000 Reads'],scale=alt.Scale(type='symlog'), axis=alt.Axis(grid=False,minExtent=40))\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "c7 = alt.Chart(z,width=100).mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'leukocytephase_cluster_2_kurz:O',\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None,\n",
    "        header=alt.Header(labelOrient='bottom', labelPadding=313)\n",
    "    ),\n",
    "     x=alt.X(\"domain:O\", title=None, axis=alt.Axis(labels=False, ticks=False), scale=alt.Scale(paddingInner=1), sort=Gruppen),    \n",
    "    y=alt.Y(\"value:Q\",title='Fraction [%]',scale=alt.Scale(type='symlog',domain=[0,100]), axis=alt.Axis(grid=False,minExtent=40)), \n",
    "    color=alt.Color(\"domain:N\",sort=Gruppen,legend=alt.Legend(title=None,orient='top'),\n",
    "                    scale=alt.Scale(domain=Gruppen,range=['#EE6677', '#228833', '#CCBB44', '#66CCEE', '#AA3377'])),\n",
    "\n",
    ")\n",
    "\n",
    "chart1=(c1&c2&c2_2&c3&c4&c5&c6&c7).configure_tick(thickness=2)\n",
    "\n",
    "chart1.save(\n",
    "    'Output/Composition/High_Level_Metrics.html')\n",
    "\n",
    "chart1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Level Statistics P-Values <a class=\"anchor\" id=\"pvalues\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_column = 'Chordata_Prozent pro classified'\n",
    "group_column = 'Startcluster'\n",
    "\n",
    "tuples_a = []\n",
    "for group1,group2 in it.combinations(sample_statistics[group_column].unique(),2):\n",
    "    result = mannwhitneyu(\n",
    "        sample_statistics[sample_statistics[group_column] == group1][test_column],\n",
    "        sample_statistics[sample_statistics[group_column] == group2][test_column]  \n",
    "    )\n",
    "    tuples_a.append((group1,group2,result.pvalue))\n",
    "    \n",
    "df_a = pd.DataFrame(tuples_a,columns=['Group A','Group B','Whitney'])\n",
    "df_a.to_csv('Output/Whitney_{}_{}.csv'.format(test_column,group_column))\n",
    "df_a\n",
    "\n",
    "tuples_b = []\n",
    "for group in sample_statistics[group_column].unique():\n",
    "    tuples_b.append((\n",
    "        group,\n",
    "        sample_statistics[sample_statistics[group_column] == group][test_column].mean(),\n",
    "        sample_statistics[sample_statistics[group_column] == group][test_column].std(),\n",
    "        sample_statistics[sample_statistics[group_column] == group][test_column].min(),\n",
    "        sample_statistics[sample_statistics[group_column] == group][test_column].max()\n",
    "        \n",
    "    ))\n",
    "df_b = pd.DataFrame(tuples_b,columns=['Group','Mean','Standardabweichung','Minimum','Maximum'])\n",
    "df_b.to_csv('Output/MeanAndStd_{}_{}.csv'.format(test_column,group_column))\n",
    "df_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview Top Taxa per Taxonomic Level <a class=\"anchor\" id=\"toptaxa\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TOP_X = 20\n",
    "LEVEL = 'S'\n",
    "SAMPLES = PAPER_SAMPLES\n",
    "NORMALIZE = True\n",
    "EXCLUDE = None\n",
    "INCLUDE = None\n",
    "\n",
    "#################\n",
    "\n",
    "\n",
    "hash_samples = hash(str(SAMPLES))\n",
    "\n",
    "df_filtered = get_normalized_abundances(kraken_dataframe,level=LEVEL,included_taxa_filter=INCLUDE,excluded_taxa_filter=EXCLUDE,samples=SAMPLES,normalize=NORMALIZE)\n",
    "df_filtered['Read Fraction'] = df_filtered['readcount'] / df_filtered.groupby(['time','patientid'])['readcount'].transform('sum')\n",
    "\n",
    "taxa_we_look_at = list(df_filtered.groupby('taxon')['Read Fraction'].mean().sort_values(ascending=False)[:(TOP_X+1)].keys())\n",
    "df_filtered = df_filtered[df_filtered['taxon'].isin(taxa_we_look_at)]\n",
    "\n",
    "df_filtered = df_filtered.pivot(index=['patientid','time'],values=['Read Fraction'],columns=['taxon']).fillna(0)\n",
    "\n",
    "df_filtered.to_csv('Output/Top_{}_Taxa_{}_{}_{}-Without_{}.csv'.format(TOP_X,LEVEL,hash_samples,'Normalized' if NORMALIZE else 'Raw',str(EXCLUDE)))\n",
    "\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Unmapped and Human Fractions <a class=\"anchor\" id=\"human\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmapped = kraken_dataframe[kraken_dataframe['level'] == 'U'].set_index(['patientid','time'])['readcount'].rename('Unmapped')\n",
    "mapped = kraken_dataframe[kraken_dataframe['level'] == 'R'].set_index(['patientid','time'])['readcount'].rename('Mapped')\n",
    "\n",
    "chordata = get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    level='P',\n",
    "    included_taxa_filter=CHORDATA,\n",
    "    normalize=False,\n",
    "    samples=None\n",
    ")\n",
    "\n",
    "chordata = chordata[chordata['taxon'] == 'Chordata'].set_index(['patientid','time'])['readcount'].rename('Chordata')\n",
    "\n",
    "combined = pd.concat([unmapped,mapped,chordata],axis=1).fillna(0)\n",
    "combined['Other Classified'] = combined['Mapped']-combined['Chordata']\n",
    "\n",
    "combined = combined[['Unmapped','Chordata','Other Classified']]\n",
    "combined = combined.div(combined.sum(axis=1), axis=0)\n",
    "\n",
    "combined = combined.reset_index()\n",
    "\n",
    "combined.to_csv('Output/Unmapped_Mapped_Chordata_Other.csv')\n",
    "\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['sample'] = combined['patientid']+'/'+combined['time'].astype(str)\n",
    "\n",
    "data = combined[['sample','Unmapped','Chordata','Other Classified']].melt(id_vars=['sample'])\n",
    "\n",
    "alt.Chart(data).mark_bar().encode(\n",
    "    x='sample',\n",
    "    y='value',\n",
    "    color='variable',\n",
    "    tooltip=['sample','value','variable']\n",
    ").save('Output/Unmapped_Chordata_Other.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMR Gene Counts Calculations <a class=\"anchor\" id=\"amr\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_per_read=amr.groupby(['patientid','time'])['readcount'].sum()\n",
    "\n",
    "gene_per_read.to_excel('Output/gene_per_read.xlsx',index=False)\n",
    "\n",
    "gene_per_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxa Counts (Diversity) Per Level <a class=\"anchor\" id=\"taxacount\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES = PAPER_SAMPLES\n",
    "EXCLUDE = [CHORDATA]\n",
    "############################\n",
    "\n",
    "os.makedirs('Output/Diversity',exist_ok=True)\n",
    "\n",
    "charts = []\n",
    "\n",
    "for taxonomic_level in ['S','G','P']:\n",
    "    table = get_normalized_abundances(\n",
    "        kraken_dataframe,\n",
    "        level=taxonomic_level,\n",
    "        excluded_taxa_filter=EXCLUDE,\n",
    "        samples=SAMPLES\n",
    "    )\n",
    "    \n",
    "    table = table[table['taxonid']!='-1337']\n",
    "     \n",
    "    taxaCounts = table.groupby(['patientid','time'],as_index=False)['taxon'].count().rename(\n",
    "        columns={'taxon' : '{}_{}'.format(taxonomic_level,'Normalized')}\n",
    "    )\n",
    "    \n",
    "    charts.append(taxaCounts)\n",
    "\n",
    "\n",
    "reduce(lambda x,y : pd.merge(x,y,how='left',on=['patientid','time']),charts).to_csv(\n",
    "    'Output/Diversity/taxa_counts_{}-Without_{}_Normalized.csv'.format(\n",
    "        hash(str(SAMPLES)),\n",
    "        str(EXCLUDE)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "charts = []\n",
    "\n",
    "\n",
    "for taxonomic_level in ['S','G','P']:\n",
    "    table = get_normalized_abundances(\n",
    "        kraken_dataframe,\n",
    "        level=taxonomic_level,excluded_taxa_filter=EXCLUDE,normalize=False)\n",
    "    \n",
    "    table = table[table['taxonid']!='-1337']\n",
    "    \n",
    "    taxaCounts = table.groupby(['patientid','time'],as_index=False)['taxon'].count().rename(\n",
    "        columns={'taxon' : '{}_{}'.format(taxonomic_level,'Raw')}\n",
    "    )\n",
    "    \n",
    "    charts.append(taxaCounts)\n",
    "        \n",
    "reduce(lambda x,y : pd.merge(x,y,how='left',on=['patientid','time']),charts).to_csv(\n",
    "    'Output/Diversity/taxa_counts_{}-Without_{}_RAW.csv'.format(\n",
    "        hash(str(SAMPLES)),\n",
    "        str(EXCLUDE)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barplots Compositions <a class=\"anchor\" id=\"barplots\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_X = 20\n",
    "LEVEL = 'S'\n",
    "#Filters\n",
    "INCLUDE = None\n",
    "EXCLUDE = [CHORDATA]\n",
    "\n",
    "SAMPLES = PAPER_SAMPLES\n",
    "\n",
    "RELATIVE = True # If False, abundances are relative to root\n",
    "\n",
    "OPTIONAL_ANNOTATIONS = None\n",
    "\n",
    "SORTING = 'Sample ID' #Default='sample'\n",
    "\n",
    "GROUPING = None#'Startcluster' # None if you don't want any Grouping, otherwise a column to group by\n",
    "\n",
    "NORMALIZE = True\n",
    "\n",
    "DISCARD_CUTOFF = 100\n",
    "#################\n",
    "\n",
    "os.makedirs('Output/Composition',exist_ok=True)\n",
    "\n",
    "input_table = get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    level=LEVEL,\n",
    "    samples=SAMPLES,\n",
    "    included_taxa_filter=INCLUDE,\n",
    "    excluded_taxa_filter=EXCLUDE,\n",
    "    normalize=NORMALIZE\n",
    ")\n",
    "\n",
    "\n",
    "if RELATIVE:\n",
    "    input_table['Read Fraction (%)'] = input_table['readcount'] / input_table.groupby(['time','patientid'])['readcount'].transform('sum')\n",
    "else:\n",
    "    root_levels = kraken_dataframe[kraken_dataframe['level'] == 'R']\n",
    "    root_levels = root_levels[root_levels['sample'].isin(input_table['sample'].unique())][['sample','readcount']].rename(columns={'readcount':'Reads At Root'})\n",
    "    input_table = pd.merge(input_table,root_levels,how='left',on='sample')\n",
    "    input_table['Read Fraction (%)'] = input_table['readcount'] / input_table['Reads At Root']\n",
    "    input_table = input_table.drop(columns=['Reads At Root'])\n",
    "    \n",
    "# determine the top taxa based on means\n",
    "taxa_we_look_at = list(input_table.groupby('taxon')['Read Fraction (%)'].mean().sort_values(ascending=False)[:(TOP_X+1)].keys())+['Unassigned at Level']\n",
    "print('Determined the following taxa as relevant:',taxa_we_look_at)\n",
    "\n",
    "total_reads = input_table.groupby('sample',as_index=False)['readcount'].sum()\n",
    "masked_samples = total_reads[total_reads['readcount'] < DISCARD_CUTOFF]['sample']\n",
    "\n",
    "\n",
    "#assign everything else to the \"other\" group and readjust sum\n",
    "input_table.loc[~input_table['taxon'].isin(taxa_we_look_at), 'taxon'] = 'Other'\n",
    "input_table.loc[input_table['sample'].isin(masked_samples), 'taxon'] = 'Not enough reads'\n",
    "\n",
    "input_table = input_table.groupby(['taxon','time','patientid'],as_index=False).sum()\n",
    "\n",
    "input_table['id'] = (\n",
    "    input_table['patientid'].astype(str)+'/'+input_table['time'].astype(str)\n",
    ")\n",
    "\n",
    "input_table['other'] = input_table['taxon'] == 'Other'\n",
    "\n",
    "input_table = input_table.rename(columns={\n",
    "    'id' : 'Sample ID',\n",
    "    'taxon' : 'Taxon'\n",
    "})\n",
    "\n",
    "colorMap = {}\n",
    "\n",
    "taxa = input_table['Taxon'].unique()\n",
    "\n",
    "palette = sns.color_palette(\"tab20\",n_colors=len(taxa)) #hls\n",
    "\n",
    "for tax,col in zip(list(taxa),palette):\n",
    "    colorMap[tax] = col #colors.to_hex(col)\n",
    "    \n",
    "altdomain = []\n",
    "altrange = []\n",
    "\n",
    "for x in input_table['Taxon'].unique():\n",
    "    \n",
    "    if x == 'Other' or x == 'Unassigned at Level' or x == 'Not enough reads':\n",
    "        continue\n",
    "\n",
    "    c = colorMap[x]\n",
    "    altdomain.append(x)\n",
    "    color = colors.to_hex(c)\n",
    "    altrange.append(color)\n",
    "    \n",
    "altdomain.append('Other')\n",
    "altrange.append(colors.to_hex((1,1,1)))\n",
    "altdomain.append('Unassigned at Level')\n",
    "altrange.append(colors.to_hex((0.15,0.15,0.15)))\n",
    "altdomain.append('Not enough reads')\n",
    "altrange.append(colors.to_hex((0.55,0.55,0.55)))\n",
    "\n",
    "input_table = pd.merge(input_table,sample_statistics,how='left',left_on=['patientid','time'],right_on=['PatID','time'])\n",
    "\n",
    "input_table['Aktuelles Cluster'] = input_table['Aktuelles Cluster'].fillna('Unassigned')\n",
    "\n",
    "charts = []\n",
    "\n",
    "groups = input_table[GROUPING].unique() if GROUPING != None else [None]\n",
    "\n",
    "for group in groups:\n",
    "    \n",
    "    grouptable = None\n",
    "    if group != None:\n",
    "        #Reduce to required columns to keep output reasonably small\n",
    "        grouptable = input_table[input_table['Aktuelles Cluster'] == group]\n",
    "    else:\n",
    "        grouptable = input_table\n",
    "    \n",
    "    patientlist_sorted = sorted(\n",
    "        grouptable[SORTING].unique().tolist()\n",
    "    )\n",
    "    \n",
    "    chart =  alt.Chart(\n",
    "          grouptable,title=str(group)\n",
    "      ).transform_calculate(\n",
    "      order=f\"-indexof({altdomain}, datum.Taxon)\"\n",
    "        ).mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "            x=alt.X('{}:N'.format(SORTING),sort=patientlist_sorted),\n",
    "            y=alt.Y('Read Fraction (%):Q',scale=alt.Scale(\n",
    "                domain=(0,1))\n",
    "                   ),\n",
    "            color=alt.Color('Taxon:N',legend=alt.Legend(columns=1,symbolLimit=0,labelLimit=0),scale=alt.Scale(domain=altdomain,range=altrange)),\n",
    "            tooltip=['readcount','Read Fraction (%)','Taxon'],\n",
    "            order=alt.Order('order:Q')\n",
    "      )\n",
    "    \n",
    "    if OPTIONAL_ANNOTATIONS != None:\n",
    "        for optional_annotation in OPTIONAL_ANNOTATIONS:\n",
    "            chart &= alt.Chart(grouptable,title=str(group)).mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "                x=alt.X('{}:N'.format(SORTING),sort=patientlist_sorted),\n",
    "                y=alt.Y('{}'.format(optional_annotation))\n",
    "            )\n",
    "\n",
    "    charts.append(\n",
    "        chart\n",
    "         \n",
    "        \n",
    "    )\n",
    "\n",
    "chart = reduce(lambda x,y : x&y, charts).configure_axis(\n",
    "    labelFontSize=16, titleFontSize=16\n",
    ").configure_title(fontSize=20).configure_legend(titleFontSize=20, labelFontSize=16)\n",
    "\n",
    "chart.save(\n",
    "    'Output/Composition/Barplots-Top_{}-{}-Only_{}-Without_{}-{}-{}_{}_{}_{}.html'.format(\n",
    "        TOP_X,\n",
    "        LEVEL,\n",
    "        INCLUDE,\n",
    "        str(EXCLUDE),\n",
    "        hash(str(SAMPLES)),\n",
    "        'Relative' if RELATIVE else 'Absolute',\n",
    "        hash(str(OPTIONAL_ANNOTATIONS)),\n",
    "        SORTING,\n",
    "        GROUPING\n",
    "    )\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_table_pivot = input_table.pivot(index=['time','patientid'],columns='Taxon',values='Read Fraction (%)')\n",
    "\n",
    "input_table_pivot.transpose()\n",
    "\n",
    "input_table_pivot=input_table_pivot.fillna(0)\n",
    "\n",
    "input_table_pivot.to_csv(\n",
    "    'Output/Composition/Compositions-Top_{}-{}-Only_{}-Without_{}-{}-{}_{}_{}_{}.csv'.format(\n",
    "        TOP_X,\n",
    "        LEVEL,\n",
    "        INCLUDE,\n",
    "        str(EXCLUDE),\n",
    "        hash(str(SAMPLES)),\n",
    "        'Relative' if RELATIVE else 'Absolute',\n",
    "        hash(str(OPTIONAL_ANNOTATIONS)),\n",
    "        SORTING,\n",
    "        GROUPING\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient Journeys <a class=\"anchor\" id=\"barplots_time\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_X = 20\n",
    "\n",
    "os.makedirs('Output/Patient_Journey',exist_ok=True)\n",
    "\n",
    "### Prepare Annotations\n",
    "\n",
    "sample_statistics['leukocytephase_cluster_2_kurz_kuerzer']=sample_statistics[\n",
    "    'leukocytephase_cluster_2_kurz'\n",
    "].replace('Leukozytopenia','N').replace('Pre TX','P').replace('Reconstitution','R')\n",
    "\n",
    "outcomes_reduced = outcomes[\n",
    "    ['Pat ID',\n",
    "     'Day relative to HSCT',\n",
    "     'Day relative to HSCT.1',\n",
    "     'Day relative to HSCT.2',\n",
    "     'Day relative to HSCT.3']\n",
    "         ].rename(\n",
    "    columns={\n",
    "        'Day relative to HSCT' : 'Relapse',\n",
    "        'Day relative to HSCT.1' : 'Acute GvHD Grade 1-2',\n",
    "        'Day relative to HSCT.2' : 'Acute GvHD Grade 3-4',\n",
    "        'Day relative to HSCT.3' : 'Death',\n",
    "    }\n",
    ")\n",
    "outcomes_reduced['No Adverse Event']=outcomes_reduced.apply( lambda x : 0 if x.count() <= 1 else np.NaN,axis=1)\n",
    "outcomes_reduced\n",
    "\n",
    "outcomes_reduced = outcomes_reduced.melt(id_vars=['Pat ID'])\n",
    "outcomes_reduced = outcomes_reduced[(outcomes_reduced['value']==outcomes_reduced['value'])]\n",
    "\n",
    "outcomes_reduced.loc[\n",
    "    (outcomes_reduced['Pat ID']=='18.2'),'KI ID'\n",
    "] = '18.2'\n",
    "outcomes_reduced.loc[(outcomes_reduced['Pat ID']=='18.2'),'value'] -= OFFSET_PAT_18\n",
    "\n",
    "#Load Marker Gene Data\n",
    "\n",
    "print('Load Marker Gene Data')\n",
    "\n",
    "specframe = kraken_dataframe[kraken_dataframe['level'] == 'S']\n",
    "\n",
    "specframe=specframe[specframe['sample'].isin(PAPER_SAMPLES)] \n",
    "specframe['estimated abundance'] = (specframe['readcount'] / specframe.groupby(['time','patientid'])['readcount'].transform('sum')).round(10)\n",
    "\n",
    "marker_species = {}\n",
    "\n",
    "for group in MARKER_SPECIES:\n",
    "    marker_species[group] = specframe[\n",
    "        specframe['taxon'].str.startswith(tuple(MARKER_SPECIES[group]))\n",
    "    ]\n",
    "    marker_species[group]['time2'] = marker_species[group]['time'] + 1\n",
    "\n",
    "charts = []\n",
    "\n",
    "#### Shannon Block\n",
    "shannon_df = get_normalized_abundances(kraken_dataframe,samples=PAPER_SAMPLES,level=LEVEL,excluded_taxa_filter=[CHORDATA],normalize=True)\n",
    "def shannon(values):\n",
    "    max_shannon = math.log(len(values))\n",
    "    shannon = -sum(x*math.log(x) for x in values)\n",
    "    shannon_quotient = shannon/max_shannon\n",
    "    cols = ['Shannon Index','Shannon Max Value','Shannon Quotient']\n",
    "    return pd.Series((shannon,max_shannon,shannon_quotient),index=cols)\n",
    "\n",
    "shannon_df['Read Fraction'] = shannon_df['readcount']/shannon_df.groupby(['patientid','time'])['readcount'].transform('sum')\n",
    "shannon_df = shannon_df.groupby('sample',as_index=False)['Read Fraction'].apply(func= lambda x : shannon(x))\n",
    "shannon_df['time'] = shannon_df['sample'].str.split('/',expand=True)[1]\n",
    "\n",
    "for idx,row in sample_statistics.iterrows():\n",
    "    \n",
    "        \n",
    "    patient = row['PatID']\n",
    "    samples = [x for x in PAPER_SAMPLES if x.split('/')[0] == str(patient)]\n",
    "   \n",
    "    cluster = row['leukocytephase_cluster_kurz'].split('_')[1]\n",
    "    \n",
    "    compcharts = []\n",
    "    \n",
    "    for taxonomic_level in ['P','G','S']:\n",
    "        \n",
    "        for filter_option in [CHORDATA,FUNGI,VIRUSES]:\n",
    "            if filter_option == CHORDATA:\n",
    "                input_table = get_normalized_abundances(kraken_dataframe,level=taxonomic_level,excluded_taxa_filter=[CHORDATA],samples=samples)\n",
    "            else:\n",
    "                input_table = get_normalized_abundances(kraken_dataframe,level=taxonomic_level,included_taxa_filter=filter_option,samples=samples)\n",
    "\n",
    "            #input_table = input_table[input_table['time'].between(ZEIT_CUTOFF_1,ZEIT_CUTOFF_2)]\n",
    "\n",
    "            input_table['readAnteil'] = input_table['readcount'] / input_table.groupby(['time','patientid'])['readcount'].transform('sum')\n",
    "\n",
    "\n",
    "            input_table = pd.merge(input_table,sample_statistics,how='left',left_on=['patientid','time'],right_on=['PatID','time'])\n",
    "\n",
    "            input_table = pd.merge(input_table,sample_characteristics,how='left',left_on=['patientid','time'],right_on=['PatID','time'])\n",
    "\n",
    "            input_table['Startcluster'] = input_table['leukocytephase_cluster_kurz_x'].apply(lambda x: x.split('_')[1])\n",
    "\n",
    "            input_table = input_table.rename(columns={\n",
    "                'readAnteil' : 'Read Fraction (%)',\n",
    "                'taxon' : 'Taxon'\n",
    "            })\n",
    "\n",
    "            input_table['Leukozyten*1000proÂµL_Abnahme']=input_table['Leukozyten*1000proÂµL_Abnahme'].apply(lambda x : 0 if isinstance(x,str) else x)\n",
    "            input_table['readlength kbp']=input_table['median length alle reads']/1000\n",
    "\n",
    "            input_table['timeplus1'] = input_table['time']+1\n",
    "\n",
    "\n",
    "            le_tableaux = input_table\n",
    "            le_tableaux = le_tableaux[le_tableaux['patientid'] == patient]\n",
    "            taxa_we_look_at = le_tableaux.groupby('Taxon')[\n",
    "                'Read Fraction (%)'\n",
    "            ].mean().sort_values(ascending=False)[:(TOP_X+1)].keys()\n",
    "            taxa_we_look_at = list(taxa_we_look_at) + ['Unassigned at Level']\n",
    "            #print(taxa_we_look_at)\n",
    "            le_tableaux.loc[~le_tableaux['Taxon'].isin(taxa_we_look_at), 'Taxon'] = 'Other'\n",
    "            le_tableaux = le_tableaux.groupby(['Taxon','time','patientid'],as_index=False).sum()\n",
    "        \n",
    "            #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "            colorMap = {}\n",
    "\n",
    "            taxa = le_tableaux['Taxon'].unique()\n",
    "\n",
    "            palette = sns.color_palette(\"tab20\",n_colors=len(taxa)) #hls\n",
    "\n",
    "            for tax,col in zip(list(taxa),palette):\n",
    "                colorMap[tax] = col #colors.to_hex(col)\n",
    "\n",
    "            altdomain = []\n",
    "            altrange = []\n",
    "\n",
    "            for x in le_tableaux['Taxon'].unique():\n",
    "\n",
    "                if x == 'Other' or x == 'Unassigned at Level':\n",
    "                    continue\n",
    "\n",
    "                c = colorMap[x]\n",
    "                altdomain.append(x)\n",
    "                color = colors.to_hex(c)\n",
    "                altrange.append(color)\n",
    "\n",
    "            altdomain.append('Other')\n",
    "            altrange.append(colors.to_hex((1,1,1)))\n",
    "            altdomain.append('Unassigned at Level')\n",
    "            altrange.append(colors.to_hex((0,0,0)))\n",
    "            #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "            compchart = alt.Chart(le_tableaux,width=700,height=130,title=[\n",
    "                 '{}'.format(\n",
    "                {'P':'Phylum','S':'Species','G':'Genus'}[taxonomic_level]),{\n",
    "                    FUNGI : 'Only Fungi',\n",
    "                    VIRUSES : 'Only Viruses',\n",
    "                    CHORDATA : 'Total (Filtered for Human)'\n",
    "                }[filter_option]]\n",
    "                    ).transform_calculate(\n",
    "                      order=f\"-indexof({altdomain}, datum.Taxon)\"\n",
    "                    ).mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "                            x=alt.X('time:Q',title=None,axis=alt.Axis(tickMinStep=1)),\n",
    "                            y=alt.Y('Read Fraction (%):Q',title=['Read Fraction (%)',''],\n",
    "                                    scale=alt.Scale(domain=(0,1)),stack=True),\n",
    "                            order = 'order:Q',\n",
    "                            color=alt.Color('Taxon:N', legend=alt.Legend(columns=4,\n",
    "            orient='top',\n",
    "            legendX=0, legendY=-140,\n",
    "            direction='horizontal',\n",
    "            titleAnchor='middle'),scale=alt.Scale(domain=altdomain,range=altrange)),\n",
    "                            tooltip=['readcount','Read Fraction (%)','patientid','Taxon','time'],\n",
    "                        )\n",
    "            compcharts.append(compchart)\n",
    "        \n",
    "    patientsub = sample_statistics[sample_statistics['PatID']== patient]\n",
    "\n",
    "    phasechart = (alt.Chart(sample_statistics[sample_statistics['PatID']== patient],width=700,title='Leukocyte Phase (Pre-TX / Nadir / Reconstitution)').mark_text(fontStyle='bold').encode(\n",
    "                    x=alt.X('time:Q',axis=None),\n",
    "                    text='leukocytephase_cluster_2_kurz_kuerzer:N'\n",
    "                ))\n",
    "    \n",
    "    mscharts = []\n",
    "    for group in marker_species:\n",
    "        ms_group_table = marker_species[group]\n",
    "        ms_group_table['primary identifier'] = ms_group_table['taxon'].apply(lambda x: x.split(' ')[0])\n",
    "        ms_group_table = ms_group_table.groupby(['primary identifier','patientid','time','time2'],as_index=False)['estimated abundance','readcount'].sum()\n",
    "\n",
    "        c =alt.Chart(ms_group_table[ms_group_table['patientid'] == patient],title='Marker Species: '+group).mark_point(shape='square',filled=True).encode(\n",
    "            x=alt.X('time:Q',title=None),\n",
    "            x2=alt.X2('time2:Q'),\n",
    "            y=alt.Y('primary identifier'),\n",
    "            color=alt.Color('estimated abundance',title='Estimated Abundance (Not Normalized)'),\n",
    "            tooltip=['estimated abundance','readcount']\n",
    "        )\n",
    "        mscharts.append(c)\n",
    "    \n",
    "    patientoutcomes= outcomes_reduced[outcomes_reduced['Pat ID'] == patient]\n",
    "    outcome_set = list(set(patientoutcomes['value'])-{'?'})\n",
    "    last_outcome=max(outcome_set) if len(outcome_set)>0 else -999\n",
    "    last_leuko=max(patientsub['time'].max()+5,last_outcome)\n",
    "\n",
    "\n",
    "    leukosub = leukocytes[leukocytes['relatives_datum'].between(patientsub['time'].min()-5,last_leuko)]\n",
    "    leukosub = leukosub[leukosub['KI ID'].astype(str) == patient]\n",
    "    leukochart= alt.Chart(leukosub,height=130,width=700).mark_line(point=True).encode(\n",
    "        x=alt.X('relatives_datum:Q',axis=alt.Axis(grid=False,tickMinStep=1),title='Day'),\n",
    "        y=alt.Y('Messwert_Zahl',title=['Leukocytes','(1000/µL)']),\n",
    "        order='relatives_datum'\n",
    "    )+alt.Chart(patientsub).mark_rule().encode(\n",
    "        x='time'\n",
    "    )\n",
    "    \n",
    "    #### OUTCOMES\n",
    "\n",
    "    outcomechart = None\n",
    "    if 'No Adverse Event' in patientoutcomes['variable'].unique():\n",
    "        outcomechart = alt.Chart({'values':[{}]},width=700).mark_text(\n",
    "        align=\"left\", baseline=\"top\",fontStyle='bold',\n",
    "                fontSize=12\n",
    "        ).encode(\n",
    "            x=alt.value(0),  # pixels from left\n",
    "            y=alt.value(0),  # pixels from top\n",
    "            text=alt.value('No adverse event recorded!')\n",
    "        )\n",
    "    else:\n",
    "        if len(patientoutcomes[patientoutcomes['value'] != '?']) > 0:\n",
    "            outcomechart = alt.Chart(patientoutcomes,height=100,width=700).mark_point().encode(\n",
    "                x=alt.X('value:Q',axis=alt.Axis(grid=False,tickMinStep=1),title=None),\n",
    "                y=alt.Y('variable',title=['Outcome'])\n",
    "            )\n",
    "        if '?' in patientoutcomes['value'].unique():\n",
    "            agvhd_section = alt.Chart({'values':[{}]},width=700).mark_text(\n",
    "                align=\"left\", baseline=\"top\",fontStyle='bold',\n",
    "                    fontSize=12\n",
    "            ).encode(\n",
    "                x=alt.value(0),  # pixels from left\n",
    "                y=alt.value(0),  # pixels from top\n",
    "                text=alt.value('Acute GvHD recorded without specified timepoint!')\n",
    "            )\n",
    "            if outcomechart == None:\n",
    "                outcomechart = agvhd_section\n",
    "            else:\n",
    "                outcomechart &= agvhd_section\n",
    "    if outcomechart == None:\n",
    "        outcomechart = alt.Chart({'values':[{}]},width=700).mark_text(\n",
    "        align=\"left\", baseline=\"top\",fontStyle='bold',\n",
    "                fontSize=12\n",
    "        ).encode(\n",
    "            x=alt.value(0),  # pixels from left\n",
    "            y=alt.value(0),  # pixels from top\n",
    "            text=alt.value('Control Group')\n",
    "        )        \n",
    "    outcomechart = outcomechart.properties(title='Recorded Outcomes')\n",
    "    \n",
    "    #### Outliers\n",
    "    \n",
    "    outliertable = patientsub[['Outlier High Unclassified', 'Outlier High Human', 'Outlier High ARG Reads',\n",
    "                               'Outlier Low Bacteria', 'Outlier High Fungi', 'Outlier High Other Eukaryota',\n",
    "                               'Outlier High Virus', 'Outlier High Archae', 'Outlier High DNA per g Stool',\n",
    "                               'Outlier High Total Reads', 'Outlier High Median Reads Length', 'time']].melt(id_vars = 'time')\n",
    "    outlierchart = alt.Chart(outliertable[outliertable['value']=='y']).mark_point().encode(\n",
    "        x = 'time',\n",
    "        y = 'variable')\n",
    "    \n",
    "    #### Mikrogramm DNA \n",
    "    \n",
    "    \n",
    "    dnachart= alt.Chart(patientsub,height=100,width=700).mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "        x=alt.X('time:Q',axis=alt.Axis(grid=False,tickMinStep=1),title=None),\n",
    "        y=alt.Y('Mikrogramm DNA pro g Stuhl ',title=['DNA','(µg/g Stool)'],scale=alt.Scale(type='log'),axis=alt.Axis(tickCount=10, format=\".1e\")),\n",
    "        order='time',\n",
    "        tooltip=['Mikrogramm DNA pro g Stuhl ']\n",
    "    )\n",
    "\n",
    "\n",
    "    libchart = alt.Chart(patientsub,height=100,width=700).mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "        x=alt.X('time:Q',axis=alt.Axis(grid=False,tickMinStep=1),title=None),\n",
    "        y=alt.Y('Total Reads',title=['Total Reads'],scale=alt.Scale(type='log'),axis=alt.Axis(tickCount=10, format=\".1e\")),\n",
    "        order='time',\n",
    "        tooltip=['Total Reads']\n",
    "    )\n",
    "\n",
    "    divchart = alt.Chart(patientsub,height=100,width=700,title='Diversity Analysis based on normalized read fractions').mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "        x=alt.X('time:Q',axis=alt.Axis(grid=False,tickMinStep=1),title=None),\n",
    "        y=alt.Y('S_True',title=['Species Count','(Normalized, No Human)']),\n",
    "        order='time'\n",
    "    )\n",
    "    \n",
    "    shannonchart = alt.Chart(shannon_df[shannon_df['sample'].isin(samples)],height=100,width=700).mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "        x=alt.X('time:Q',axis=alt.Axis(grid=False,tickMinStep=1),title=None),\n",
    "        y=alt.Y('Shannon Quotient',title=['Shannon Quotient'])\n",
    "        \n",
    "    )\n",
    "    \n",
    "    #### Unclassified, Classified, HUMAN\n",
    "    Gruppen=[\n",
    "        'Other classified',\n",
    "        'Unclassified',\n",
    "        'Human'\n",
    "    ]\n",
    "    z = patientsub[[\n",
    "        'Other classified_Prozent ohne human pro total reads',\n",
    "        'Unclassified_Prozent pro total reads',\n",
    "        'Human_Prozent pro total reads',\n",
    "        'time',\n",
    "        'Total Reads']]\n",
    "    \n",
    "    z = z.melt(id_vars=['time','Total Reads'])\n",
    "    \n",
    "    z[['Group','Rest']] = z['variable'].str.split(\n",
    "        '_',\n",
    "        expand=True\n",
    "    )\n",
    "\n",
    "    z = z.drop(columns=['variable'])\n",
    "    z = z.pivot(index=['time','Total Reads'],columns='Group',values='value')\n",
    "\n",
    "    z = z.reset_index()\n",
    "    \n",
    "    z = z.melt(id_vars=['time','Total Reads'])\n",
    "    z = z.rename(columns={'value' : 'Fraction'})\n",
    "    \n",
    "    z['Count'] = z['Fraction'] * z['Total Reads']/100\n",
    "    \n",
    "    toplevelreadchart = alt.Chart(z,title='Top-Level Read Counts').transform_calculate(\n",
    "                      order=f\"-indexof({Gruppen}, datum.Group)\"\n",
    "                    ).mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "        x=alt.X(\"time:Q\", title=None, axis=alt.Axis(labels=False), scale=alt.Scale(paddingInner=1)), \n",
    "        y=alt.Y(\"Fraction:Q\",title='Fraction [%]',scale=alt.Scale(type='symlog',domain=[0,100]), \n",
    "                axis=alt.Axis(grid=False),stack=True), \n",
    "        color=alt.Color(\"Group:N\",sort=Gruppen,legend=alt.Legend(title=None,orient='top'),\n",
    "                    scale=alt.Scale(\n",
    "                        domain=Gruppen,range=['#EE6677', '#228833', '#CCBB44'])\n",
    "                       ),\n",
    "        order='order:Q',\n",
    "        tooltip=['Fraction','Count']\n",
    "    )\n",
    "    \n",
    "    ###### ARG \n",
    "    \n",
    "    argchart = alt.Chart(patientsub,height=100,width=700,title='AB Resistance Analysis').mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "        x=alt.X('time:Q',axis=alt.Axis(grid=False,tickMinStep=1),title=None),\n",
    "        y=alt.Y('ARG Reads pro Eingabe Reads *10000',title=['Reads with','Resistancegenehits/10K']),\n",
    "        order='time'\n",
    "    )\n",
    "    \n",
    "    #### Top Level View\n",
    "    Gruppen=[\n",
    "        'Bacteria',\n",
    "        'Fungi',\n",
    "        'Viruses'\n",
    "    ]\n",
    "    z = patientsub[['Bacteria_Prozent pro classified ohne human',\n",
    "                    'Fungi_Prozent pro classified ohne human',\n",
    "                    'Viruses_Prozent pro classified ohne human','time','classified reads ohne human']]\n",
    "    z = z.melt(id_vars=['time','classified reads ohne human'])\n",
    "    z[['Group','Rest']] = z['variable'].str.split('_',expand=True)\n",
    "\n",
    "    z = z.drop(columns=['variable'])\n",
    "    z = z.pivot(index=['time','classified reads ohne human'],columns='Group',values='value')\n",
    "\n",
    "    z = z.reset_index()\n",
    "    z = z.melt(id_vars=['time','classified reads ohne human'])\n",
    "    z = z.rename(columns={'value' : 'Fraction'})\n",
    "    z['Count'] = z['Fraction'] * z['classified reads ohne human']/100\n",
    "    \n",
    "    toplevelcompchart = alt.Chart(z,title='Top-Level Composition, Normalized, No Human').transform_calculate(\n",
    "                      order=f\"-indexof({Gruppen}, datum.Group)\"\n",
    "                    ).mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "        x=alt.X(\"time:Q\", title=None, axis=alt.Axis(labels=False), scale=alt.Scale(paddingInner=1)), \n",
    "        y=alt.Y(\"Fraction:Q\",title='Fraction [%]',scale=alt.Scale(type='symlog',domain=[0,100]), \n",
    "                axis=alt.Axis(grid=False),stack=True), \n",
    "        color=alt.Color(\"Group:N\",sort=Gruppen,legend=alt.Legend(title=None,orient='top'),\n",
    "                    scale=alt.Scale(\n",
    "                        domain=Gruppen,range=['#EE6677', '#228833', '#CCBB44'])\n",
    "                       ),\n",
    "        order='order:Q',\n",
    "        tooltip=['Fraction','Count']\n",
    "    )\n",
    "\n",
    "    combinedchart = reduce(lambda x,y : (x&y).resolve_scale(\n",
    "        x='shared',color='independent'\n",
    "    ),[\n",
    "        leukochart,\n",
    "        phasechart,\n",
    "        outcomechart,\n",
    "        outlierchart,\n",
    "        dnachart,\n",
    "        libchart,\n",
    "        toplevelreadchart,\n",
    "        toplevelcompchart]+\n",
    "        compcharts\n",
    "        +mscharts+[\n",
    "          divchart,\n",
    "        shannonchart,\n",
    "        argchart\n",
    "    ]).configure_axisY(minExtent=40).properties(title='Patient {} / Initial Cluster: {}'.format(patient,cluster))\n",
    "\n",
    "    combinedchart.save('Output/Patient_Journey/{}.html'.format(patient))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bray-Curtis Distance based PCoA <a class=\"anchor\" id=\"pcoa\"></a>\n",
    "$$\n",
    "B_{i,j} = 1 - \\frac{2C_{i,j}}{S_i+S_j}\n",
    "$$\n",
    "\n",
    "For relative abundances, this becomes:\n",
    "\n",
    "$1-C_{i,j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtable = get_normalized_abundances(kraken_dataframe,\n",
    "                                        level='G',\n",
    "                                        samples=PAPER_SAMPLES,\n",
    "                                        excluded_taxa_filter=[CHORDATA],\n",
    "                                        included_taxa_filter=None,\n",
    "                                       normalize=False)\n",
    "\n",
    "\n",
    "\n",
    "subtable['readAnteil'] = subtable['readcount'] / subtable.groupby(['time','patientid'])['readcount'].transform('sum')\n",
    "\n",
    "subtable = subtable.drop(columns=['level','readcount'])\n",
    "\n",
    "pivot_table = subtable.pivot_table(index=['patientid','time'],columns=['taxon'],aggfunc=sum).fillna(0)\n",
    "data = pivot_table.values\n",
    "\n",
    "samples = pivot_table.index.tolist()\n",
    "taxa_count = len(data[0])\n",
    "\n",
    "def braycurtis(indexA,indexB):\n",
    "    cij = 0\n",
    "    for taxonIndex in range(taxa_count):\n",
    "        cij += min(\n",
    "            data[indexA][taxonIndex],\n",
    "            data[indexB][taxonIndex]\n",
    "        )\n",
    "    return 1-cij\n",
    "\n",
    "bc_tuples = []\n",
    "\n",
    "distance_matrix = np.zeros(shape=(len(samples),len(samples)))\n",
    "\n",
    "for x in range(len(samples)):\n",
    "    for y in range(x+1,len(samples)):\n",
    "        print('Calculating all distancesfor sample {} / {}'.format(x+1,len(samples)),end='\\r')\n",
    "        distance = braycurtis(x,y)\n",
    "        distance_matrix[x][y] = distance\n",
    "        distance_matrix[y][x] = distance\n",
    "        bc_tuples.append((samples[x][0]+'/'+str(samples[x][1]),samples[y][0]+'/'+str(samples[y][1]),distance))\n",
    "        \n",
    "INVERT_X = True\n",
    "\n",
    "PCoA = ordination.pcoa(distance_matrix,number_of_dimensions=2)\n",
    "\n",
    "tuples = []\n",
    "\n",
    "for idx,s in PCoA.samples.iterrows():\n",
    "    sample = samples[int(idx)]\n",
    "    patient = sample[0]\n",
    "    time = sample[1]\n",
    "\n",
    "    tuples.append((s.PC1,s.PC2,patient,time))\n",
    "\n",
    "pcoa_table = pd.DataFrame(\n",
    "    tuples,\n",
    "    columns=['x','y','patient','time']\n",
    ")\n",
    "\n",
    "pcoa_table['Name'] = pcoa_table['patient'].astype(str)+'/'+pcoa_table['time'].astype(str)\n",
    "\n",
    "\n",
    "pcoa_table = pd.merge(sample_statistics,pcoa_table,left_on=['time','PatID'],right_on=['time','patient'],how='right')\n",
    "\n",
    "pcoa_table['Sample Type'] = pcoa_table['Name'].apply(lambda x : 'Control' if x.startswith('G') else 'patient')\n",
    "\n",
    "if INVERT_X:\n",
    "    pcoa_table['x'] = -pcoa_table['x']\n",
    "\n",
    "selection = alt.selection_multi(fields=['last_sample_reconstitution'])\n",
    "color = alt.condition(selection,\n",
    "                      alt.Color('Startcluster:N', legend=None),\n",
    "                      alt.value('lightgray'))\n",
    "\n",
    "\n",
    "pcoa_combined = (alt.Chart(pcoa_table).transform_calculate(\n",
    "    order = f\"indexof({selection.name}.Startcluster || [],datum.Startcluster)\"\n",
    ").mark_point(filled=True).encode(\n",
    "    x='x:Q',\n",
    "    y='y:Q',\n",
    "    color=color,\n",
    "    order=alt.Order('order:N',sort='ascending'),\n",
    "    tooltip=['Name']\n",
    ").interactive() | alt.Chart(pcoa_table).mark_rect().encode(\n",
    "    y=alt.Y('last_sample_reconstitution:N',title=None),\n",
    "    color=color\n",
    ").add_selection(selection))\n",
    "\n",
    "pcoa_combined.save(\n",
    "    'Output/PCOA.html'\n",
    ")\n",
    "\n",
    "pcoa_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_frame = pd.DataFrame(bc_tuples,columns=['Sample X','Sample Y','BCD'])\n",
    "bc_frame[['Patient X','Time X']] = bc_frame['Sample X'].str.split('/',expand=True)\n",
    "bc_frame[['Patient Y','Time Y']] = bc_frame['Sample Y'].str.split('/',expand=True)\n",
    "\n",
    "#Determine most extreme timepoints for each patient\n",
    "\n",
    "extreme_points = pd.concat(\n",
    "    [bc_frame.groupby('Patient X')['Time X'].min().rename('Earliest Time'),\n",
    "    bc_frame.groupby('Patient X')['Time X'].max().rename('Latest Time')],axis=1\n",
    ").reset_index()\n",
    "\n",
    "#Eliminate rows where earliest is >= 0 or latest is <= 0\n",
    "eliminated = extreme_points[\n",
    "    (extreme_points['Earliest Time'].astype(int) < 0)&\n",
    "    (extreme_points['Latest Time'].astype(int) > 0)\n",
    "]\n",
    "\n",
    "print(\n",
    "    'For the following patients no valid pre/post pair could be generated: {} (They will be EXCLUDED from analysis)'.format(\n",
    "        \n",
    "        set(extreme_points['Patient X']).difference(set(eliminated['Patient X']))\n",
    "    )\n",
    ")\n",
    "\n",
    "bc_frame = bc_frame[\n",
    "    (bc_frame['Patient X'].isin(eliminated['Patient X']))&\n",
    "    (bc_frame['Patient Y'].isin(eliminated['Patient X']))&\n",
    "    (bc_frame['Patient X'] == bc_frame['Patient Y'])\n",
    "]\n",
    "\n",
    "eliminated = eliminated.set_index('Patient X')\n",
    "\n",
    "bc_frame['Earliest X'] = bc_frame['Patient X'].apply(lambda x :  eliminated.loc[str(x)]['Earliest Time'])\n",
    "bc_frame['Latest X'] = bc_frame['Patient X'].apply(lambda x :  eliminated.loc[str(x)]['Latest Time'])\n",
    "\n",
    "bc_frame = bc_frame[\n",
    "    (bc_frame['Time X'] == bc_frame['Earliest X'])&\n",
    "    (bc_frame['Time Y'] == bc_frame['Latest X'])\n",
    "\n",
    "]\n",
    "\n",
    "bc_frame['Time X'] = bc_frame['Time X'].astype(int)\n",
    "bc_frame['Time Y'] = bc_frame['Time Y'].astype(int)\n",
    "\n",
    "bc_frame = pd.merge(\n",
    "    bc_frame,\n",
    "    sample_statistics[['PatID','time','Startcluster']],\n",
    "    how='left',\n",
    "    left_on=['Patient X','Time X'],\n",
    "    right_on=['PatID','time']\n",
    ")\n",
    "\n",
    "bc_frame\n",
    "\n",
    "bc_frame = pd.merge(\n",
    "    bc_frame,\n",
    "    sample_statistics[['PatID','time','leukocytephase_cluster_2_kurz']],\n",
    "    how='left',\n",
    "    left_on=['Patient Y','Time Y'],\n",
    "    right_on=['PatID','time']\n",
    ")[['Sample X','Sample Y','BCD','Startcluster','leukocytephase_cluster_2_kurz']]\n",
    "alt.Chart(bc_frame).mark_boxplot().encode(\n",
    "    x='Startcluster:N',\n",
    "    y='BCD'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Time Overview <a class=\"anchor\" id=\"sampletimes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_reduced = outcomes[\n",
    "    ['Pat ID',\n",
    "     'Day relative to HSCT',\n",
    "     'Day relative to HSCT.1',\n",
    "     'Day relative to HSCT.2',\n",
    "     'Day relative to HSCT.3']\n",
    "         ].rename(\n",
    "    columns={\n",
    "        'Day relative to HSCT' : 'Relapse',\n",
    "        'Day relative to HSCT.1' : 'Acute GvHD Grade 1-2',\n",
    "        'Day relative to HSCT.2' : 'Acute GvHD Grade 3-4',\n",
    "        'Day relative to HSCT.3' : 'Death',\n",
    "    }\n",
    ")\n",
    "outcomes_reduced['No Adverse Event']=outcomes_reduced.apply( lambda x : 0 if x.count() <= 1 else np.NaN,axis=1)\n",
    "outcomes_reduced\n",
    "\n",
    "outcomes_reduced = outcomes_reduced.melt(id_vars=['Pat ID'])\n",
    "outcomes_reduced = outcomes_reduced[(outcomes_reduced['value']==outcomes_reduced['value'])]\n",
    "\n",
    "outcomes_reduced.loc[\n",
    "    (outcomes_reduced['Pat ID']=='18.2'),'KI ID'\n",
    "] = '18.2'\n",
    "outcomes_reduced.loc[(outcomes_reduced['Pat ID']=='18.2'),'value'] -= OFFSET_PAT_18\n",
    "outcomes_reduced = outcomes_reduced[outcomes_reduced['variable'] != 'No Adverse Event']\n",
    "\n",
    "outcomes_reduced = outcomes_reduced.rename(columns={\n",
    "    'Pat ID' : 'PatID',\n",
    "    'variable' : 'Adverse Event',\n",
    "    'value' : 'time'\n",
    "})\n",
    "\n",
    "\n",
    "############\n",
    "\n",
    "sample_statistics['id'] = sample_statistics['PatID']+'/'+sample_statistics['time'].astype(str)\n",
    "overview = sample_statistics[sample_statistics['id'].isin(PAPER_SAMPLES)]\n",
    "overview = overview[~overview['PatID'].str.startswith('G')]\n",
    "\n",
    "\n",
    "combined = pd.concat([overview,outcomes_reduced])\n",
    "\n",
    "\n",
    "\n",
    "patientlist_sorted = []\n",
    "\n",
    "for startcluster in combined['Startcluster'].unique():\n",
    "    clusterlist = sorted(combined[combined['Startcluster'] == startcluster]['PatID'].unique())\n",
    "    patientlist_sorted += clusterlist\n",
    "\n",
    "        \n",
    "overview = (alt.Chart(combined[combined['Adverse Event'] == combined['Adverse Event']]).mark_point(size=38,color='black').encode(\n",
    "    x=alt.X('time',scale=alt.Scale(type='symlog'),axis=alt.Axis(grid=False)),\n",
    "    y=alt.Y('PatID',sort=patientlist_sorted),\n",
    "    shape=alt.Shape('Adverse Event:N',scale=alt.Scale(\n",
    "        domain=['Acute GvHD Grade 1','Acute GvHD Grade 3','Death','Relapse'],\n",
    "        range=['circle','square','cross','triangle']))\n",
    ")+alt.Chart(combined,width=800).mark_circle().encode(\n",
    "    x=alt.X('time',scale=alt.Scale(type='symlog'),axis=alt.Axis(grid=False)),\n",
    "    y=alt.Y('PatID:N',sort=patientlist_sorted,axis=alt.Axis(grid=True)),\n",
    "    color='Startcluster:N'\n",
    ")+alt.Chart(combined).mark_text(dx=0,dy=-6).encode(\n",
    "    x=alt.X('time',scale=alt.Scale(type='symlog'),axis=alt.Axis(grid=False)),\n",
    "    y=alt.Y('PatID:N',sort=patientlist_sorted,axis=alt.Axis(grid=True)),\n",
    "    text='time'\n",
    "))\n",
    "\n",
    "overview.save('Output/Samples.html')\n",
    "\n",
    "overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zymo Std. Analysis <a class=\"anchor\" id=\"zymo\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_level = get_normalized_abundances(kraken_dataframe,samples=['Zymo_Power/-100'],level='S',excluded_taxa_filter=None,normalize=False)\n",
    "\n",
    "species_level['Read Fraction (%)'] = species_level['readcount'] / species_level.groupby(['sample'])['readcount'].transform('sum')\n",
    "species_level = species_level.rename(columns={'sample' : 'Sample ID','taxon' : 'Taxon'})\n",
    "species_level = pd.concat([species_level,zymo_theory])[['Sample ID','Taxon','Read Fraction (%)']]\n",
    "\n",
    "\n",
    "melt = zymo_minimap.melt(id_vars='Index')\n",
    "\n",
    "def clean(x):\n",
    "    x = x.replace('_',' ').replace('albican','albicans')  \n",
    "    if x.startswith('Escherichia coli'):\n",
    "        return 'Escherichia coli'\n",
    "    return x\n",
    "\n",
    "melt['variable'] = melt['variable'].apply(\n",
    "    clean\n",
    ")\n",
    "\n",
    "\n",
    "melt = melt.rename(\n",
    "    columns={\n",
    "    'variable' : 'Taxon',\n",
    "        'Index' : 'Sample ID',\n",
    "        'value' : 'Read Fraction (%)'\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "melt = melt[melt['Sample ID'].isin(['Power/Reads','Power/Bases'])]\n",
    "\n",
    "\n",
    "melt['Sample ID'] = melt['Sample ID'].apply(lambda x : x + '_Minimap2')\n",
    "\n",
    "melt['Taxon'] = melt['Taxon'].apply(lambda x : 'Unmapped' if x == 'unmapped' else x)\n",
    "\n",
    "\n",
    "combined = pd.concat([melt,species_level])\n",
    "\n",
    "\n",
    "taxa_we_look_at = list(combined.groupby('Taxon')['Read Fraction (%)'].mean().sort_values(ascending=False)[:(20+1)].keys())\n",
    "print('Determined the following taxa as relevant:',taxa_we_look_at)\n",
    "\n",
    "#assign everything else to the \"other\" group and readjust sum\n",
    "combined.loc[~combined['Taxon'].isin(taxa_we_look_at), 'Taxon'] = 'Other'\n",
    "combined = combined.groupby(['Taxon','Sample ID'],as_index=False).sum()\n",
    "\n",
    "\n",
    "colorMap = {}\n",
    "\n",
    "taxa = combined['Taxon'].unique()\n",
    "\n",
    "palette = sns.color_palette(\"tab20\",n_colors=len(taxa)) #hls\n",
    "\n",
    "for tax,col in zip(list(taxa),palette):\n",
    "    colorMap[tax] = col #colors.to_hex(col)\n",
    "    \n",
    "altdomain = []\n",
    "altrange = []\n",
    "\n",
    "for x in combined['Taxon'].unique():\n",
    "    \n",
    "    if x == 'Other' or x == 'Unassigned at Level' or x == 'Unmapped':\n",
    "        continue\n",
    "\n",
    "    c = colorMap[x]\n",
    "    altdomain.append(x)\n",
    "    color = colors.to_hex(c)\n",
    "    altrange.append(color)\n",
    "    \n",
    "altdomain.append('Other')\n",
    "altrange.append(colors.to_hex((1,1,1)))\n",
    "altdomain.append('Unassigned at Level')\n",
    "altrange.append(colors.to_hex((0.15,0.15,0.15)))\n",
    "altdomain.append('Unmapped')\n",
    "altrange.append(colors.to_hex((0.45,0.45,0.45)))\n",
    "\n",
    "c= alt.Chart(\n",
    "  combined\n",
    ").transform_calculate(\n",
    "order=f\"-indexof({altdomain}, datum.Taxon)\"\n",
    ").mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "    x=alt.X('Sample ID:N',sort=['Zymo_Power/-100']\n",
    "),\n",
    "    y=alt.Y('Read Fraction (%):Q',scale=alt.Scale(domain=(0,1))),\n",
    "    color=alt.Color('Taxon:N',legend=alt.Legend(columns=2,symbolLimit=0,labelLimit=0),scale=alt.Scale(domain=altdomain,range=altrange)),\n",
    "    tooltip=['Read Fraction (%)','Taxon'],\n",
    "    order=alt.Order('order:Q')\n",
    ")\n",
    "c.save('Output/Zymo_Overview.html')\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eukaryotes  <a class=\"anchor\" id=\"eukaryotes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Input/eukaryotes.csv',dtype={'Taxon' : str})\n",
    "data['Taxon Name'] = data['Taxon'].map(idtonames)\n",
    "data['Chi Average'] = data['Chi Values'].apply(lambda x : sum(int(y) for y in eval(x))/len(eval(x)) if len(eval(x)) != 0 else '?')\n",
    "\n",
    "\n",
    "taxa_sorting = data.groupby('Taxon')['Total Reads'].sum().sort_values(ascending=False).keys().to_list()\n",
    "\n",
    "tooltip = ['Mapped Reads','Horizontal Coverage','Taxon Name','Sample','Chi Average','Total Reads']\n",
    "\n",
    "\n",
    "alt.Chart(data,title='Horizontal Coverage').mark_rect().encode(\n",
    "    x='Sample:N',\n",
    "    y=alt.Y('Taxon Name:N',sort=taxa_sorting),\n",
    "    color=alt.condition(\n",
    "        alt.datum['Horizontal Coverage'] == 0,\n",
    "        alt.value('lightgrey'),\n",
    "        'Horizontal Coverage:Q'\n",
    "    ),\n",
    "    tooltip=tooltip\n",
    "    \n",
    ").save('Output/HorizontalCoverageEukaryota.html')\n",
    "\n",
    "\n",
    "\n",
    "data['Mapped Reads Count'] = data['Mapped Reads'] * data['Total Reads']\n",
    "\n",
    "top_level_stats = pd.concat(\n",
    "    [data.groupby('Taxon Name')['Total Reads'].sum(),\n",
    "    data.groupby('Taxon Name')['Mapped Reads Count'].sum()/data.groupby('Taxon Name')['Total Reads'].sum()],axis=1\n",
    ").reset_index().rename(columns={0 : 'Mapped Fraction'}).sort_values(by='Mapped Fraction')\n",
    "\n",
    "top_level_stats.to_csv('Output/excel_eukaryota.csv',index=False)\n",
    "\n",
    "data.groupby('Taxon Name')[['Mapped Reads Count','Total Reads']].sum().sort_values(by='Total Reads').to_csv('Overview_Eukaryota.csv')\n",
    "\n",
    "\n",
    "\n",
    "aggregated = data.groupby('Taxon Name',as_index=False)[['Total Reads','Mapped Reads Count']].sum()\n",
    "aggregated['Fraction'] = aggregated['Mapped Reads Count'] / aggregated['Total Reads']\n",
    "\n",
    "alt.Chart(aggregated,width=800,height=800).mark_point().encode(\n",
    "    x='Total Reads',\n",
    "    y='Fraction',\n",
    "    color='Taxon Name',\n",
    "    tooltip=['Total Reads','Mapped Reads Count','Taxon Name','Fraction']\n",
    ").interactive().save('Output/VerifiedEukaryotaVSTotalReads.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 20\n",
    "\n",
    "found_in_x_samples = data.groupby('Taxon Name')['Sample'].size()\n",
    "found_in_x_samples = found_in_x_samples[found_in_x_samples > x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 500\n",
    "\n",
    "average_above_x = data.groupby('Taxon Name')['Total Reads'].mean()\n",
    "average_above_x = average_above_x[average_above_x > x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = data[\n",
    "    data['Taxon Name'].isin(\n",
    "        set(found_in_x_samples.keys()).intersection(set(average_above_x.keys()))\n",
    "    )\n",
    "   \n",
    "]\n",
    "\n",
    "len(subset['Taxon Name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alt.Chart(subset).mark_boxplot().encode(\n",
    "    x=alt.X('Taxon Name'),\n",
    "    y='Mapped Reads'\n",
    ").save('Output/VerificationEukaryota.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Metamaps Kongruence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metamaps_dataframe['Sample'] = metamaps_dataframe['patientid']+'_'+metamaps_dataframe['time'].astype(str)\n",
    "merged = pd.merge(data,metamaps_dataframe[\n",
    "    (metamaps_dataframe['Sample'].isin(data['Sample']))&\n",
    "    (metamaps_dataframe['readcount'] >= 100)\n",
    "]\n",
    "                  \n",
    "                  ,left_on=['Sample','Taxon Name'],right_on=['Sample','taxon'],how='left')\n",
    "merged = merged.fillna(0)\n",
    "merged['Excess Kraken'] = merged['Total Reads']-merged['readcount']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(merged).mark_point().encode(\n",
    "    y=alt.Y('Excess Kraken',scale=alt.Scale(type='symlog')),\n",
    "    x=alt.X('Mapped Reads',bin=alt.Bin(maxbins=10))\n",
    ").save('Output/ExcessKrakenEukaryota.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crassphage <a class=\"anchor\" id=\"crassphage\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kraken_data = kraken_dataframe[kraken_dataframe['taxon'].isin(['uncultured crAssphage','root'])]\n",
    "kraken_data = kraken_data[kraken_data['sample'].isin(PAPER_SAMPLES)]\n",
    "kraken_data = kraken_data.pivot(\n",
    "    index='sample',columns='taxon',values='readcount'\n",
    ").fillna(0)\n",
    "\n",
    "kraken_data['crassphage_detected'] = kraken_data['uncultured crAssphage']/kraken_data['root']\n",
    "\n",
    "kraken_data = kraken_data.reset_index()\n",
    "\n",
    "def rename(x):\n",
    "    split = x.rsplit('_',1)\n",
    "    return split[0]+'/'+split[1]\n",
    "\n",
    "minimap_data = pd.read_csv('Input/Crassphage/summary.csv')\n",
    "minimap_data['Sample'] = minimap_data['Sample'].apply(rename)\n",
    "\n",
    "\n",
    "minimap_data_aggregated = minimap_data.groupby('Sample')[['Fraction Mapped','Mapped Reads']].sum().reset_index()\n",
    "\n",
    "\n",
    "combined = pd.merge(kraken_data,minimap_data,left_on='sample',right_on='Sample',how='left')\n",
    "\n",
    "max_value = max(\n",
    "    combined['Fraction Mapped'].max(),combined['crassphage_detected'].max()\n",
    ")\n",
    "\n",
    "line = pd.DataFrame({\n",
    "    'X': [0, max_value],\n",
    "    'Y': [0, max_value],\n",
    "})\n",
    "\n",
    "\n",
    "(alt.Chart(combined,width=700,height=700).mark_point().encode(\n",
    "    y='sum(Fraction Mapped)',\n",
    "    x='crassphage_detected',\n",
    "    tooltip='sample'\n",
    ")+alt.Chart(line,width=700,height=700).mark_line(color= 'lightgray').encode(\n",
    "        x= 'X',\n",
    "        y= 'Y'\n",
    "    )).interactive().save('Output/CrassphageKrakenVsMinimap.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap, which Crassphage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_table = minimap_data.pivot(index='Sample',columns='Reference',values='Fraction Mapped').fillna(0).melt(ignore_index=False).reset_index().rename(columns={'value' : 'Fraction Mapped'})\n",
    "heatmap_table['Unambiguous'] = heatmap_table['Reference'] != 'Ambiguous'\n",
    "heatmap_table_simplified = heatmap_table.groupby(['Sample','Unambiguous'],as_index=False)['Fraction Mapped'].sum()\n",
    "\n",
    "top_x = 100\n",
    "\n",
    "\n",
    "heatmap_table_filtered = heatmap_table[\n",
    "    (heatmap_table['Reference']!='Ambiguous')&\n",
    "    (heatmap_table['Sample'].isin(PAPER_SAMPLES)  )  \n",
    "]\n",
    "heatmap_table_filtered['Fraction Mapped']=heatmap_table_filtered['Fraction Mapped']/heatmap_table_filtered.groupby('Sample')['Fraction Mapped'].transform('sum')\n",
    "top_refs = heatmap_table_filtered.groupby('Reference')['Fraction Mapped'].sum().sort_values(ascending=False).keys().tolist()[:top_x]\n",
    "\n",
    "heatmap_table_filtered = heatmap_table_filtered[\n",
    "    \n",
    "    heatmap_table_filtered['Reference'].isin(top_refs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(alt.Chart(heatmap_table_simplified[heatmap_table_simplified['Sample'].isin(PAPER_SAMPLES)]).mark_bar().encode(\n",
    "    x=alt.X('Sample',sort=sort_samples(PAPER_SAMPLES),axis=alt.Axis(orient='top')),\n",
    "    y=alt.Y('Fraction Mapped',stack=True),\n",
    "    color='Unambiguous'\n",
    ")&alt.Chart(heatmap_table_filtered).mark_rect().encode(\n",
    "    x=alt.X('Sample',sort=sort_samples(PAPER_SAMPLES),axis=None),\n",
    "    y=alt.Y('Reference',sort=top_refs),\n",
    "    color=alt.condition(\n",
    "        alt.datum['Fraction Mapped'] == 0,\n",
    "        alt.value('lightgrey'),\n",
    "        alt.Color('Fraction Mapped:Q',title='Fraction of uniquely assigned reads')\n",
    "    )\n",
    ")).resolve_scale(x='shared',color='independent').save('Output/CrassphageOverview.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migration = pd.read_csv('Input/Crassphage/migration.csv',dtype={'Tax ID' : str})\n",
    "migration['Fraction'] = migration['Reads']/migration.groupby('Sample')['Reads'].transform('sum')\n",
    "migration['Taxon Name'] = migration['Tax ID'].map(idtonames)\n",
    "\n",
    "top_hits = migration.groupby('Tax ID')['Fraction'].mean().sort_values(ascending=False).keys()[:20]\n",
    "migration = migration[migration['Tax ID'].isin(top_hits)]\n",
    "\n",
    "\n",
    "alt.Chart(migration).mark_boxplot().encode(\n",
    "    y='Fraction',\n",
    "    x=alt.X('Taxon Name:N'),\n",
    "    tooltip=['Reads','Sample','Fraction','Taxon Name']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = pd.read_csv('Input/crassphage_details.csv')\n",
    "\n",
    "charts = []\n",
    "\n",
    "for metric in ['Percentage Aligned','Mapping Quality','Identity']:\n",
    "\n",
    "    amount,edges = np.histogram(details[metric],bins=100)\n",
    "\n",
    "    centers = []\n",
    "    for x,y in zip(edges[:-1],edges[1:]):\n",
    "        centers.append((x+y)/2)\n",
    "\n",
    "    tuples = []\n",
    "    for x,y in zip(centers,amount):\n",
    "        tuples.append((x,y))\n",
    "\n",
    "    df = pd.DataFrame(tuples,columns=[metric,'Count'])\n",
    "\n",
    "    c = alt.Chart(df).mark_bar().encode(\n",
    "        x=metric,\n",
    "        y=alt.Y('Count',scale=alt.Scale(type='symlog'))\n",
    "    )\n",
    "    \n",
    "    charts.append(c)\n",
    "    \n",
    "reduce(lambda x,y : x&y, charts).save('Output/CrassphageAlignmentDetails.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marker Species Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Load Marker Gene Data')\n",
    "\n",
    "ms_frame = get_normalized_abundances(kraken_dataframe,normalize=True,level='S',samples=PAPER_SAMPLES)\n",
    "\n",
    "ms_frame=ms_frame[ms_frame['sample'].isin(PAPER_SAMPLES)] \n",
    "ms_frame['estimated abundance'] = (ms_frame['readcount'] / ms_frame.groupby(['time','patientid'])['readcount'].transform('sum')).round(10)\n",
    "\n",
    "ms_frame = ms_frame[\n",
    "    ms_frame['taxon'].str.startswith(tuple(MARKER_SPECIES_REDUCED))\n",
    "]\n",
    "\n",
    "ms_frame = ms_frame[ms_frame['taxon'] != 'Saccharomyces cerevisiae x Saccharomyces kudriavzevii']\n",
    "\n",
    "ms_frame = ms_frame.pivot(index=['patientid','time'],\n",
    "                          columns=['taxon'],\n",
    "                          values=['estimated abundance']).fillna(0).reset_index().melt(id_vars=['patientid','time'])\n",
    "\n",
    "\n",
    "ms_frame =pd.merge(\n",
    "    ms_frame,sample_statistics[['PatID','time','leukocytephase_cluster_2_kurz']],\n",
    "    how='left',\n",
    "    left_on=['patientid','time'],\n",
    "    right_on=['PatID','time']\n",
    ")[['patientid','time','taxon','value','leukocytephase_cluster_2_kurz']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(ms_frame,width=100,height=100).mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'leukocytephase_cluster_2_kurz:O',\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None\n",
    "    ),\n",
    "    y=alt.Y('value:Q',axis=alt.Axis(grid=False,minExtent=40), title='Estimated Abundance'),\n",
    "    color='taxon',\n",
    "    x=alt.X('taxon',title=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illumina Sequencing / Population Shifts <a class=\"anchor\" id=\"strains\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load samplesheet (To resolve illumina ids -> patient/time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesheetDictPatient = {}\n",
    "samplesheetDictTime = {}\n",
    "    \n",
    "samplesheet = pd.read_csv('Input/samples.tsv',sep='\\t')\n",
    "#Retain only entries that have illumina files\n",
    "samplesheet=samplesheet[samplesheet['illuminafile'] ==samplesheet['illuminafile']]\n",
    "for idx,row in samplesheet.iterrows():\n",
    "    samplesheetDictPatient[row['illuminafile']] = row['patientid']\n",
    "    samplesheetDictTime[row['illuminafile']] = row['time']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process/annotate distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGH_CONFIDENCE_CUTOFF = 50\n",
    "output_folder = 'Output/StrainAnalysis/GutTrSnp_{}'.format(HIGH_CONFIDENCE_CUTOFF)\n",
    "\n",
    "guttrsnp_distances = pd.read_csv('Input/GutTrSnp/RealData_{}/distances.csv'.format(\n",
    "   HIGH_CONFIDENCE_CUTOFF \n",
    "),dtype={\n",
    "    'Destination ID':str,\n",
    "    'Source ID' :str,\n",
    "    'Taxon' : str\n",
    "})\n",
    "\n",
    "#Kick out the stuff where there was no overlap and thus no distance\n",
    "guttrsnp_distances = guttrsnp_distances.dropna()\n",
    "guttrsnp_distances=guttrsnp_distances[guttrsnp_distances['Share Of Overlap'] >= 0.01]\n",
    "\n",
    "guttrsnp_distances['Taxon Name'] = guttrsnp_distances['Taxon'].map(idtonames)\n",
    "guttrsnp_distances['Source'] = guttrsnp_distances['Source ID']+'/'+guttrsnp_distances['Source Time'].astype(str)\n",
    "guttrsnp_distances['Destination'] = guttrsnp_distances['Destination ID']+'/'+guttrsnp_distances['Destination Time'].astype(str)\n",
    "guttrsnp_distances['PrePost Pair'] = (\n",
    "    guttrsnp_distances['Source ID'] == guttrsnp_distances['Destination ID']\n",
    ")&(\n",
    "guttrsnp_distances['Source Time'] < 0\n",
    ")&(\n",
    "guttrsnp_distances['Destination Time'] > 0\n",
    ")\n",
    "guttrsnp_distances['Within Patient'] = guttrsnp_distances['Source ID'] == guttrsnp_distances['Destination ID']\n",
    "#DoT\n",
    "guttrsnp_distances['Elapsed Time'] = guttrsnp_distances['Destination Time'] - guttrsnp_distances['Source Time']\n",
    "guttrsnp_distances['Distance over Time'] = guttrsnp_distances['GutTrSnp Distance'] / guttrsnp_distances['Elapsed Time']\n",
    "\n",
    "guttrsnp_distances['Distance Name'] = guttrsnp_distances['Source Time'].astype(str) + '->' + guttrsnp_distances['Destination Time'].astype(str)\n",
    "\n",
    "coverages = pd.read_csv('Input/GutTrSnp/RealData_{}/coverages.csv'.format(\n",
    "    HIGH_CONFIDENCE_CUTOFF\n",
    "),dtype={\n",
    "    'Patient ID' : str,\n",
    "    'Taxon ID' : str\n",
    "})\n",
    "guttrsnp_distances=pd.merge(guttrsnp_distances,coverages,how='left',left_on=['Source ID','Source Time','Taxon'],right_on=['Patient ID','Time','Taxon ID'])\n",
    "guttrsnp_distances=pd.merge(guttrsnp_distances,coverages,how='left',left_on=['Destination ID','Destination Time','Taxon'],right_on=['Patient ID','Time','Taxon ID'])\n",
    "\n",
    "\n",
    "\n",
    "guttrsnp_distances = guttrsnp_distances.drop(columns=[\n",
    "    'Patient ID_x','Patient ID_y',\n",
    "    'Time_x','Time_y',\n",
    "    'Taxon ID_x','Taxon ID_y'\n",
    "])\n",
    "\n",
    "\n",
    "bins=[0,10,20,50,100,200,500,1000,2000]\n",
    "\n",
    "\n",
    "guttrsnp_distances['VCG Source'] = pd.cut(guttrsnp_distances['Average Vertical Coverage_x'],bins)\n",
    "guttrsnp_distances['VCG Destination'] = pd.cut(guttrsnp_distances['Average Vertical Coverage_y'],bins)\n",
    "guttrsnp_distances = guttrsnp_distances.dropna()\n",
    "guttrsnp_distances['VCG Source']=guttrsnp_distances['VCG Source'].astype(str)\n",
    "guttrsnp_distances['VCG Destination']=guttrsnp_distances['VCG Destination'].astype(str)\n",
    "\n",
    "timepoints = {}\n",
    "\n",
    "for taxon in guttrsnp_distances['Taxon'].unique():\n",
    "    for source_id in guttrsnp_distances['Source ID'].unique():\n",
    "        subtable = guttrsnp_distances[\n",
    "            (\n",
    "                guttrsnp_distances['Taxon'] == taxon\n",
    "            )&(\n",
    "                guttrsnp_distances['Source ID'] == source_id\n",
    "                \n",
    "            )\n",
    "        ]\n",
    "        timepoints[(taxon,source_id)] = sorted(subtable['Source Time'].unique().tolist())\n",
    "        \n",
    "def sequential_test(row):\n",
    "    if row['Within Patient']:\n",
    "        if (row['Taxon'],row['Source ID']) in timepoints:\n",
    "            sequence = timepoints[(row['Taxon'],row['Source ID'])]\n",
    "            if sequence.index(row['Destination Time'])-sequence.index(row['Source Time']) == 1:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "guttrsnp_distances['Sequential'] = guttrsnp_distances.apply(sequential_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_to_simulation={\n",
    "    'Bacteroides vulgatus ATCC 8482' : 'PVulgatus',\n",
    "    'Enterococcus faecium' : 'EFaecium',\n",
    "    'Lactobacillus gasseri ATCC 33323 = JCM 1131' : 'LGasseri',\n",
    "    'Shigella sonnei 53G' : 'EColi',\n",
    "    \n",
    "}\n",
    "\n",
    "guttrsnp_distances_simulation = pd.read_csv('Input/GutTrSnp/SubspeciesSimulation/aggregatedDistances.csv',dtype={'Taxon' : str}).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Checks / Generic Analysis\n",
    "\n",
    "\n",
    "os.makedirs(output_folder,exist_ok=True)\n",
    "\n",
    "plots = []\n",
    "curation = []\n",
    "for taxon in guttrsnp_distances['Taxon Name'].unique():#['Bacteroides vulgatus ATCC 8482','Flavonifractor plautii','Bacteroides uniformis','Parabacteroides distasonis ATCC 8503','Parabacteroides merdae']:\n",
    "    \n",
    "    taxontable = guttrsnp_distances[\n",
    "        (guttrsnp_distances['Taxon Name'] == taxon)&\n",
    "        (guttrsnp_distances['Sequential'])&\n",
    "        (guttrsnp_distances['Share Of Overlap'] >= 0.5)\n",
    "    ]\n",
    "       \n",
    "    taxontable['PrePost Pair'] = taxontable['PrePost Pair'].map(\n",
    "        {\n",
    "            True : 'Yes',\n",
    "            False : 'No'\n",
    "        }\n",
    "    )\n",
    "   \n",
    "    if len(taxontable) < 3:\n",
    "        continue    \n",
    " \n",
    "    min_dist = taxontable['GutTrSnp Distance'].min()\n",
    "    max_dist = taxontable['GutTrSnp Distance'].max()\n",
    "    step = (max_dist-min_dist)/50\n",
    "    \n",
    "    max_line = alt.Chart(\n",
    "        pd.DataFrame(\n",
    "        [(max_dist)],columns=['Distance']\n",
    "    )\n",
    "    ).mark_rule().encode(\n",
    "        x='Distance',\n",
    "        size=alt.value(3)\n",
    "    )\n",
    "    \n",
    "    if taxon in mapping_to_simulation:\n",
    "        \n",
    "        subdata = guttrsnp_distances_simulation[\n",
    "            (guttrsnp_distances_simulation['Simulated Taxon'] == mapping_to_simulation[taxon])&\n",
    "            (guttrsnp_distances_simulation['Taxon'] == taxontable['Taxon'].values[0])\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        min_dist = min(subdata['GutTrSnp Distance'].min(),min_dist)\n",
    "        max_dist = min(subdata['GutTrSnp Distance'].max(),max_dist)\n",
    "        step = (max_dist-min_dist)/50 \n",
    "             \n",
    "    \n",
    "    plot = alt.Chart(taxontable,height=300,title='Sequential Samples Same Patients').mark_bar().encode(\n",
    "            x=alt.X('GutTrSnp Distance',bin=alt.Bin(extent=[min_dist,max_dist],step=step),title='Distance'),\n",
    "            y=alt.Y('count()',axis=alt.Axis(tickMinStep=1),stack=True),\n",
    "            color=alt.Color('PrePost Pair',title='Across Transplantation')\n",
    "        )\n",
    "    \n",
    "        \n",
    "    plot = (plot |alt.Chart(taxontable,height=300,title='Sequential Samples Same Patients').mark_point().encode(\n",
    "            y=alt.Y('GutTrSnp Distance',title='Distance'),\n",
    "            x='Elapsed Time',\n",
    "            tooltip=['Source','Destination','Share Of Overlap'],\n",
    "            color=alt.Color('PrePost Pair',title='Across Transplantation')\n",
    "        )          \n",
    "    )\n",
    "    \n",
    "    curation.append(\n",
    "        taxontable\n",
    "    )\n",
    "                   \n",
    "                \n",
    "    prepairs = guttrsnp_distances[\n",
    "        (guttrsnp_distances['Taxon Name'] == taxon)&\n",
    "        (guttrsnp_distances['Source ID'] != guttrsnp_distances['Destination ID'])&\n",
    "        (guttrsnp_distances['Source Time'] < 0)&\n",
    "        (guttrsnp_distances['Destination Time'] < 0)\n",
    "    ]\n",
    "    \n",
    "        \n",
    "    plot =  (plot | alt.Chart(\n",
    "        prepairs,height=300,title='Pre-Samples Different Patients'\n",
    "    ).mark_bar().encode(\n",
    "        x=alt.X('GutTrSnp Distance',bin=alt.Bin(extent=[min_dist,max_dist],step=step),title='Distance'),\n",
    "            y=alt.Y('count()',axis=alt.Axis(tickMinStep=1))\n",
    "    ) \n",
    "    )\n",
    "    \n",
    "    all_distances = guttrsnp_distances[\n",
    "        (guttrsnp_distances['Taxon Name'] == taxon)&\n",
    "        (guttrsnp_distances['Share Of Overlap'] >= 0.5)\n",
    "    ]\n",
    "    \n",
    "    plot =  (plot | (alt.Chart(\n",
    "            all_distances,height=300,title='All Distances'\n",
    "        ).mark_bar().encode(\n",
    "            x=alt.X('GutTrSnp Distance',bin=alt.Bin(maxbins=50),title='Distance'),\n",
    "            y=alt.Y('count()',axis=alt.Axis(tickMinStep=1))\n",
    "        ) +max_line)\n",
    "    )\n",
    "\n",
    "    if taxon in mapping_to_simulation:\n",
    "        \n",
    "        subdata = guttrsnp_distances_simulation[\n",
    "            (guttrsnp_distances_simulation['Simulated Taxon'] == mapping_to_simulation[taxon])&\n",
    "            (guttrsnp_distances_simulation['Taxon'] == taxontable['Taxon'].values[0])\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        plot = (plot |(alt.Chart(\n",
    "            subdata[subdata['GutTrSnp Distance'] != 0],height=300,title='Simulated Data'\n",
    "        ).mark_bar().encode(\n",
    "            x=alt.X('GutTrSnp Distance',bin=alt.Bin(maxbins=50),title='Distance'),\n",
    "            y=alt.Y('count()',axis=alt.Axis(tickMinStep=1))\n",
    "        )+max_line))\n",
    "        \n",
    "    plots.append(\n",
    "        plot.properties(title=taxon)\n",
    "    )\n",
    "\n",
    "\n",
    "          \n",
    "chart = reduce(lambda x,y : x& y,plots)\n",
    "\n",
    "chart.save(output_folder+'/Overview.html')\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of manually curated shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = pd.read_excel('Input/Annotations/Patient_Statistics (2).xlsx',dtype={'Pat ID' : str})\n",
    "outcomes_reduced = outcomes[\n",
    "    ['Pat ID',\n",
    "     'Day relative to HSCT',\n",
    "     'Day relative to HSCT.1',\n",
    "     'Day relative to HSCT.2',\n",
    "     'Day relative to HSCT.3']\n",
    "         ].rename(\n",
    "    columns={\n",
    "        'Day relative to HSCT' : 'Relapse',\n",
    "        'Day relative to HSCT.1' : 'Acute GvHD Grade 1-2',\n",
    "        'Day relative to HSCT.2' : 'Acute GvHD Grade 3-4',\n",
    "        'Day relative to HSCT.3' : 'Death',\n",
    "    }\n",
    ")\n",
    "outcomes_reduced\n",
    "\n",
    "outcomes_reduced = outcomes_reduced.melt(id_vars=['Pat ID'])\n",
    "outcomes_reduced = outcomes_reduced[(outcomes_reduced['value']==outcomes_reduced['value'])]\n",
    "\n",
    "outcomes_reduced.loc[\n",
    "    (outcomes_reduced['Pat ID']=='18.2'),'KI ID'\n",
    "] = '18.2'\n",
    "outcomes_reduced.loc[(outcomes_reduced['Pat ID']=='18.2'),'value'] -= OFFSET_PAT_18\n",
    "outcomes_reduced = outcomes_reduced[['Pat ID','variable','value']]\n",
    "outcomes_reduced = outcomes_reduced[outcomes_reduced['value'] != '?']\n",
    "\n",
    "outcomes_reduced\n",
    "\n",
    "manual_curation = pd.read_csv('Input/curation_reduced.csv')\n",
    "manual_curation['Distance Name'] = manual_curation['Taxon Name']+':'+manual_curation['Source']+'->'+manual_curation['Destination']\n",
    "manual_curation['Time X'] = manual_curation['Source'].str.split('/',expand=True)[1].astype(int)\n",
    "manual_curation['Time Y'] = manual_curation['Destination'].str.split('/',expand=True)[1].astype(int)\n",
    "manual_curation['Pat ID'] = manual_curation['Source'].str.split('/',expand=True)[0]\n",
    "manual_curation['Any Annotation'] = manual_curation['Clinical Annotation']!='No events tracked'\n",
    "\n",
    "sample_times = (manual_curation.groupby(['Pat ID','Taxon Name'])['Time X'].apply(list)+manual_curation.groupby(['Pat ID','Taxon Name'])['Time Y'].apply(list)\n",
    ").reset_index().explode(0).rename(columns={0:'Time'}).drop_duplicates()\n",
    "\n",
    "charts=[]\n",
    "for taxon in manual_curation['Taxon Name'].unique():\n",
    "    st = manual_curation[manual_curation['Taxon Name'] == taxon]\n",
    "    orm = outcomes_reduced[outcomes_reduced['Pat ID'].isin(st['Pat ID'].unique())]\n",
    "    orm['variable'] = orm['variable'].replace(\n",
    "        {'Acute GvHD Grade 3-4' : 'GvHD Grade 3-4'}\n",
    "    )\n",
    "    c = alt.Chart(st).mark_line().encode(\n",
    "        x=alt.X('Time X',title=None),\n",
    "        x2=alt.X2('Time Y',title=None),\n",
    "        y=alt.Y('Pat ID',title='Patient'),\n",
    "        color=alt.Color('Manual Curation',scale=alt.Scale(\n",
    "            domain=['Shift','Stable'],range=['Orange','Grey']\n",
    "        ))\n",
    "    )+alt.Chart(orm).mark_point(color='black',size=56).encode(\n",
    "        x=alt.X('value',title='Day'),\n",
    "        y=alt.Y('Pat ID',title='Patient'),\n",
    "        shape=alt.Shape('variable',\n",
    "                       scale=alt.Scale(\n",
    "                           domain=['GvHD Grade 3-4','Relapse','Death'],range=['circle','square','cross']))\n",
    "    )+alt.Chart(pd.DataFrame([(0)],columns=['x'])).mark_rule().encode(x='x')+alt.Chart(\n",
    "    sample_times[sample_times['Taxon Name'] == taxon]\n",
    "\n",
    "    ).mark_point(color='black',filled=True).encode(\n",
    "        y='Pat ID',\n",
    "        x=alt.X('Time',title=None)\n",
    "    )\n",
    "    charts.append(c.properties(title=taxon,width=800))\n",
    "reduce(lambda x,y : x&y, charts).resolve_scale(x='shared')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
