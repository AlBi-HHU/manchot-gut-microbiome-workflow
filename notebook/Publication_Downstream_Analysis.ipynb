{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTPREFIX = 'manchot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages / imports\n",
    "\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import math\n",
    "import altair as alt\n",
    "import os\n",
    "import numpy as np\n",
    "import colorcet as cc\n",
    "import matplotlib.colors as colors\n",
    "import glob\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.stats import mannwhitneyu, wilcoxon, pearsonr\n",
    "from functools import lru_cache,reduce\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "import scipy.spatial.distance as ssd\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "#from skbio.stats import ordination\n",
    "\n",
    "os.makedirs('Output',exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Definitions / Helper Functions](#defines)\n",
    "* [Load Data](#loaddata)\n",
    "* [Normalization Function](#normalize)\n",
    "* Analysis\n",
    "    * [Top Level Statistics](#toplevel)\n",
    "    * [P-Values Top Level Statistics](#pvalues)\n",
    "    * [Overview Top Taxa per Level](#toptaxa)\n",
    "    * [Unmapped and Human Fraction](#human)\n",
    "    * [AMR Gene Counts](#amr)\n",
    "    * [Taxa Counts (Diversity)](#taxacount)\n",
    "    * [Barplots Compositions](#barplots)\n",
    "    * [Bray-Curtis Based PCoA](#pcoa)\n",
    "    * [Sampling Times Overview](#sampletimes)\n",
    "    * [Zymo Std Analysis](#zymo)\n",
    "    * [Eukaryotes Analysis](#eukaryotes)\n",
    "    * [Crassphage Analysis](#crassphage)\n",
    "    * [Population Level Analysis / Illumina](#strains)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions / Helper Functions <a class=\"anchor\" id=\"defines\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePrePostPairs(table : pd.DataFrame, patientcolumn: str = 'patientid' , timecolumn: str ='time' ):\n",
    "    result = []\n",
    "    counts_of_common_occurences = table.groupby([patientcolumn,timecolumn]).size().reset_index()\n",
    "    for patient in counts_of_common_occurences[patientcolumn].unique():\n",
    "        subtable = counts_of_common_occurences[counts_of_common_occurences[patientcolumn] == patient]\n",
    "        for time1,time2 in it.combinations(subtable[timecolumn].unique(),2):\n",
    "            if time1 < 0 and time2 > 0:\n",
    "                result.append((patient,time1,time2))\n",
    "            elif time1 > 0 and time2 < 0:\n",
    "                result.append((patient,time2,time1))\n",
    "    return result\n",
    "\n",
    "\n",
    "#Helper Function that returns a preceding timepoint (if one exists) for each row in a table\n",
    "def find_previous_sample(table,row):\n",
    "    #print (row ['PatID'])\n",
    "    timepoints_patient = list(table[table['PatID']==row['PatID']]['time'].unique())\n",
    "    timepoints_filtered = [timepoint for timepoint in timepoints_patient if timepoint < row['time']]\n",
    "    if len(timepoints_filtered) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return sorted(timepoints_filtered)[-1]\n",
    "\n",
    "#Helper function that recalculates some relative days and relabels some patients\n",
    "def adjust_table(df):\n",
    "\n",
    "    df.loc[(df['samplename'] == '999_0'), 'samplename'] = 'G1_-100'\n",
    "    df.loc[df['samplename'] == '132_-100', 'samplename'] = '45_-100'\n",
    "    df.loc[df['samplename'] == '133_558', 'samplename'] = '14_558'\n",
    "    df.loc[df['samplename'] == '134_-5', 'samplename'] = '46_-5'\n",
    "\n",
    "def sort_samples(samples):\n",
    "    return sorted(\n",
    "        samples,\n",
    "        key=lambda x : ('G' in x,\n",
    "                        int(x.split('/')[0].replace('G','').split('.')[0]),\n",
    "                        int(x.split('/')[0].replace('G','').split('.')[-1]),\n",
    "                        int(x.split('/')[1]))\n",
    "    )\n",
    "\n",
    "\n",
    "def is_below_or_equal(x,y):\n",
    "    if x in ['-2','-3']:\n",
    "        return False\n",
    "    if x == y:\n",
    "        return True\n",
    "    \n",
    "    parent_node = taxonomy[x]\n",
    "    if parent_node == y: #Parent was the node we looked for\n",
    "        return True\n",
    "    elif parent_node == x: #This is only the case at the root node\n",
    "        return False\n",
    "    else: #We need to keep looking\n",
    "        return is_below_or_equal(parent_node,y)\n",
    "\n",
    "#Constants, frequently used\n",
    "CHORDATA = '7711'\n",
    "FUNGI = '4751'\n",
    "VIRUSES = '10239'\n",
    "BACTERIA = '2'\n",
    "ARCHAEA = '2157'\n",
    "EUKARYOTA = '2759'\n",
    "PROTOZOA = '1891100'\n",
    "FIRMICUTES = '1239'\n",
    "BACTEROIDETES = '976'\n",
    "ACTINOBACTERIA = '201174'\n",
    "PROTEOBACTERIA = '1224'\n",
    "VERRUCOMICROBIA = '74201'\n",
    "BACTEROIDES = '816'\n",
    "PREVOTELLA = '838'\n",
    "ALISTIPES = '239759'\n",
    "PARABACTEROIDES = '375288'\n",
    "PLANTAE = '33090'\n",
    "HOMO='9605'\n",
    "\n",
    "## Custom Taxon IDs\n",
    "NOT_ENOUGH_READS = -103\n",
    "NOT_VALIDATED = -104\n",
    "\n",
    "OFFSET_PAT_18 = 161\n",
    "\n",
    "PAPER_SAMPLES = ['G1/-100','G2/-100','G3/-100','G4/-100','G5/-100','G6/-100',\n",
    "             'G7/-100','G8/-100','G9/-100','G10/-100','G11/-100',\n",
    "             '4/-7','7/-6','11/-2','12/-6','14/-7','15/-6','16/-5','17/-7','19/-2',\n",
    "             '21/-9','24/-10','28/-5','29/-5','32/-5','34/-6','38/-6','39/-6','42/-8',\n",
    "             '13/-7','18/-3','22/-4','23/-9','25/-62','37/-5','40/-1','41/-6','20/-4',\n",
    "             '26/-3','31/-6','33/-2','36/-6',\n",
    "             '4/1','11/0','15/6','16/2','17/9','17/13','24/0','24/9','28/1','29/1','34/10','39/1','42/6',\n",
    "             '18/13','22/0','23/12','25/1','37/1','41/14','26/12','31/6','33/7',\n",
    "             '15/16','28/21','29/15','34/18','38/15','39/15','42/18','42/30',\n",
    "             '25/15','37/17','37/22','40/17','26/27',\n",
    "             '12/34','24/49','28/63','34/41',\n",
    "             '18/91','21/50','22/91','23/61','25/31','31/58','33/47','36/38','36/63',\n",
    "             '11/182','12/342','14/558','15/104','15/189','16/163','16/171','17/154',\n",
    "             '19/153','24/127','28/237','29/186','39/115',\n",
    "             '13/298','18.2/-9','18.2/16','18.2/71','21/120','23/130','37/138','26/185','31/178']\n",
    "\n",
    "PAPER_AND_NEW_SAMPLES = ['G1/-100','G2/-100','G3/-100','G4/-100','G5/-100','G6/-100',\n",
    "             'G7/-100','G8/-100','G9/-100','G10/-100','G11/-100',\n",
    "             '4/-7','7/-6','11/-2','12/-6','14/-7','15/-6','16/-5','17/-7','19/-2',\n",
    "             '21/-9','24/-10','28/-5','29/-5','32/-5','34/-6','38/-6','39/-6','42/-8',\n",
    "             '13/-7','18/-3','22/-4','23/-9','25/-62','37/-5','40/-1','41/-6','20/-4',\n",
    "             '26/-3','31/-6','33/-2','36/-6',\n",
    "             '4/1','11/0','15/6','16/2','17/9','17/13','24/0','24/9','28/1','29/1','34/10','39/1','42/6',\n",
    "             '18/13','22/0','23/12','25/1','37/1','41/14','26/12','31/6','33/7',\n",
    "             '15/16','28/21','29/15','34/18','38/15','39/15','42/18','42/30',\n",
    "             '25/15','37/17','37/22','40/17','26/27',\n",
    "             '12/34','24/49','28/63','34/41',\n",
    "             '18/91','21/50','22/91','23/61','25/31','31/58','33/47','36/38','36/63',\n",
    "             '11/182','12/342','14/558','15/104','15/189','16/163','16/171','17/154',\n",
    "             '19/153','24/127','28/237','29/186','39/115',\n",
    "             '13/298','18.2/-9','18.2/16','18.2/71','21/120','23/130','37/138','26/185','31/178', '11/672', '12/711',\n",
    "              '15/565', '16/542', '17/530', '19/581', '23/440',\n",
    "                '24/416', '25/322', '26/394', '28/360', \n",
    "               '33/278', '34/296', '37/292', '38/256', '39/255', '40/236', '41/161', \n",
    "              '42.3/164', '43/-7', '44/-6', '44/12', '44/28', '46/-5', '47/-3', \n",
    "                '47.3/48', '48/-8', '48/3', '49/-3', '49/4', '50/-1', '50/1', '50/22', \n",
    "               '52/-1', '53/2', '54/-7', '54/2', '56/-7','53/-6']\n",
    "\n",
    "NEW_SAMPLES = [ '4/29', '11/672', '12/711',\n",
    "              '15/565', '16/24', '16/83', '16/542', '17/-1', '17/530', '17.2/154', '19/581', '20/-1', '21/-3', '23/440',\n",
    "                '24/416', '25/322', '26/394', '27/2', '27/14', '27/174', '27/174', '27/234', '27/379', '28/360', '32/3',\n",
    "               '33/278', '34/296', '36/-4', '37/292', '38/256', '39/255', '40/236', '41/161', \n",
    "              '42.3/164', '43/-7', '44/-6', '44/12', '44/28', '45/-100', '46/-5', '47/-3', \n",
    "                '47.3/48', '48/-8', '48/3', '49/-3', '49/4', '50/-1', '50/1', '50/22', '51/5',\n",
    "               '52/-1','53/-6', '53/2', '54/-7', '54/2', '55/2', '56/-7']\n",
    "\n",
    "ALL_SAMPLES = ['G1/-100','G2/-100','G3/-100','G4/-100','G5/-100','G6/-100',\n",
    "             'G7/-100','G8/-100','G9/-100','G10/-100','G11/-100',\n",
    "             '4/-7','7/-6','11/-2','12/-6','14/-7','15/-6','16/-5','17/-7','19/-2',\n",
    "             '21/-9','24/-10','28/-5','29/-5','32/-5','34/-6','38/-6','39/-6','42/-8',\n",
    "             '13/-7','18/-3','22/-4','23/-9','25/-62','37/-5','40/-1','41/-6','20/-4',\n",
    "             '26/-3','31/-6','33/-2','36/-6',\n",
    "             '4/1','11/0','15/6','16/2','17/9','17/13','24/0','24/9','28/1','29/1','34/10','39/1','42/6',\n",
    "             '18/13','22/0','23/12','25/1','37/1','41/14','26/12','31/6','33/7',\n",
    "             '15/16','28/21','29/15','34/18','38/15','39/15','42/18','42/30',\n",
    "             '25/15','37/17','37/22','40/17','26/27',\n",
    "             '12/34','24/49','28/63','34/41',\n",
    "             '18/91','21/50','22/91','23/61','25/31','31/58','33/47','36/38','36/63',\n",
    "             '11/182','12/342','14/558','15/104','15/189','16/163','16/171','17/154',\n",
    "             '19/153','24/127','28/237','29/186','39/115',\n",
    "             '13/298','18.2/-9','18.2/16','18.2/71','21/120','23/130','37/138','26/185','31/178', '4/29', '11/672', '12/711',\n",
    "              '15/565', '16/24', '16/83', '16/542', '17/-1', '17/530', '17.2/154', '19/581', '20/-1', '21/-3', '23/440',\n",
    "                '24/416', '25/322', '26/394', '27/2', '27/14', '27/174', '27/174', '27/234', '27/379', '28/360', '32/3',\n",
    "               '33/278', '34/296', '36/-4', '37/292', '38/256', '39/255', '40/236', '41/161', '42.1/164', '42.2/164',\n",
    "              '42.3/164', '42.4/164', '43/-7', '44/-6', '44/12', '44/28', '45/-100', '46/-5', '47/-3', '47.1/48',\n",
    "               '47.2/48', '47.3/48', '47.4/48', '48/-8', '48/3', '49/-3', '49/4', '50/-1', '50/1', '50/22', '51/5',\n",
    "               '52/-1', '53/2', '54/-7', '54/2', '55/2', '56/-7']\n",
    "\n",
    "DUPLICATES = ['15/104',\n",
    " '16/163',\n",
    " '18.2/71',\n",
    " '21/50',\n",
    " '23/61',\n",
    " '29/15',\n",
    " '31/58',\n",
    " '34/10',\n",
    " '34/18',\n",
    " '36/38',\n",
    " '37/17',\n",
    " '42/18']\n",
    "\n",
    "\n",
    "HEALTHY_SAMPLES = ['G1/-100','G2/-100','G3/-100','G4/-100','G5/-100','G6/-100',\n",
    "             'G7/-100','G8/-100','G9/-100','G10/-100','G11/-100']\n",
    "\n",
    "ZYMO_SAMPLES_NEW = ['Zymo_Zymo/-100','Zymo_EZ1/-100','Zymo_Power/-100','Zymo_Pro/-100']+['Stool_Power_11/-100',\n",
    " 'Stool_Power_21/-100',\n",
    " 'Stool_Pro_11/-100',\n",
    " 'Stool_Pro_21/-100',\n",
    " 'Stool_Zymo_11/-100',\n",
    " 'Stool_Zymo_21/-100',\n",
    " 'Stool_EZ1_12/-100',\n",
    " 'Stool_EZ1_21/-100']\n",
    "\n",
    "LIFELINES = ['LL81_E07_7627', 'LL91_A04_8564',\n",
    "       'LL93_H07_8785', 'LL87_A09_8215', 'LL80_A10_7551',\n",
    "       'LL47_D03_4330', 'LL63_B04_5872', 'LL83_F03_7788',\n",
    "       'LL66_C11_6217', 'LL80_D01_7482', 'LL92_D01_8637',\n",
    "       'LL59_F11_5548', 'LL83_G11_7853', 'LL72_C02_6721',\n",
    "       'LL84_D03_7882', 'LL45_F06_4164', 'LL89_E11_8427',\n",
    "       'LL60_B04_5584', 'LL45_F11_4204', 'LL70_E06_6539'\n",
    "    \n",
    "]\n",
    "\n",
    "MARKER_GENERA = [\n",
    "    'Prevotella',\n",
    "    'Enterobacter',\n",
    "    'Bacteroides',\n",
    "    'Enterococcus',\n",
    "    'Faecalibacterium',\n",
    "    'Klebsiella',\n",
    "    'Escherichia',\n",
    "    'Akkermansia',\n",
    "    'Clostridium',\n",
    "    'Blautia',\n",
    "    'Skunavirus',\n",
    "    'Bifidobacterium',\n",
    "    'Eubacterium',\n",
    "    'Ruminococcus'\n",
    "]\n",
    "\n",
    "\n",
    "MARKER_SPECIES = [\n",
    "    'Akkermansia muciniphila',\n",
    "    'Bacteroides thetaiotaomicron',\n",
    "    'Bacteroides ovatus',\n",
    "    'Faecalibacterium prausnitzii',\n",
    "    'Enterococcus faecium',\n",
    "    'Enterococcus faecalis',\n",
    "    'Enterocloster bolteae',\n",
    "    'Candida albicans',\n",
    "    'Saccharomyces cerevisiae',\n",
    "    'Toxoplasma gondii',\n",
    "    'Blautia wexlerae',\n",
    "    'Eubacterium biforme',\n",
    "    '[Ruminococcus] gnavus',\n",
    "    'uncultured crAssphage'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data <a class=\"anchor\" id=\"loaddata\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kraken2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This data can be collected from the snakemake workflow\n",
    "kraken_dataframe = pd.read_csv(\n",
    "    '../data/output/mapping/{}_KrakenFullDump.csv'.format(PROJECTPREFIX),\n",
    "    usecols=[1,2,3,4,5],dtype={'samplename' : str,'taxonid':str}\n",
    ")\n",
    "adjust_table(kraken_dataframe)\n",
    "\n",
    "lifelines_data = pd.read_csv('Input/KrakenLifelines.csv',usecols=[0,1,2,3,4],dtype={'samplename' : str,'taxonid':str}\n",
    "            )\n",
    "if 'patientid' in lifelines_data.columns:\n",
    "    print('>>><<<\\nThe lifelines data has an old format, the columns patientid and time need to be combined to a new column with format {patientid}_{time}\\n>>><<<')\n",
    "    assert(False)\n",
    "kraken_dataframe = pd.concat([kraken_dataframe,lifelines_data\n",
    "]\n",
    "                            \n",
    "                            )\n",
    "\n",
    "crassphages_species = kraken_dataframe[kraken_dataframe['taxon'] == 'uncultured crAssphage']\n",
    "crassphages_species['level'] = 'G'\n",
    "crassphages_species['taxon'] = 'Crassphage Pseudo-Genus'\n",
    "\n",
    "kraken_dataframe = pd.concat([kraken_dataframe,crassphages_species])\n",
    "\n",
    "\n",
    "kraken_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sample Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PatID is specified as String (Text) so IDs like 18.2 don't get confused as decimal numbers\n",
    "\n",
    "#Tables contain annotations regarding the individual samples\n",
    "sample_statistics = pd.read_excel('Input/Annotations/SampleStatistics.xlsx',dtype={'PatID' : str})\n",
    "\n",
    "#Outcomes\n",
    "outcomes = pd.read_excel('Input/Annotations/PatientStatistics.xlsx',dtype={'Pat ID' : str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMR Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_SAMPLES_UNDERSCORE = [s.replace('/','_') for s in PAPER_SAMPLES]\n",
    "\n",
    "#can be extracted from the snakemake workflow, used to normalize the AMR read counts\n",
    "sequencing_stats = pd.read_csv('../data/output/manchot_sampleStats.reads.filtered.2.csv')[[\n",
    "    'samplename','nReads','nBases','median','mean','standard deviation','minimum','maximum'\n",
    "]]\n",
    "\n",
    "\n",
    "adjust_table(sequencing_stats)\n",
    "sequencing_stats = sequencing_stats[sequencing_stats['samplename'].isin(PAPER_SAMPLES_UNDERSCORE)]\n",
    "\n",
    "readcount_min = sequencing_stats['nReads'].min()\n",
    "\n",
    "sequencing_stats = sequencing_stats.set_index(['samplename'])\n",
    "sequencing_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fraction(x):\n",
    "    if x['samplename'] in sequencing_stats.index:\n",
    "        return x['readcount'] / sequencing_stats.loc[x['samplename']]['nReads']\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "#can be extracted from the snakemake workflow\n",
    "amr = pd.read_csv('../data/output/amr/manchot_fulldump_amr.csv').rename(columns={'Samplename':'samplename'})\n",
    "\n",
    "amr['readcount'] = amr.apply(\n",
    "    lambda x : 1/x['Ambiguous Assignments'] if x['Ambiguous Assignments'] != 0 else 1,axis=1\n",
    ")\n",
    "adjust_table(amr)\n",
    "amr = amr[amr['samplename'].isin(PAPER_SAMPLES_UNDERSCORE)]\n",
    "\n",
    "amr['Gene Pro Read'] =amr.apply(lambda x : calc_fraction(x) ,axis=1)\n",
    "\n",
    "amr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically update the sample statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amr_stats = pd.concat(\n",
    "    [\n",
    "        (amr.groupby('samplename')['readcount'].sum()).rename('ARG Reads'),\n",
    "        (sequencing_stats.groupby('samplename')['nReads'].sum()*10000).rename('ARG Input Reads'),\n",
    "        (amr.groupby('samplename')['Gene Pro Read'].sum()*10000).rename('ARG Reads per 10000 Reads')\n",
    "    ],axis=1\n",
    ").reset_index()\n",
    "amr_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['ARG Reads','ARG Input Reads','ARG Reads per 10000 Reads']:\n",
    "    if column in sample_statistics.columns:\n",
    "        print('The column {} was already found in the table and will be replaced!'.format(column))\n",
    "        sample_statistics = sample_statistics.drop(columns=column)\n",
    "sample_statistics['samplename'] = sample_statistics['PatID']+'_'+sample_statistics['time'].astype(str)\n",
    "sample_statistics=pd.merge(sample_statistics,amr_stats,on='samplename',how='left')\n",
    "sample_statistics = sample_statistics.drop(columns='samplename')\n",
    "sample_statistics.to_excel('Input/Annotations/SampleStatistics.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kraken2/Metamaps Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idtonames = {}\n",
    "\n",
    "with open('Input/Taxonomy/names.dmp','r') as f:\n",
    "    for l in f.read().splitlines():\n",
    "        d=[x.strip() for x in l.split('|')]\n",
    "        if d[3] == 'scientific name':\n",
    "            idtonames[d[0]] = d[1]\n",
    "\n",
    "\n",
    "taxonomy = {}\n",
    "\n",
    "levels = {}\n",
    "\n",
    "with open('Input/Taxonomy/nodes.dmp','r') as f:\n",
    "    for l in f.read().splitlines():\n",
    "        d=[x.strip() for x in l.split('|')]\n",
    "        taxonomy[d[0]] = d[1]\n",
    "        levels[d[0]] = d[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zymo Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zymo_theory = pd.read_csv('Input/zymo_theory.csv',\n",
    "                          header=None,\n",
    "                          names=['Taxon','Read Fraction']\n",
    "                         )\n",
    "\n",
    "zymo_theory_taxa = zymo_theory['Taxon'].tolist()\n",
    "zymo_theory['Read Fraction (%)'] = zymo_theory['Read Fraction']/100\n",
    "zymo_theory['Sample ID'] = 'Zymo Theoretical Composition'\n",
    "\n",
    "zymo_minimap = pd.read_csv('Input/zymo_minimap_verification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = {} #Multilevel\n",
    "\n",
    "for validation_level, kraken_key in {\n",
    "    'species' : 'S',\n",
    "    'genus' : 'G'\n",
    "}.items():\n",
    "    \n",
    "    if not os.path.exists('../data/output/validation/{}_{}.csv'.format(PROJECTPREFIX,validation_level)):\n",
    "        print('Could not load validation data for project prefix {} and level {}, check if this is ok'.format(PROJECTPREFIX,validation_level))\n",
    "        continue\n",
    "    validation_data[kraken_key] =pd.read_csv('../data/output/validation/{}_{}.csv'.format(PROJECTPREFIX,validation_level),dtype={'Taxon ID':str})\n",
    "    validation_data[kraken_key] = validation_data[kraken_key].rename(columns={'Sample' : 'samplename'})\n",
    "    adjust_table(validation_data[kraken_key])\n",
    "\n",
    "    validation_data[kraken_key][['patientid','time']] = validation_data[kraken_key]['samplename'].str.rsplit('_',1,expand=True) #specific for manchot project\n",
    "    validation_data[kraken_key]['time'] = validation_data[kraken_key]['time'].astype(int)\n",
    "    validation_data[kraken_key]['Taxon Name'] = validation_data[kraken_key]['Taxon ID'].map(idtonames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization Function <a class=\"anchor\" id=\"normalize\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    level, #No default value here to avoid accidental mistakes!\n",
    "    samples = None,\n",
    "    random_seed = (4+8+15+16+23+42),\n",
    "    normalize=True,\n",
    "    excluded_taxa_filter = None, #Can be a list of taxa\n",
    "    included_taxa_filter = None, #Only one taxa, becomes new root\n",
    "):\n",
    "    \n",
    "    ### Helper Functions for Filtering\n",
    "    \n",
    "    not_found_taxa = set()\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def is_not_below(x,y):\n",
    "        parent_node = taxonomy[x]\n",
    "        if parent_node == y: #Parent was the node we looked for\n",
    "            return False\n",
    "        elif parent_node == x: #This is only the case at the root node\n",
    "            return True\n",
    "        else: #We need to keep looking\n",
    "            return is_not_below(parent_node,y)\n",
    "\n",
    "\n",
    "    def filter_function(row,taxon):\n",
    "        if row['taxonid'] == '0': #Root Node\n",
    "            return False\n",
    "        if row['taxonid'] not in taxonomy:\n",
    "            #print('Warning: TaxID {} is not in the taxonomy, this is (potentially) bad!'.format(row['taxonid']))\n",
    "            return False\n",
    "        if row['taxonid'] == taxon: #This is the taxon itself, remove\n",
    "            return False\n",
    "        #Otherwise we will check if the taxon is below our target taxon in the taxonomy\n",
    "        return is_not_below(row['taxonid'],taxon)\n",
    "\n",
    "\n",
    "    ###\n",
    "    \n",
    "    #We begin by assuming the raw kraken dataframe as input\n",
    "    working_table = kraken_dataframe\n",
    "\n",
    "    \n",
    "    #####################\n",
    "    #   SELECT SAMPLES\n",
    "    #\n",
    "    #####################    \n",
    "    \n",
    "    #Filter to target samples\n",
    "    if samples != None:\n",
    "        working_table = working_table[working_table['samplename'].isin(samples)]    \n",
    "\n",
    "\n",
    "    #####################\n",
    "    #   CALCULATE UNASSIGNED READS\n",
    "    #\n",
    "    #####################    \n",
    "\n",
    "    #Determine root readcounts\n",
    "    root_readcounts_kr = None\n",
    "\n",
    "    if included_taxa_filter != None:\n",
    "        root_readcounts_kr = working_table[\n",
    "            working_table['taxonid']==included_taxa_filter\n",
    "        ].groupby(\n",
    "            ['samplename']\n",
    "        )['readcount'].sum()\n",
    "\n",
    "    else: #If we don't filter we take all reads -> R\n",
    "        root_readcounts_kr = working_table[\n",
    "            working_table['level']=='R'\n",
    "        ].groupby(\n",
    "            ['samplename']\n",
    "        )['readcount'].sum()\n",
    "\n",
    "\n",
    "    if excluded_taxa_filter != None:\n",
    "            root_readcounts_kr -= working_table[\n",
    "            working_table['taxonid'].isin(excluded_taxa_filter)\n",
    "        ].groupby(\n",
    "            ['samplename']\n",
    "        )['readcount'].sum()\n",
    "\n",
    "    readcounts_at_level=None\n",
    "\n",
    "    if included_taxa_filter != None:\n",
    "        include = working_table[\n",
    "            working_table['level'] == level\n",
    "        ].apply(lambda x : filter_function(x,included_taxa_filter),axis=1)\n",
    "\n",
    "        readcounts_at_level = working_table[\n",
    "            working_table['level'] == level\n",
    "        ][~include]\n",
    "\n",
    "        readcounts_at_level = readcounts_at_level.groupby(\n",
    "            ['samplename']\n",
    "        )['readcount'].sum()\n",
    "    else:\n",
    "        #Determine total read counts at level\n",
    "        readcounts_at_level = working_table[\n",
    "            working_table['level'] == level\n",
    "        ].groupby(\n",
    "            ['samplename']\n",
    "        )['readcount'].sum()\n",
    "\n",
    "\n",
    "    if excluded_taxa_filter != None:\n",
    "        for taxon in excluded_taxa_filter:\n",
    "            include = working_table[\n",
    "                working_table['level'] == level\n",
    "            ].apply(lambda x : filter_function(x,taxon),axis=1)\n",
    "\n",
    "            excluded_taxon_sum = working_table[\n",
    "                working_table['level'] == level\n",
    "            ][~include]\n",
    "\n",
    "            readcounts_at_level -= excluded_taxon_sum.groupby(\n",
    "            ['samplename']\n",
    "                )['readcount'].sum()\n",
    "\n",
    "    #Add unassigned at level\n",
    "    unassigned_entries = []\n",
    "    for sample in readcounts_at_level.keys():\n",
    "\n",
    "        difference = root_readcounts_kr[sample]-readcounts_at_level[sample]\n",
    "        \n",
    "        addon_table = pd.DataFrame([\n",
    "            (difference,'Unassigned at Level','-2',level,sample)\n",
    "        ],columns=[\n",
    "            'readcount','taxon','taxonid','level','samplename'\n",
    "        ])\n",
    "        \n",
    "        unassigned_entries.append(addon_table)\n",
    "                \n",
    "    #Combine into one table\n",
    "    if len(unassigned_entries) != 0:\n",
    "        unassigned_table = pd.concat(unassigned_entries)\n",
    "        working_table = pd.concat([unassigned_table,working_table])\n",
    "    \n",
    "    #####################\n",
    "    #   DOWNSAMPLING\n",
    "    #\n",
    "    #####################\n",
    "    \n",
    "    #Filter to target level\n",
    "    downsampled_table = working_table[working_table['level'] == level]  \n",
    "    \n",
    "    #Filter for taxon if required\n",
    "    if included_taxa_filter != None:\n",
    "        print('Reducing composition to subtree below taxon: {}'.format(idtonames[included_taxa_filter]))\n",
    "        include = downsampled_table.apply(lambda x : filter_function(x,included_taxa_filter),axis=1)\n",
    "        downsampled_table = downsampled_table[~include]\n",
    "           \n",
    "    if normalize:\n",
    "    \n",
    "        # Identify lowest read count\n",
    "        readAnzahlen = downsampled_table.groupby('samplename')['readcount'].sum()\n",
    "        minimaleReadAnzahl = readAnzahlen.min()\n",
    "        print('The minimal read count across all samples is [Taxonomic Level {}]: {}'.format(level,minimaleReadAnzahl))\n",
    "\n",
    "        frames = []\n",
    "\n",
    "        # Draw new counts for each sample\n",
    "        for sample in downsampled_table['samplename'].unique():\n",
    "\n",
    "            sample_table = downsampled_table[downsampled_table['samplename'] == sample]\n",
    "            sample = sample_table.sample(\n",
    "                n=round(minimaleReadAnzahl),\n",
    "                random_state=random_seed,\n",
    "                weights='readcount',\n",
    "                replace=True\n",
    "            )\n",
    "\n",
    "            sample = sample.groupby([\n",
    "                'taxon',\n",
    "                'taxonid',\n",
    "                'samplename',\n",
    "                'level'\n",
    "            ],as_index=False).count()\n",
    "\n",
    "            frames.append(sample)\n",
    "\n",
    "        #Overwrite table with downsampled entries\n",
    "        downsampled_table = pd.concat(frames)   \n",
    "    \n",
    "    #####################\n",
    "    #   FILTERING II\n",
    "    #\n",
    "    #####################\n",
    "    \n",
    "    unassigned_downsampled_table = downsampled_table[downsampled_table['taxonid'] == '-2']\n",
    "    assigned_downsampled_table = downsampled_table[downsampled_table['taxonid'] != '-2']\n",
    "    \n",
    "    \n",
    "\n",
    "    if excluded_taxa_filter != None:\n",
    "        for taxon in excluded_taxa_filter:\n",
    "            include = assigned_downsampled_table.apply(lambda x : filter_function(x,taxon),axis=1)\n",
    "            assigned_downsampled_table = assigned_downsampled_table[include]\n",
    "   \n",
    "\n",
    "    tables=[unassigned_downsampled_table,assigned_downsampled_table]\n",
    "    \n",
    "    #Create dummy entries for patients that have nothing\n",
    "\n",
    "\n",
    "    if samples != None:\n",
    "        dummy_entries = []\n",
    "        for sample_id in samples:\n",
    "            if sample_id not in assigned_downsampled_table['samplename'].unique():\n",
    "                print('Creating a dummy entry for sample {} (No reads with the selected parameters)'.format(sample_id))\n",
    "                addon_table = pd.DataFrame([\n",
    "                            (0,'Absolutely Nothing','Nothing','N',sample_id)\n",
    "                        ],columns=[\n",
    "                            'readcount','taxon','taxonid','level','samplename'\n",
    "                        ]) \n",
    "                dummy_entries.append(addon_table)\n",
    "                tables+=dummy_entries\n",
    "\n",
    "    downsampled_table = pd.concat(tables)\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "    return downsampled_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that applies validation with specified cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_validation(\n",
    "    normalized_kraken_dataframe,\n",
    "    level = 'S',\n",
    "    validation_cutoff = 0.2,\n",
    "    readcount_cutoff = 5\n",
    "):\n",
    "    with_validation = pd.merge(\n",
    "        normalized_kraken_dataframe,\n",
    "        validation_data[level][['samplename','Taxon ID','Validation Rate']],\n",
    "        left_on=['samplename','taxonid'],\n",
    "        right_on=['samplename','Taxon ID'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    #Phase 1: Kick out low abundance groups, assign to \"Not enough reads\"\n",
    "    with_validation.loc[with_validation['readcount']<readcount_cutoff,'taxon'] = 'Not enough reads'\n",
    "    with_validation.loc[with_validation['readcount']<readcount_cutoff,'taxonid'] = NOT_ENOUGH_READS\n",
    "\n",
    "    #Phase 2: Check for rest if validates, if not assign to \"Not validated\"\n",
    "    with_validation['Validated'] = (with_validation['Validation Rate'] >= validation_cutoff)\n",
    "    with_validation.loc[\n",
    "        (with_validation['Validated']!=True)&~(with_validation['taxon'] == 'Not enough reads'), 'taxon'\n",
    "    ] = 'Not validated'\n",
    "    with_validation.loc[\n",
    "        (with_validation['Validated']!=True)&~(with_validation['taxon'] == 'Not enough reads'), 'taxonid'\n",
    "    ] = NOT_VALIDATED\n",
    "    \n",
    "    #Readjust sum (group multiple \"Not enough reads\" entries together)\n",
    "    return with_validation.groupby(['taxon','taxonid','samplename'],as_index=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Species Validation Info for Top-Level Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_level_tuples = []\n",
    "\n",
    "validation_map = {\n",
    "    'Viruses' : (VIRUSES,[]),\n",
    "    'OtherEukaryota':(EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]),\n",
    "    'Fungi':(FUNGI,[]),\n",
    "    'Bacteria':(BACTERIA,[]),\n",
    "    'Plants':(PLANTAE,[]),\n",
    "    'Archaea':(ARCHAEA,[]),\n",
    "    'Microbiome' : ('1',[CHORDATA,PLANTAE]),\n",
    "    'Human' : (CHORDATA,[])\n",
    "\n",
    "}\n",
    "\n",
    "for group in validation_map:\n",
    "    \n",
    "    TAXON,EXCLUDE = validation_map[group]\n",
    "\n",
    "    \n",
    "    input_table = get_normalized_abundances(\n",
    "        kraken_dataframe,\n",
    "        level='S',\n",
    "        samples=[s.replace('/','_') for s in PAPER_SAMPLES], #New sample format uses underscore '_' instead of '/'\n",
    "        included_taxa_filter=TAXON,\n",
    "        excluded_taxa_filter=EXCLUDE,\n",
    "        normalize=False\n",
    "    )\n",
    "\n",
    "    with_validation = apply_validation(\n",
    "        input_table,\n",
    "        level = 'S',\n",
    "        validation_cutoff = 0.2,\n",
    "        readcount_cutoff = 5\n",
    "    )\n",
    "\n",
    "    for samplename in PAPER_SAMPLES:\n",
    "    \n",
    "        samplename = samplename.replace('/','_')\n",
    "\n",
    "\n",
    "        subtable = with_validation[with_validation['samplename'] == samplename]\n",
    "        \n",
    "\n",
    "        \n",
    "        subtable['Validated'] = (subtable['Validation Rate'].astype(float) >= 0.2)\n",
    "        #print(subtable)\n",
    "        weighted_validation_rate= (\n",
    "            subtable['Validated']*subtable['readcount']\n",
    "        ).sum()/subtable['readcount'].sum()\n",
    "        top_level_tuples.append((group,samplename,weighted_validation_rate >= 0.8))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(top_level_tuples,columns=['Group','Sample','Validated']).to_csv('Output/ValidationTopLevelGroups_MoreThan80.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Level Statistics & Visualization <a class=\"anchor\" id=\"toplevel\"></a>\n",
    "\n",
    "Here we generate top-level plots for the different timepoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats kraken\n",
    "classified = kraken_dataframe[kraken_dataframe['level'].isin(['R','U'])].groupby(['samplename','taxon','level'],as_index=False).sum().pivot(\n",
    "    index=['samplename'],columns=['level'],values=['readcount']\n",
    ")\n",
    "\n",
    "classified = classified.rename(columns={'R' : 'Classified','U' : 'Unclassified'})\n",
    "\n",
    "classified.columns = classified.columns.get_level_values(1)\n",
    "\n",
    "total = kraken_dataframe[kraken_dataframe['level'].isin(['R','U'])].groupby(['samplename'],as_index=False).sum()\n",
    "total = total.rename(columns={'readcount' : 'Total'})\n",
    "\n",
    "combined = pd.merge(total,classified,how='left',on=['samplename'])\n",
    "\n",
    "for taxon in [CHORDATA,BACTERIA,FUNGI,VIRUSES,EUKARYOTA,ARCHAEA,PLANTAE]:\n",
    "    taxonOnly = kraken_dataframe[kraken_dataframe['taxonid'] == taxon].groupby(['samplename'],as_index=False).sum()\n",
    "    taxonOnly = taxonOnly.rename(columns={'readcount' : idtonames[taxon]})\n",
    "    combined = pd.merge(combined,taxonOnly,how='left',on=['samplename'])\n",
    "    \n",
    "combined = combined.set_index(['samplename'])\n",
    "\n",
    "for column in combined.columns:\n",
    "    print(column)\n",
    "    combined[column+'_Prozent'] = (combined[column]/combined['Total'])*100\n",
    "\n",
    "    \n",
    "combined.to_csv('stats_kraken.csv')\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_level_validation = pd.read_csv('Output/ValidationTopLevelGroups_MoreThan80.csv')\n",
    "\n",
    "\n",
    "#Some relabeling\n",
    "sample_statistics_relabeling = sample_statistics\n",
    "#rename healthy to control\n",
    "sample_statistics_relabeling['timephase'] = sample_statistics_relabeling['timephase'].apply(\n",
    "    lambda x : 'Control' if x == 'Healthy' else x)\n",
    "sample_statistics_relabeling['timephase'] = sample_statistics_relabeling['timephase'].apply(\n",
    "    lambda x : 'Pre-Tx' if x == 'Pre TX' else x)\n",
    "sample_statistics_relabeling['timephase'] = sample_statistics_relabeling['timephase'].apply(\n",
    "    lambda x : 'Leukopenia' if x == 'Leukozytopenia' else x)\n",
    "\n",
    "sample_statistics_relabeling['samplename'] = sample_statistics_relabeling['PatID']+'_'+sample_statistics_relabeling['time'].astype(str)\n",
    "\n",
    "domain_like_groups=['Microbiome','Human','Plants','Unclassified']\n",
    "x = sample_statistics_relabeling[\n",
    "    ['timephase',\n",
    "     'Unclassified_%','Human_%','Plants_%','Microbiome_%',\n",
    "     'samplename']\n",
    "]\n",
    "\n",
    "x = x.melt(id_vars=['samplename','timephase'])\n",
    "\n",
    "x[['domain','percent_sign']] = x['variable'].str.split('_',expand=True)\n",
    "x = x.drop(columns=['percent_sign'])\n",
    "x = x.pivot(index=['samplename','timephase'],columns='domain',values='value')\n",
    "x = x.reset_index()\n",
    "x = x.melt(id_vars=['samplename','timephase'])\n",
    "x = pd.merge(x,top_level_validation,\n",
    "             left_on=['samplename','domain'],right_on=['Sample','Group'],how='left').fillna('Not an outlier')\n",
    "\n",
    "\n",
    "c1 = alt.Chart(sample_statistics_relabeling,width=100,height=100).mark_boxplot(\n",
    "    ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'timephase:O',\n",
    "        header=alt.Header(labels=False),\n",
    "        spacing=0,\n",
    "        sort=['Control', 'Pre-Tx','Leukopenia','Reconstitution'],\n",
    "        title=None\n",
    "    ),\n",
    "    y=alt.Y('Microgram DNA per g Stool', axis=alt.Axis(grid=False,minExtent=40), title=['DNA [µg]','Per g Stool']),\n",
    "\n",
    "    )\n",
    "\n",
    "c2 = alt.Chart(sample_statistics_relabeling,width=100,height=100).mark_boxplot(\n",
    "    ticks=True,median={'color':'black'}\n",
    ").encode(\n",
    "    column=alt.Column(\n",
    "        'timephase:O',\n",
    "        header=alt.Header(labels=False),\n",
    "        spacing=0,\n",
    "        sort=['Control', 'Pre-Tx','Leukopenia','Reconstitution'],\n",
    "        title=None\n",
    "    ),\n",
    "     y=alt.Y('Total Reads per DNA Library:Q',scale=alt.Scale(type='log'),\n",
    "             axis=alt.Axis(grid=False,tickCount=10, format='.1e',minExtent=40),title='Total Reads')\n",
    "    )\n",
    "\n",
    "c2_2 = alt.Chart(sample_statistics_relabeling,width=100,height=100).mark_boxplot(\n",
    "    ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'timephase:O',\n",
    "        header=alt.Header(labels=False),\n",
    "        spacing=0,\n",
    "        sort=['Control', 'Pre-Tx','Leukopenia','Reconstitution'],\n",
    "        title=None\n",
    "    ),\n",
    "     y=alt.Y('Median Read Length:Q',scale=alt.Scale(type='log'),\n",
    "             axis=alt.Axis(grid=False,tickCount=10, format='.1e',minExtent=40),title='Median Read Length')\n",
    "    )\n",
    "\n",
    "c3 = alt.Chart(x,width=100).mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'timephase:O',\n",
    "        spacing=0,\n",
    "        sort=['Control', 'Pre-Tx','Leukopenia','Reconstitution'],\n",
    "        title=None,\n",
    "        header=alt.Header(labelOrient='bottom', labelPadding=313)\n",
    "    ),\n",
    "     x=alt.X(\"domain:O\", title=None, axis=alt.Axis(labels=False, ticks=False), \n",
    "             scale=alt.Scale(paddingInner=1), sort=domain_like_groups),    \n",
    "    y=alt.Y(\"value:Q\",title=['Fraction','[% Reads]'],\n",
    "            scale=alt.Scale(type='symlog',domain=[0,100]),\n",
    "            axis=alt.Axis(grid=False,minExtent=40,values=[0.1,0.5,1,5,10,20,50,100])), \n",
    "    color=alt.condition(\n",
    "        (alt.datum['Validated'] != False),\n",
    "        alt.Color(\"domain:N\",sort=domain_like_groups,legend=alt.Legend(orient='right',\n",
    "                                                                       symbolType='square',title='Reads',\n",
    "                                                                      symbolStrokeWidth=0,symbolOpacity=1),\n",
    "                    scale=alt.Scale(domain=domain_like_groups,\n",
    "                                    range=['#dd1c77', '#fecc5c', '#006837', '#253494'])),\n",
    "                alt.value('lightgrey')\n",
    "    )\n",
    "    ,tooltip=['samplename', 'value']\n",
    ")\n",
    "\n",
    "\n",
    "chart1=(c1&c2&c2_2&c3).configure_tick(thickness=2)\n",
    "\n",
    "chart1.save('Output/Composition/High_Level_Metrics.html')\n",
    "\n",
    "chart1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some relabeling\n",
    "sample_statistics_relabeling = sample_statistics\n",
    "#rename healthy to control\n",
    "sample_statistics_relabeling['timephase'] = sample_statistics_relabeling['timephase'].apply(\n",
    "    lambda x : 'Control' if x == 'Healthy' else x)\n",
    "sample_statistics_relabeling['timephase'] = sample_statistics_relabeling['timephase'].apply(\n",
    "    lambda x : 'Pre-Tx' if x == 'Pre TX' else x)\n",
    "sample_statistics_relabeling['timephase'] = sample_statistics_relabeling['timephase'].apply(\n",
    "    lambda x : 'Leukopenia' if x == 'Leukozytopenia' else x)\n",
    "\n",
    "\n",
    "domain_like_groups=['Bacteria','Fungi','OtherEukaryota','Viruses','Archaea']\n",
    "z = sample_statistics_relabeling[\n",
    "    ['timephase',\n",
    "     'Bacteria_%','Fungi_%','OtherEukaryota_%','Viruses_%','Archaea_%',\n",
    "     'samplename']\n",
    "]\n",
    "z = z.melt(id_vars=['samplename','timephase'])\n",
    "z[['domain','variable']] = z['variable'].str.split('_',expand=True)\n",
    "z = z.drop(columns=['variable'])\n",
    "z = z.pivot(index=['samplename','timephase'],columns='domain',values='value')\n",
    "z = z.reset_index()\n",
    "z = z.melt(id_vars=['samplename','timephase'])\n",
    "z = pd.merge(\n",
    "    z,top_level_validation,\n",
    "             left_on=['samplename','domain'],right_on=['Sample','Group'],how='left')\n",
    "\n",
    "\n",
    "\n",
    "c5 = alt.Chart(sample_statistics_relabeling,width=100,height=100).mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'timephase:O',\n",
    "        header=alt.Header(labels=False),\n",
    "        spacing=0,\n",
    "        sort=['Control', 'Pre-Tx','Leukopenia','Reconstitution'],\n",
    "        title=None\n",
    "    ),\n",
    "    y=alt.Y('Detected Species (Validated and Normalized)', axis=alt.Axis(grid=False,minExtent=40), title=['Detected Species','(Validated and Normalized)'])\n",
    "    )\n",
    "\n",
    "c6 =alt.Chart(sample_statistics_relabeling,width=100,height=100).mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'timephase:O',\n",
    "        spacing=0,\n",
    "        sort=['Control', 'Pre-Tx','Leukopenia','Reconstitution'],\n",
    "        title=None,\n",
    "        header=alt.Header(labels=False)\n",
    "    ),\n",
    "    y=alt.Y('ARG Reads per 10000 Reads:Q', \n",
    "            title=['ARG-carrying Reads','Per 10,000 Reads'],\n",
    "            scale=alt.Scale(type='symlog'), axis=alt.Axis(grid=False,minExtent=40))\n",
    "    )\n",
    "\n",
    "\n",
    "c7 = alt.Chart(z,width=100).mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'timephase:O',\n",
    "        spacing=0,\n",
    "        sort=['Control', 'Pre-Tx','Leukopenia','Reconstitution'],\n",
    "        title=None,\n",
    "        header=alt.Header(labelOrient='bottom', labelPadding=313)\n",
    "    ),\n",
    "    tooltip=['samplename'],\n",
    "    x=alt.X(\"domain:O\", title=None, \n",
    "            axis=alt.Axis(labels=False, ticks=False),\n",
    "            scale=alt.Scale(paddingInner=1), sort=domain_like_groups),    \n",
    "    y=alt.Y(\"value:Q\",title=['Fraction','[% Reads]'],scale=alt.Scale(type='symlog',domain=[0,100]),\n",
    "            axis=alt.Axis(grid=False,minExtent=40, values=[0.1,0.5,1,5,10,20,50,100])), \n",
    "    color=alt.condition(\n",
    "        (alt.datum['Validated'] != False),\n",
    "        alt.Color(\n",
    "            \"domain:N\",\n",
    "            sort=domain_like_groups,\n",
    "            legend=alt.Legend(title='Microbial Reads',orient='right',symbolType='square', symbolStrokeWidth=0,symbolOpacity=1),\n",
    "            scale=alt.Scale(domain=domain_like_groups,range=['#EE6677','#fecc5c','#228833', '#66CCEE', '#AA3377'])\n",
    "        ),\n",
    "        alt.value('lightgrey')\n",
    "    )\n",
    ")\n",
    "\n",
    "chart2=(c5&c6&c7).configure_tick(thickness=2)\n",
    "\n",
    "chart2.save(\n",
    "    'Output/Composition/High_Level_Metrics_2.html')\n",
    "\n",
    "chart2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.groupby(['domain','timephase'])['value'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation DNA / Reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(sample_statistics).mark_point().encode(\n",
    "    x='Microgram DNA per g Stool',\n",
    "    y='Total Reads per DNA Library:Q'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Criterion\n",
    "A quick overview of how many samples pass a certain threshold of reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for THRESHOLD in [10000,50000,100000]:\n",
    "    print(THRESHOLD,len(sample_statistics[sample_statistics['Total Reads'] > THRESHOLD]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Level Statistics P-Values <a class=\"anchor\" id=\"pvalues\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare each group with respect to a specific property and calculate a mannwhitney-u test to determine if one distribution is \"larger\" than the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_columns = ['Microgram DNA per g Stool',\n",
    "        'Total Reads', \n",
    "       'Median Read Length',\n",
    "       'Unclassified_%', 'Human_%',\n",
    "                'Detected Species (Validated and Normalized)',\n",
    "                'ARG Reads per 10000 Reads']\n",
    "\n",
    "group_column = 'timephase'\n",
    "\n",
    "tuples_a = []\n",
    "for test_column in test_columns:\n",
    "    for group1,group2 in it.combinations(sample_statistics[group_column].unique(),2):\n",
    "        result = mannwhitneyu(\n",
    "            sample_statistics[sample_statistics[group_column] == group1][test_column],\n",
    "            sample_statistics[sample_statistics[group_column] == group2][test_column]  \n",
    "        )\n",
    "        tuples_a.append((group1,group2,test_column,result.pvalue))\n",
    "    \n",
    "df_a = pd.DataFrame(tuples_a,columns=['Group A','Group B','Category','p-Value Whitney U'])\n",
    "df_a = df_a.set_index(['Category','Group A','Group B'])\n",
    "\n",
    "#Calculate Significances\n",
    "SIGNIFICANCES = [0.05,0.01,0.001]\n",
    "#We correct for the number of experiments\n",
    "for idx,significance in enumerate(SIGNIFICANCES):\n",
    "    corrected_significance = significance/len(df_a)\n",
    "    df_a['{} Significance {} (Corrected for test count: {:.2E})'.format(\n",
    "        '*'*(idx+1),significance,corrected_significance\n",
    "    )] = df_a['p-Value Whitney U'] < corrected_significance\n",
    "\n",
    "df_a.to_csv('Output/Whitney_{}.csv'.format(group_column))\n",
    "df_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the column which was tested we will also track Mean/Min/Max + Std. Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_column = 'Median Readlength'\n",
    "\n",
    "tuples_b = []\n",
    "for group in sample_statistics[group_column].unique():\n",
    "    tuples_b.append((\n",
    "        group,\n",
    "        sample_statistics[sample_statistics[group_column] == group][test_column].mean(),\n",
    "        sample_statistics[sample_statistics[group_column] == group][test_column].std(),\n",
    "        sample_statistics[sample_statistics[group_column] == group][test_column].min(),\n",
    "        sample_statistics[sample_statistics[group_column] == group][test_column].max()\n",
    "        \n",
    "    ))\n",
    "df_b = pd.DataFrame(tuples_b,columns=['Group','Mean','Std. Deviation','Minimum','Maximum'])\n",
    "df_b.to_csv('Output/MeanAndStd_{}_{}.csv'.format(test_column,group_column))\n",
    "df_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxa Counts (Diversity) Per Level <a class=\"anchor\" id=\"taxacount\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES = PAPER_SAMPLES\n",
    "EXCLUDE = [CHORDATA, PLANTAE]\n",
    "############################\n",
    "\n",
    "os.makedirs('Output/Diversity',exist_ok=True)\n",
    "\n",
    "for normalized in [True,False] :\n",
    "    charts = []\n",
    "\n",
    "    for taxonomic_level in ['S','G']:\n",
    "        table = get_normalized_abundances(\n",
    "            kraken_dataframe,\n",
    "            level=taxonomic_level,\n",
    "            excluded_taxa_filter=EXCLUDE,\n",
    "            samples=[s.replace('/','_') for s in SAMPLES], #New sample format uses underscore '_' instead of '/'\n",
    "            normalize=normalized \n",
    "        )\n",
    "\n",
    "        table = apply_validation(table,level=taxonomic_level,validation_cutoff=0.2,readcount_cutoff=5)\n",
    "\n",
    "        table = table[table['taxonid']!='-2']\n",
    "\n",
    "        taxaCounts = table.groupby(['samplename'],as_index=False)['taxonid'].count().rename(\n",
    "            columns={'taxonid' : '{}_{}'.format(taxonomic_level,'Normalized')}\n",
    "        )\n",
    "\n",
    "        charts.append(taxaCounts)\n",
    "\n",
    "\n",
    "    reduce(lambda x,y : pd.merge(x,y,how='left',on=['samplename']),charts).to_csv(\n",
    "        'Output/Diversity/taxa_counts_{}-Without_{}_Normalized={}.csv'.format(\n",
    "            hash(str(SAMPLES)),\n",
    "            str(EXCLUDE),\n",
    "            normalized\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Sample Statistics Table with calculated diversity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity = pd.read_csv('Output/Diversity/taxa_counts_{}-Without_{}_Normalized={}.csv'.format(\n",
    "hash(str(SAMPLES)),\n",
    "    [CHORDATA,PLANTAE],\n",
    "    True\n",
    ")\n",
    "                       )[['S_Normalized','samplename']]\n",
    "if 'Detected Species (Validated and Normalized)' in sample_statistics.columns:\n",
    "    print('The column was already found in the table and will be replaced!')\n",
    "    sample_statistics = sample_statistics.drop(columns='Detected Species (Validated and Normalized)')\n",
    "sample_statistics['samplename'] = sample_statistics['PatID']+'_'+sample_statistics['time'].astype(str)\n",
    "sample_statistics=pd.merge(sample_statistics,diversity,on='samplename',how='left')\n",
    "sample_statistics = sample_statistics.drop(columns='samplename').rename(columns={\n",
    "    'S_Normalized' : 'Detected Species (Validated and Normalized)'\n",
    "})\n",
    "sample_statistics.to_excel('Input/Annotations/SampleStatistics.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize the precalculated diversity values (calculated above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table= sample_statistics[sample_statistics['timephase']=='Pre-Tx']\n",
    "print(table.groupby('Startcluster')['Detected Species (Validated and Normalized)'].median())\n",
    "alt.Chart(table).mark_boxplot().encode(\n",
    "    x='Startcluster:N',\n",
    "    y='Detected Species (Validated and Normalized)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barplots Compositions <a class=\"anchor\" id=\"barplots\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_X = 30\n",
    "LEVEL = 'S'\n",
    "DISCARD_CUTOFF = 5\n",
    "\n",
    "#Filters\n",
    "INCLUDE = None #Use 'None' to use root as top node\n",
    "EXCLUDE = [CHORDATA,PLANTAE]\n",
    "\n",
    "SAMPLES = PAPER_SAMPLES\n",
    "\n",
    "def SORTFUNCTION(x):\n",
    "    try:\n",
    "        return (\n",
    "            x[0] == 'G', #Sort by G or regular patient first\n",
    "            float(x.split('G')[-1].split('_')[0]), #Then Patient ID\n",
    "            int(x.split('G')[-1].split('_')[1]), #Then Time\n",
    "        )\n",
    "    except:\n",
    "        return (True,1,hash(x))\n",
    "    \n",
    "GROUPING = 'timephase_and_cluster'\n",
    "SORTING = 'Sample ID'\n",
    "\n",
    "NORMALIZE = False\n",
    "\n",
    "#################\n",
    "\n",
    "os.makedirs('Output/Composition',exist_ok=True)\n",
    "\n",
    "input_table = get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    level=LEVEL,\n",
    "    samples=[s.replace('/','_') for s in SAMPLES], #New sample format uses underscore '_' instead of '/'\n",
    "    included_taxa_filter=INCLUDE,\n",
    "    excluded_taxa_filter=EXCLUDE,\n",
    "    normalize=NORMALIZE\n",
    ")\n",
    "\n",
    "with_validation = apply_validation(\n",
    "    input_table,\n",
    "    level = 'S',\n",
    "    validation_cutoff = 0.2,\n",
    "    readcount_cutoff = 5\n",
    ")\n",
    "   \n",
    "\n",
    "#Calculate Read Fractions\n",
    "with_validation['Read Fraction'] = with_validation['readcount']/with_validation.groupby('samplename')['readcount'].transform('sum')\n",
    "\n",
    "total_reads = with_validation.groupby('samplename',as_index=False)['readcount'].sum()\n",
    "sample_statistics['samplename']= sample_statistics['PatID']+'_'+sample_statistics['time'].astype(str)\n",
    "total_reads = pd.merge(total_reads,sample_statistics[['samplename','Classified Reads']],on='samplename',how='left').fillna(0)\n",
    "total_reads['Total Fraction'] = total_reads['readcount']/total_reads['Classified Reads']\n",
    "total_reads['Sample ID'] = total_reads['samplename']\n",
    "\n",
    "# determine the top taxa based on means\n",
    "taxa_we_look_at = list(with_validation.groupby('taxon')['Read Fraction'].sum().sort_values(ascending=False)[:(TOP_X+2)].keys())\n",
    "if 'Not enough reads' not in taxa_we_look_at:\n",
    "    taxa_we_look_at.append('Not enough reads')\n",
    "if 'Not validated' not in taxa_we_look_at:\n",
    "    taxa_we_look_at.append('Not validated')\n",
    "print('Determined the following taxa as relevant:',taxa_we_look_at)\n",
    "\n",
    "#assign everything else to the \"other\" group and readjust sum\n",
    "with_validation.loc[~with_validation['taxon'].isin(taxa_we_look_at), 'taxon'] = 'Other'\n",
    "with_validation = with_validation.groupby(['taxon','samplename'],as_index=False).sum()\n",
    "\n",
    "\n",
    "with_validation['other'] = with_validation['taxon'] == 'Other'\n",
    "\n",
    "with_validation = with_validation.rename(columns={\n",
    "    'samplename' : 'Sample ID',\n",
    "    'taxon' : 'Taxon'\n",
    "})\n",
    "\n",
    "colorMap = {}\n",
    "\n",
    "taxa = taxa_we_look_at+['Other']\n",
    "\n",
    "palette = cc.glasbey_light\n",
    "bright = palette[::2]\n",
    "muted = palette[1::2]\n",
    "palette = bright+muted\n",
    "\n",
    "taxa_we_look_at_assigned = taxa_we_look_at\n",
    "\n",
    "taxa_we_look_at_assigned.remove('Not validated')\n",
    "taxa_we_look_at_assigned.remove('Not enough reads')\n",
    "\n",
    "for tax,col in zip(taxa_we_look_at_assigned,palette):\n",
    "    colorMap[tax] = col #colors.to_hex(col)\n",
    "    \n",
    "altdomain = []\n",
    "altrange = []\n",
    "\n",
    "for x in taxa_we_look_at_assigned:\n",
    "\n",
    "    c = colorMap[x]\n",
    "    altdomain.append(x)\n",
    "    color = colors.to_hex(c)\n",
    "    altrange.append(color)\n",
    "    \n",
    "altdomain.append('Other')\n",
    "altrange.append(colors.to_hex((1,1,1)))\n",
    "altdomain.append('Not enough reads')\n",
    "altrange.append(colors.to_hex((0.15,0.15,0.15)))\n",
    "altdomain.append('Not validated')\n",
    "altrange.append(colors.to_hex((0.55,0.55,0.55)))\n",
    "\n",
    "with_validation = pd.merge(\n",
    "    with_validation,\n",
    "    sample_statistics,\n",
    "    how='left',\n",
    "    left_on=['Sample ID'],\n",
    "    right_on=['samplename'])\n",
    "with_validation[GROUPING]=with_validation[GROUPING].fillna('Unknown Group')\n",
    "\n",
    "maxfraction = total_reads['Total Fraction'].max()\n",
    "\n",
    "colorMap\n",
    "\n",
    "charts = []\n",
    "\n",
    "title_mapping = {\n",
    "    '1_Gesund' : 'Control',\n",
    "    'pre_1' : 'Cluster 1',\n",
    "    'pre_2' : 'Cluster 2',\n",
    "    'pre_3' : 'Cluster 3',\n",
    "    'leukozytopenia_1' : ['Pre-Tx','Cluster 1'],\n",
    "    'leukozytopenia_2' : ['Pre-Tx','Cluster 2'],\n",
    "    'leukozytopenia_3' : ['Pre-Tx','Cluster 3'],\n",
    "    'reconstitution_1' : ['Pre-Tx','Cluster 1'],\n",
    "    'reconstitution_2' : ['Pre-Tx','Cluster 2'],\n",
    "    'reconstitution_3' : ['Pre-Tx','Cluster 3']\n",
    "\n",
    "}\n",
    "\n",
    "groups = with_validation[GROUPING].unique() if GROUPING != None else [None]\n",
    "print('Order of charts:')\n",
    "\n",
    "for group in groups:\n",
    "    \n",
    "    print(group)\n",
    "    grouptable = None\n",
    "    if group != None:\n",
    "        #Reduce to required columns to keep output reasonably small\n",
    "        grouptable = with_validation[with_validation[GROUPING] == group]\n",
    "    else:\n",
    "        grouptable = with_validation\n",
    "    \n",
    "    patientlist_sorted = None\n",
    "    if isinstance(SORTING,list):#If we provide a list (manual sorting) use this\n",
    "        patientlist_sorted = SORTING\n",
    "    else:\n",
    "        patientlist_sorted = sorted(\n",
    "            grouptable[SORTING].unique().tolist(),\n",
    "            key=lambda x : SORTFUNCTION(x)\n",
    "        )\n",
    "    \n",
    "        chart = alt.Chart(\n",
    "          grouptable,title=title_mapping[group]\n",
    "      ).transform_calculate(\n",
    "      order=f\"-indexof({altdomain}, datum.Taxon)\"\n",
    "        ).mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "            x=alt.X('Sample ID:N',sort=patientlist_sorted, axis=alt.Axis(labels=False),title=None),\n",
    "            y=alt.Y('Read Fraction:Q',scale=alt.Scale(\n",
    "                domain=(0,1)),title=['Estimated', 'Abundance']\n",
    "                   ),\n",
    "            color=alt.Color('Taxon:N',\n",
    "                            legend=alt.Legend(columns=1,symbolLimit=0,labelLimit=0,rowPadding=15),\n",
    "                            sort=taxa,\n",
    "                            scale=alt.Scale(domain=altdomain,range=altrange)),\n",
    "            tooltip=['readcount','Read Fraction','Taxon'],\n",
    "            order=alt.Order('order:Q')\n",
    "      )& alt.Chart(grouptable).mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "        x=alt.X('Sample ID:N',sort=patientlist_sorted,title=None),\n",
    "        y=alt.Y('ARG Reads per 10000 Reads',scale=alt.Scale(domain=[0,350]),title=['ARG-carrying Reads','Per 10,000 Reads'])\n",
    "    )\n",
    "\n",
    "    charts.append(\n",
    "        chart\n",
    "         \n",
    "        \n",
    "    )\n",
    "\n",
    "chart = reduce(lambda x,y : x&y, charts).configure_axis(\n",
    "    labelFontSize=16, titleFontSize=16\n",
    ").configure_title(fontSize=20).configure_legend(titleFontSize=20, labelFontSize=16)\n",
    "\n",
    "chart.save(\n",
    "    'Output/Composition/Barplots-Top_{}-{}-Only_{}-Without_{}-{}_{}_{}.html'.format(\n",
    "        TOP_X,\n",
    "        LEVEL,\n",
    "        INCLUDE,\n",
    "        str(EXCLUDE),\n",
    "        hash(str(SAMPLES)),\n",
    "        hash(str(SORTING)) if isinstance(SORTING,list) else SORTING,\n",
    "        GROUPING\n",
    "    )\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section Pre-Tx alloHSCT microbiomes could be grouped into 3 distinct clusters:\n",
    "Sort by timephase and group and check for the dominating species which abundance they take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presence above threshold in sample groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE= None\n",
    "EXCLUDE = [CHORDATA,PLANTAE]\n",
    "\n",
    "sample_statistics['samplename'] = sample_statistics['PatID']+'_'+sample_statistics['time'].astype(str)\n",
    "\n",
    "#Refetch data (no other)\n",
    "\n",
    "input_table = get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    level=LEVEL,\n",
    "    samples=[s.replace('/','_') for s in SAMPLES], #New sample format uses underscore '_' instead of '/'\n",
    "    included_taxa_filter=INCLUDE,\n",
    "    excluded_taxa_filter=EXCLUDE,\n",
    "    normalize=NORMALIZE\n",
    ")\n",
    "\n",
    "with_validation = apply_validation(\n",
    "    input_table,\n",
    "    level = 'S',\n",
    "    validation_cutoff = 0.2,\n",
    "    readcount_cutoff = 5\n",
    ")\n",
    "   \n",
    "\n",
    "#Calculate Read Fractions\n",
    "with_validation['Read Fraction'] = with_validation['readcount']/with_validation.groupby('samplename')['readcount'].transform('sum')\n",
    "\n",
    "with_validation = pd.merge(\n",
    "    with_validation,\n",
    "    sample_statistics,\n",
    "    how='left',\n",
    "    left_on=['samplename'],\n",
    "    right_on=['samplename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = with_validation.groupby(['taxon','timephase_and_cluster'])['Read Fraction'].mean().reset_index().pivot(\n",
    "    index='taxon',\n",
    "    columns='timephase_and_cluster',\n",
    "    values='Read Fraction'\n",
    ")\n",
    "t1['Sum'] = t1.sum(axis=1)\n",
    "t1 = t1.sort_values(by='Sum',ascending=False)\n",
    "t1.to_csv('Output/meanabundances_timephase_and_cluster.csv')\n",
    "\n",
    "t2 = with_validation.groupby(['taxon','timephase'])['Read Fraction'].mean().reset_index().pivot(\n",
    "    index='taxon',\n",
    "    columns='timephase',\n",
    "    values='Read Fraction'\n",
    ")\n",
    "t2['Sum'] = t2.sum(axis=1)\n",
    "t2 = t2.sort_values(by='Sum',ascending=False)\n",
    "t2.to_csv('Output/meanabundances_timephase.csv')\n",
    "\n",
    "t3 = with_validation.groupby(['taxon','Startcluster'])['Read Fraction'].mean().reset_index().pivot(\n",
    "    index='taxon',\n",
    "    columns='Startcluster',\n",
    "    values='Read Fraction'\n",
    ")\n",
    "t3['Sum'] = t3.sum(axis=1)\n",
    "t3 = t3.sort_values(by='Sum',ascending=False)\n",
    "t3.to_csv('Output/meanabundances_Startcluster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = []\n",
    "\n",
    "species_of_interest = ['Enterococcus faecium']\n",
    "thresholds = [0.05,0.1,0.2,0.25,0.5]\n",
    "for timephase_and_cluster in with_validation['timephase_and_cluster'].unique():\n",
    "\n",
    "    tc_table = with_validation[with_validation['timephase_and_cluster'] == timephase_and_cluster]\n",
    "    \n",
    "    for species in species_of_interest:\n",
    "        \n",
    "        sp_table = tc_table[tc_table['taxon'] == species]\n",
    "\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "\n",
    "            nr_of_samples = len(sp_table)\n",
    "            nr_of_occurences = len(sp_table[sp_table['Read Fraction'] > threshold])\n",
    "            \n",
    "            tuples.append((timephase_and_cluster,species,threshold,nr_of_samples,nr_of_occurences,nr_of_occurences/nr_of_samples if nr_of_samples != 0 else 'NaN'))\n",
    "\n",
    "for timephase in with_validation['timephase'].unique():\n",
    "\n",
    "    tc_table = with_validation[with_validation['timephase'] == timephase]\n",
    "    \n",
    "    for species in species_of_interest:\n",
    "        \n",
    "        sp_table = tc_table[tc_table['taxon'] == species]\n",
    "\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "\n",
    "            nr_of_samples = len(sp_table)\n",
    "            nr_of_occurences = len(sp_table[sp_table['Read Fraction'] > threshold])\n",
    "            \n",
    "            tuples.append((timephase,species,threshold,nr_of_samples,nr_of_occurences,nr_of_occurences/nr_of_samples if nr_of_samples != 0 else 'NaN'))\n",
    "\n",
    "            \n",
    "for cluster in with_validation['Startcluster'].unique():\n",
    "\n",
    "    tc_table = with_validation[with_validation['Startcluster'] == cluster]\n",
    "    \n",
    "    for species in species_of_interest:\n",
    "        \n",
    "        sp_table = tc_table[tc_table['taxon'] == species]\n",
    "\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "\n",
    "            nr_of_samples = len(sp_table)\n",
    "            nr_of_occurences = len(sp_table[sp_table['Read Fraction'] > threshold])\n",
    "            \n",
    "            tuples.append((cluster,species,threshold,nr_of_samples,nr_of_occurences,nr_of_occurences/nr_of_samples if nr_of_samples != 0 else 'NaN'))\n",
    "            \n",
    "tc_table = with_validation\n",
    "\n",
    "for species in species_of_interest:\n",
    "\n",
    "    sp_table = tc_table[tc_table['taxon'] == species]\n",
    "\n",
    "\n",
    "    for threshold in thresholds:\n",
    "\n",
    "        nr_of_samples = len(sp_table)\n",
    "        nr_of_occurences = len(sp_table[sp_table['Read Fraction'] > threshold])\n",
    "\n",
    "        tuples.append(('Full',species,threshold,nr_of_samples,nr_of_occurences,nr_of_occurences/nr_of_samples if nr_of_samples != 0 else 'NaN'))\n",
    "\n",
    "\n",
    "\n",
    "pd.DataFrame(tuples,columns=[\n",
    "    'Timephase and Cluster','Species','Threshold','Total Samples','Samples With Presence','Fraction'\n",
    "]).to_csv('Output/PresenceInGroups.csv',index=False)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SORTFUNCTION(x):\n",
    "    try:\n",
    "        return (\n",
    "            x[0] == 'G', #Sort by G or regular patient first\n",
    "            float(x.split('G')[-1].split('_')[0]), #Then Patient ID\n",
    "            int(x.split('G')[-1].split('_')[1]), #Then Time\n",
    "        )\n",
    "    except:\n",
    "        return (True,1,hash(x))\n",
    "\n",
    "with_validation_pivot = with_validation.pivot(index=['samplename'],columns='taxon',values='Read Fraction')\n",
    "\n",
    "with_validation_pivot=with_validation_pivot.fillna(0).transpose()\n",
    "sorted_columns = sorted(with_validation_pivot.columns,key=SORTFUNCTION)\n",
    "\n",
    "with_validation_pivot['Sum'] = with_validation_pivot.sum(axis=1)\n",
    "with_validation_pivot = with_validation_pivot.sort_values(by='Sum',ascending=False)\n",
    "\n",
    "with_validation_pivot = with_validation_pivot[sorted_columns]\n",
    "with_validation_pivot.to_csv('Output/BarplotsCompositionValues.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check correlation with ARG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_corr_table = with_validation.pivot(index=['samplename','ARG Reads per 10000 Reads'],columns='taxon',values='Read Fraction').fillna(0).reset_index()\n",
    "arg_corr_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_tuples = []\n",
    "arg_corr_table.corrwith(arg_corr_table['ARG Reads per 10000 Reads']).sort_values()#\n",
    "for column in arg_corr_table.columns:\n",
    "    if column != 'samplename':\n",
    "        corr,pval = pearsonr(arg_corr_table[column],arg_corr_table['ARG Reads per 10000 Reads'])\n",
    "        corr_tuples.append((column,corr,pval))\n",
    "        \n",
    "pd.DataFrame(corr_tuples,columns=['Correlate','Pearson Correlation','P-Value']).to_csv('pearsonCorrARGWithPVal.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional analysis: We check for Pre-TX samples in cluster 2 the highest abundance per taxon and subtract the highest abundance per taxon for the other patient clusters to identify unique features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c13 = with_validation[with_validation['timephase_and_cluster'].isin(['pre_1','pre_3'])]\n",
    "c2 = with_validation[with_validation['timephase_and_cluster'].isin(['pre_2'])]\n",
    "c2.groupby('Taxon')['Read Fraction'].max()-c13.groupby('Taxon')['Read Fraction'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We track for all taxa that have an abundance over 1/4 in how many samples they occur in cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2[c2['Read Fraction'] > 0.25].groupby('Taxon')['samplename'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionaly the maximum read fractions are displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2.groupby('Taxon')['Read Fraction'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to also know how many species we need to explain 50% reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_tuples = []\n",
    "\n",
    "for timephase_and_cluster in with_validation['timephase_and_cluster'].unique():\n",
    "    subtable = with_validation[with_validation['timephase_and_cluster'] == timephase_and_cluster]\n",
    "    for sample in subtable['samplename'].unique():\n",
    "        sampletable = subtable[subtable['samplename']==sample]\n",
    "        explained = 0\n",
    "        count = 0\n",
    "        for idx,row in sampletable.sort_values(by='Read Fraction',ascending=False).iterrows():\n",
    "            if row['taxon'] not in ['Other','Not validated','Not enough reads']:\n",
    "                explained += row['Read Fraction']\n",
    "                count += 1\n",
    "                if explained >= 0.5:\n",
    "                    div_tuples.append((timephase_and_cluster,sample,count))\n",
    "                    break\n",
    "        else:\n",
    "            #div_tuples.append((timephase_and_cluster,sample,-1))\n",
    "            pass\n",
    "pd.DataFrame(div_tuples,columns=['timephase_and_cluster','samplename','count']).groupby('timephase_and_cluster').median()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leukopenia-Loss\n",
    "We analyze here how much of each taxon is lost comparing leukozytopenia -> pre tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(with_validation[\n",
    "    with_validation['timephase']=='Leukozytopenia'\n",
    "].groupby('Taxon')['Read Fraction'].sum()/len(with_validation[\n",
    "    with_validation['timephase']=='Leukozytopenia'\n",
    "]['samplename'].unique())-with_validation[\n",
    "    with_validation['timephase']=='Pre TX'\n",
    "].groupby('Taxon')['Read Fraction'].sum()/len(with_validation[\n",
    "    with_validation['timephase']=='Pre TX'\n",
    "]['samplename'].unique())).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in it.permutations(with_validation['timephase'].unique(),2):\n",
    "    ca = with_validation[with_validation['timephase']==a]\n",
    "    cb = with_validation[with_validation['timephase']==b]\n",
    "    diffs = (ca.groupby('Taxon')['Read Fraction'].max()-cb.groupby('Taxon')['Read Fraction'].max()).sort_values(ascending=True)\n",
    "    combined = pd.concat([\n",
    "        ca[ca['Read Fraction'] > 0.25].groupby('Taxon')['samplename'].count().rename('Above 25% {}'.format(a)),\n",
    "        ca.groupby('Taxon')['samplename'].count().rename('Any% {}'.format(a))\n",
    "  \n",
    "    ],axis=1).fillna(0).reset_index()\n",
    "    combined['Above 25% {}'.format(a)] = combined['Above 25% {}'.format(a)].astype(int)\n",
    "    combined = pd.merge(diffs,combined,how='outer',on='Taxon')\n",
    "    combined = combined.rename(columns={'Read Fraction':'Diff Max {} Minus {}'.format(a,b)})\n",
    "    combined.to_excel('{}_Minus_{}.xlsx'.format(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_statistics[['samplename','leukocytephase_cluster_2_kurz']].groupby('leukocytephase_cluster_2_kurz').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_1 = with_validation.groupby(['samplename','timephase'],as_index=False)['Read Fraction'].max()\n",
    "pd.concat([\n",
    "    dom_1.groupby('leukocytephase_cluster_kurz')['samplename'].count().rename('n'),\n",
    "    dom_1[dom_1['Read Fraction'] > 0.5].groupby('leukocytephase_cluster_kurz')['sample'].count()/dom_1.groupby('leukocytephase_cluster_kurz')['Sample ID'].count()\n",
    "],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occurence of Species across Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_X = 20\n",
    "LEVEL = 'S'\n",
    "\n",
    "READCOUNT_CUTOFF = 5\n",
    "VALIDATION_CUTOFF = 0.2\n",
    "PRESENCE_CUTOFF = 0.05\n",
    "\n",
    "#Filters\n",
    "GROUPS = [\n",
    "    (BACTERIA,[]),\n",
    "    (FUNGI,[]),\n",
    "    (EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]),\n",
    "    (VIRUSES,[]),\n",
    "    (ARCHAEA,[]),\n",
    "    (PLANTAE,[]),\n",
    "    (CHORDATA,[]),\n",
    "    ('1',[CHORDATA,PLANTAE])\n",
    "]\n",
    "\n",
    "SAMPLES = PAPER_SAMPLES\n",
    "\n",
    "def SORTFUNCTION(x):\n",
    "    #return x\n",
    "    return (\n",
    "        x[0] == 'G', #Sort by G or regular patient first\n",
    "        float(x.split('G')[-1].split('/')[0]), #Then Patient ID\n",
    "        int(x.split('G')[-1].split('/')[1]), #Then Time\n",
    "    )\n",
    "\n",
    "GROUPING = 'leukocytephase_cluster_kurz'\n",
    "SORTING = 'samplename'\n",
    "NORMALIZE = False\n",
    "#################\n",
    "\n",
    "os.makedirs('Output/Presence',exist_ok=True)\n",
    "\n",
    "#################\n",
    "\n",
    "for INCLUDE,EXCLUDE in GROUPS:\n",
    "\n",
    "    input_table = get_normalized_abundances(\n",
    "        kraken_dataframe,\n",
    "        level=LEVEL,\n",
    "        samples=[s.replace('/','_') for s in SAMPLES],\n",
    "        included_taxa_filter=INCLUDE,\n",
    "        excluded_taxa_filter=EXCLUDE,\n",
    "        normalize=NORMALIZE\n",
    "    )\n",
    "\n",
    "    input_table = apply_validation(input_table,\n",
    "                                   level=LEVEL,\n",
    "                                   validation_cutoff=VALIDATION_CUTOFF,\n",
    "                                   readcount_cutoff=READCOUNT_CUTOFF\n",
    "                                  )\n",
    "    \n",
    "    input_table['Read Fraction'] = input_table['readcount'] / input_table.groupby(\n",
    "        ['samplename']\n",
    "    )['readcount'].transform('sum')\n",
    "\n",
    "    sample_statistics['samplename']= sample_statistics['PatID']+'_'+sample_statistics['time'].astype(str)\n",
    "\n",
    "    merged = pd.merge(input_table,sample_statistics,how='left',left_on=['samplename'],right_on=['samplename'])\n",
    "\n",
    "    subset_taxa = merged[merged['Read Fraction'] >= PRESENCE_CUTOFF]['taxon'].unique()\n",
    "    subset = merged[\n",
    "        (merged['taxon'].isin(subset_taxa))&\n",
    "        (merged['Read Fraction'] >= PRESENCE_CUTOFF)\n",
    "    ]\n",
    "    \n",
    "    subset=subset[~subset['taxon'].isin(['Not enough reads','Not validated'])]\n",
    "    \n",
    "    charts = []\n",
    "    \n",
    "    for group in subset['timephase'].unique():\n",
    "        grouptable = (subset[\n",
    "            subset['timephase']==group\n",
    "        ].groupby(['taxon'])['samplename'].count()/len(\n",
    "                sample_statistics[\n",
    "                sample_statistics['timephase'] == group\n",
    "            ]\n",
    "        )).rename('Fraction {} (n={})'.format(\n",
    "            group,\n",
    "            len(sample_statistics[sample_statistics['timephase']==group])\n",
    "        ))\n",
    "        charts.append(grouptable)\n",
    "        \n",
    "    for group in subset['Startcluster'].unique():\n",
    "        grouptable = (subset[\n",
    "            subset['Startcluster']==group\n",
    "        ].groupby(['taxon'])['samplename'].count()/len(\n",
    "                sample_statistics[\n",
    "                sample_statistics['Startcluster'] == group\n",
    "            ]\n",
    "        )).rename('Fraction {} (n={})'.format(\n",
    "            group,\n",
    "            len(sample_statistics[sample_statistics['Startcluster']==group])\n",
    "        ))\n",
    "        charts.append(grouptable)\n",
    "        \n",
    "    totaltable = (subset.groupby(\n",
    "        ['taxon'])['samplename'].count()/112\n",
    "    ).rename('Fraction Total (n=112)')\n",
    "    charts.append(totaltable)\n",
    "    pd.concat(charts,axis=1).sort_values(by='Fraction Total (n=112)',ascending=False).to_excel(\n",
    "        'Output/Presence_Above_{}_{}_WITHOUT_{}.xlsx'.format(\n",
    "        PRESENCE_CUTOFF,\n",
    "        idtonames[INCLUDE],\n",
    "        list(idtonames[x] for x in EXCLUDE)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview all domains/groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_X = 30\n",
    "LEVEL = 'S'\n",
    "DISCARD_CUTOFF = 5\n",
    "DOMAINS = {\n",
    "    'Bacteriome':(BACTERIA,[]),\n",
    "    'Mycobiome':(FUNGI,[]),\n",
    "    'Archaeome':(ARCHAEA,[]),\n",
    "    'DNA-Virome':(VIRUSES,[]),\n",
    "    'Non-Fungal Eukaryome':(EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]),\n",
    "}\n",
    "\n",
    "SAMPLES = PAPER_SAMPLES\n",
    "\n",
    "NORMALIZE = False\n",
    "\n",
    "#################\n",
    "\n",
    "os.makedirs('Output/Composition',exist_ok=True)\n",
    "charts = {}\n",
    "raw_datasets = {}\n",
    "\n",
    "for domain in DOMAINS:\n",
    "    \n",
    "    print(domain)\n",
    "    \n",
    "    INCLUDE,EXCLUDE = DOMAINS[domain]\n",
    "\n",
    "    input_table = get_normalized_abundances(\n",
    "        kraken_dataframe,\n",
    "        level=LEVEL,\n",
    "        samples=[s.replace('/','_') for s in SAMPLES], #New sample format uses underscore '_' instead of '/'\n",
    "        included_taxa_filter=INCLUDE,\n",
    "        excluded_taxa_filter=EXCLUDE,\n",
    "        normalize=NORMALIZE\n",
    "    )\n",
    "\n",
    "    with_validation = apply_validation(\n",
    "        input_table,\n",
    "        level = 'S',\n",
    "        validation_cutoff = 0.2,\n",
    "        readcount_cutoff = 5\n",
    "    )\n",
    "\n",
    "    raw_datasets[domain] = with_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tables\n",
    "for domain in raw_datasets:\n",
    "    st = raw_datasets[domain]\n",
    "    st['Read Fraction'] = st['readcount']/st.groupby('samplename')['readcount'].transform('sum')\n",
    "    \n",
    "    stp = st.pivot(index=['samplename'],columns='taxon',values='Read Fraction')\n",
    "\n",
    "    stp=stp.fillna(0).transpose()\n",
    "    sorted_columns = sorted(stp.columns,key=SORTFUNCTION)\n",
    "\n",
    "    stp['Sum'] = stp.sum(axis=1)\n",
    "    stp = stp.sort_values(by='Sum',ascending=False)\n",
    "\n",
    "    stp = stp[sorted_columns]\n",
    "    stp.to_csv('Output/BarplotsFigure7Compositions_{}.csv'.format(domain))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SORTFUNCTION_SLASH(x):\n",
    "    try:\n",
    "        return (\n",
    "            x[0] == 'G', #Sort by G or regular patient first\n",
    "            float(x.split('G')[-1].split('/')[0]), #Then Patient ID\n",
    "            int(x.split('G')[-1].split('/')[1]), #Then Time\n",
    "        )\n",
    "    except:\n",
    "        return (True,1,hash(x))\n",
    "    \n",
    "SORTING = 'samplename'\n",
    "\n",
    "for domain in DOMAINS:\n",
    "    \n",
    "    with_validation = raw_datasets[domain].copy()\n",
    "\n",
    "    #Calculate Read Fractions\n",
    "    with_validation['Read Fraction'] = with_validation['readcount']/with_validation.groupby('samplename')['readcount'].transform('sum')\n",
    "\n",
    "    sample_statistics['samplename']= sample_statistics['PatID']+'_'+sample_statistics['time'].astype(str)\n",
    "\n",
    "    # determine the top taxa based on means\n",
    "    taxa_we_look_at = list(with_validation.groupby('taxon')['Read Fraction'].sum().sort_values(ascending=False)[:(TOP_X+2)].keys())\n",
    "    if 'Not enough reads' not in taxa_we_look_at:\n",
    "        taxa_we_look_at.append('Not enough reads')\n",
    "    if 'Not validated' not in taxa_we_look_at:\n",
    "        taxa_we_look_at.append('Not validated')\n",
    "    print('Determined the following taxa as relevant:',taxa_we_look_at)\n",
    "\n",
    "    #assign everything else to the \"other\" group and readjust sum\n",
    "    with_validation.loc[~with_validation['taxon'].isin(taxa_we_look_at), 'taxon'] = 'Other'\n",
    "    with_validation = with_validation.groupby(['taxon','samplename'],as_index=False).sum()\n",
    "\n",
    "\n",
    "    with_validation['other'] = with_validation['taxon'] == 'Other'\n",
    "\n",
    "    with_validation['Sample ID'] = with_validation['samplename'].str.replace('_','/')\n",
    "    \n",
    "    with_validation = with_validation.rename(columns={\n",
    "        'taxon' : 'Taxon'\n",
    "    })\n",
    "\n",
    "    colorMap = {}\n",
    "\n",
    "    taxa = taxa_we_look_at+['Other']\n",
    "\n",
    "    palette = cc.glasbey_light\n",
    "    bright = palette[::2]\n",
    "    muted = palette[1::2]\n",
    "    palette = bright+muted\n",
    "\n",
    "    taxa_we_look_at_assigned = taxa_we_look_at\n",
    "\n",
    "    taxa_we_look_at_assigned.remove('Not validated')\n",
    "    taxa_we_look_at_assigned.remove('Not enough reads')\n",
    "\n",
    "    for tax,col in zip(taxa_we_look_at_assigned,palette):\n",
    "        colorMap[tax] = col #colors.to_hex(col)\n",
    "\n",
    "    altdomain = []\n",
    "    altrange = []\n",
    "\n",
    "    for x in taxa_we_look_at_assigned:\n",
    "\n",
    "        c = colorMap[x]\n",
    "        altdomain.append(x)\n",
    "        color = colors.to_hex(c)\n",
    "        altrange.append(color)\n",
    "\n",
    "    altdomain.append('Other')\n",
    "    altrange.append(colors.to_hex((1,1,1)))\n",
    "    altdomain.append('Not enough reads')\n",
    "    altrange.append(colors.to_hex((0.15,0.15,0.15)))\n",
    "    altdomain.append('Not validated')\n",
    "    altrange.append(colors.to_hex((0.55,0.55,0.55)))\n",
    "\n",
    "    with_validation = pd.merge(\n",
    "        with_validation,\n",
    "        sample_statistics,\n",
    "        how='left',\n",
    "        left_on=['samplename'],\n",
    "        right_on=['samplename'])\n",
    "    \n",
    "\n",
    "    maxfraction = total_reads['Total Fraction'].max()\n",
    "\n",
    "    groups = ['1_Gesund','pre_1','pre_2','pre_3',\n",
    "              'reconstitution_1','reconstitution_2' ,'reconstitution_3' ,\n",
    "'leukozytopenia_1', 'leukozytopenia_2','leukozytopenia_3']\n",
    "\n",
    "    for group in groups:\n",
    "\n",
    "        grouptable = None\n",
    "        if group != None:\n",
    "            #Reduce to required columns to keep output reasonably small\n",
    "            grouptable = with_validation[with_validation['timephase_and_cluster'] == group]\n",
    "        else:\n",
    "            grouptable = with_validation\n",
    "\n",
    "\n",
    "        patientlist_sorted = sorted(\n",
    "            grouptable['Sample ID'].unique().tolist(),\n",
    "            key=lambda x : SORTFUNCTION_SLASH(x)\n",
    "        )\n",
    "        \n",
    "        chart =  alt.Chart(\n",
    "              grouptable\n",
    "          ).transform_calculate(\n",
    "          order=f\"-indexof({altdomain}, datum.Taxon)\"\n",
    "            ).mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "                x=alt.X('Sample ID:N',sort=patientlist_sorted,title=None),\n",
    "                y= alt.Y('Read Fraction:Q',scale=alt.Scale(\n",
    "                    domain=(0,1))\n",
    "                       ,title=None)\n",
    "                if group == '1_Gesund' else \n",
    "            alt.Y('Read Fraction:Q',scale=alt.Scale(domain=(0,1)),title=None,axis=None),\n",
    "                color=alt.Color('Taxon:N',\n",
    "                                legend=alt.Legend(symbolLimit=0,labelLimit=0,labelFontSize=17,columnPadding=12),\n",
    "                                sort=taxa,\n",
    "                                scale=alt.Scale(domain=altdomain,range=altrange)),\n",
    "                tooltip=['readcount','Read Fraction','Taxon'],\n",
    "                order=alt.Order('order:Q')\n",
    "          )\n",
    "\n",
    "        charts[(group,domain)] = chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_sorted = ['1_Gesund','pre_1','pre_2','pre_3','leukozytopenia_1', 'leukozytopenia_2','leukozytopenia_3',\n",
    "              'reconstitution_1','reconstitution_2' ,'reconstitution_3' ,\n",
    "]\n",
    "\n",
    "charts_domain = []\n",
    "for domain in DOMAINS:\n",
    "    charts_timephase = []\n",
    "    for timephase in ['1','pre','leukozytopenia','reconstitution']:\n",
    "        charts_group = []\n",
    "        groups = ['1_Gesund'] if timephase == '1' else (timephase+'_'+x for x in ['1','2','3'])\n",
    "        for idx,group in enumerate(groups):\n",
    "            chart_group = charts[(group,domain)]\n",
    "            charts_group.append(chart_group)\n",
    "        chart_timephase = reduce(lambda x,y : x|y,charts_group)\n",
    "        charts_timephase.append(chart_timephase)\n",
    "    chart_domain = reduce(lambda x,y : alt.hconcat(x,y,spacing=60), charts_timephase).properties(title=domain)\n",
    "    charts_domain.append(chart_domain)\n",
    "\n",
    "    \n",
    "reduce(\n",
    "    lambda x,y : x&y, charts_domain\n",
    ").resolve_scale(color='independent').configure_legend(\n",
    "            orient='none',\n",
    "            title=None,\n",
    "            direction='horizontal',\n",
    "            titleAlign='center',\n",
    "            columns=8,\n",
    "            titleAnchor='middle',\n",
    "            legendX=430,\n",
    "            legendY=400,\n",
    "    labelFontSize=14,\n",
    "    symbolSize=140\n",
    ").configure_title(fontSize=24, offset=5, orient='top', anchor='middle').configure_axis(labelFontSize=14).save('figure7.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bray-Curtis Distance based PCoA <a class=\"anchor\" id=\"pcoa\"></a>\n",
    "$$\n",
    "B_{i,j} = 1 - \\frac{2C_{i,j}}{S_i+S_j}\n",
    "$$\n",
    "\n",
    "For relative abundances, this becomes:\n",
    "\n",
    "$1-C_{i,j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCoA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial.distance as ssd\n",
    "from skbio.stats import ordination\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "################################\n",
    "\n",
    "LEVEL = 'S'\n",
    "READ_CUTOFF = 5\n",
    "VALIDATION_CUTOFF = 0.2\n",
    "SAMPLES = PAPER_SAMPLES\n",
    "EXCLUDED = [CHORDATA,PLANTAE]\n",
    "INCLUDED = None\n",
    "NORMALIZE = False\n",
    "INVERT_X = False\n",
    "INVERT_Y = False\n",
    "\n",
    "#################################\n",
    "\n",
    "subtable = get_normalized_abundances(kraken_dataframe,\n",
    "                                        level=LEVEL,\n",
    "                                        samples=[s.replace('/','_') for s in SAMPLES],\n",
    "                                        excluded_taxa_filter=EXCLUDED,\n",
    "                                        included_taxa_filter=INCLUDED,\n",
    "                                       normalize=NORMALIZE)\n",
    "\n",
    "\n",
    "\n",
    "subtable = apply_validation(subtable,level=LEVEL,readcount_cutoff=READ_CUTOFF,validation_cutoff=VALIDATION_CUTOFF)\n",
    "subtable['Read Fraction'] = subtable['readcount']/subtable.groupby('samplename')['readcount'].transform('sum')\n",
    "\n",
    "pivot_table = subtable.pivot(index=['samplename'],columns=['taxonid'],values=['Read Fraction']).fillna(0)\n",
    "data = pivot_table.values\n",
    "\n",
    "samples = pivot_table.index.tolist()\n",
    "taxa_count = len(data[0])\n",
    "\n",
    "def braycurtis(indexA,indexB):\n",
    "    cij = 0\n",
    "    for taxonIndex in range(taxa_count):\n",
    "        cij += min(\n",
    "            data[indexA][taxonIndex],\n",
    "            data[indexB][taxonIndex]\n",
    "        )\n",
    "    if (1-cij) > 1:\n",
    "        print(1-cij)\n",
    "    return 1-cij\n",
    "\n",
    "bc_tuples = []\n",
    "\n",
    "distance_matrix = np.zeros(shape=(len(samples),len(samples)))\n",
    "\n",
    "for x in range(len(samples)):\n",
    "    for y in range(x+1,len(samples)):\n",
    "        print('Calculating all distancesfor sample {} / {}'.format(x+1,len(samples)),end='\\r')\n",
    "        distance = braycurtis(x,y)\n",
    "        distance_matrix[x][y] = distance\n",
    "        distance_matrix[y][x] = distance\n",
    "        bc_tuples.append((samples[x][0]+'/'+str(samples[x][1]),samples[y][0]+'/'+str(samples[y][1]),distance))\n",
    "\n",
    "PCoA = ordination.pcoa(distance_matrix,number_of_dimensions=3)\n",
    "\n",
    "tuples = []\n",
    "\n",
    "for idx,s in PCoA.samples.iterrows():\n",
    "    sample = samples[int(idx)]\n",
    "\n",
    "    tuples.append((s.PC1,s.PC2,s.PC3,sample))\n",
    "    \n",
    "\n",
    "pcoa_table = pd.DataFrame(\n",
    "    tuples,\n",
    "    columns=['x','y','z','samplename']\n",
    ")\n",
    "\n",
    "pcoa_table['Name'] = pcoa_table['samplename']\n",
    "sample_statistics['samplename']= sample_statistics['PatID']+'_'+sample_statistics['time'].astype(str)\n",
    "\n",
    "pcoa_table = pd.merge(sample_statistics,pcoa_table,left_on=['samplename'],right_on=['samplename'],how='right')\n",
    "\n",
    "pcoa_table['Sample Type'] = pcoa_table['Name'].apply(lambda x : 'Control' if x.startswith('G') else 'patient')\n",
    "\n",
    "if INVERT_X:\n",
    "    pcoa_table['x'] = -pcoa_table['x']\n",
    "if INVERT_Y:\n",
    "    pcoa_table['y'] = -pcoa_table['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "calculate clustering based on the bray-curtis distances using k-means on the pcoa space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sil_tuples = []\n",
    "\n",
    "for k in [2,3,4,5,6]:\n",
    "\n",
    "    pretx_pcoa = pcoa_table[pcoa_table['timephase'].isin(['Pre TX','Healthy'])]\n",
    "\n",
    "    km= KMeans(n_clusters=k,random_state=0,n_init='auto').fit(pretx_pcoa[['x','y']])\n",
    "    pretx_pcoa['KMeansCluster'] = km.predict(pretx_pcoa[['x','y']])+1 #+1 to get cluster numbers 1 through 3\n",
    "\n",
    "    sil_tuples.append((k,silhouette_score(pretx_pcoa[['x','y']],pretx_pcoa['KMeansCluster'])))\n",
    "alt.Chart(pd.DataFrame(sil_tuples,columns=['k','Silhouette Value'])).mark_bar().encode(\n",
    "    x='k',y='Silhouette Value'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIX Colors for clusters\n",
    "group_names = ['Control','Pre-Tx Cluster 1','Pre-Tx Cluster 2','Pre-Tx Cluster 3','Samples from other phases']\n",
    "group_colors = [\n",
    "    '#00fdcf', #Phocaiecola Vulgatus\n",
    "    '#b500ff', #Bacteroides Uniformis\n",
    "    '#366962', #Enterobacter boltae\n",
    "    '#d60000', #Enterococcus Faecium\n",
    "    'lightgray'\n",
    "]\n",
    "group_colors_grey_control=[\n",
    "    'lightgray', \n",
    "    '#b500ff', #Bacteroides Uniformis\n",
    "    '#366962', #Enterobacter boltae\n",
    "    '#d60000', #Enterococcus Faecium\n",
    "    'lightgray'    \n",
    "]\n",
    "group_shapes = [\n",
    "    'square', \n",
    "    'circle', \n",
    "    'circle', \n",
    "    'circle',\n",
    "    'diamond'\n",
    "]\n",
    "\n",
    "charts = []\n",
    "\n",
    "pcoa_table['timephase'] = pcoa_table['timephase'].apply(\n",
    "    lambda x : 'Control' if x == 'Healthy' else x)\n",
    "pcoa_table['timephase'] = pcoa_table['timephase'].apply(\n",
    "    lambda x : 'Pre-Tx' if x == 'Pre TX' else x)\n",
    "pcoa_table['timephase'] = pcoa_table['timephase'].apply(\n",
    "    lambda x : 'Leukopenia' if x == 'Leukozytopenia' else x)\n",
    "\n",
    "pcoa_table['Startcluster'] = pcoa_table['Startcluster'].apply(\n",
    "    lambda x : 'Pre-Tx Cluster {}'.format(x) if isinstance(x,int) else x)\n",
    "pcoa_table['Startcluster'] = pcoa_table['Startcluster'].apply(\n",
    "    lambda x : 'Control' if x == 'Gesund' else x)\n",
    "\n",
    "\n",
    "for timepoint in ['Pre-Tx','Leukopenia','Reconstitution']:\n",
    "    \n",
    "    def calculate_visual_group(row):\n",
    "        #Selected\n",
    "        if (row['timephase'] == timepoint) or (row['timephase'] == 'Control'):\n",
    "            return row['Startcluster']\n",
    "        else:\n",
    "            return 'Samples from other phases'\n",
    "    #Calculate Shape and Color\n",
    "    pcoa_table['Visual Group {}'.format(timepoint)] = pcoa_table.apply(calculate_visual_group,axis=1)\n",
    "    timepoint_pcoa = alt.Chart(pcoa_table).mark_point(filled=True).encode(\n",
    "        x=alt.X('x:Q',title='PCoA Axis 1'),\n",
    "        y=alt.Y('y:Q',title='PCoA Axis 2'),\n",
    "        color=alt.Color('Visual Group {}'.format(timepoint),scale=alt.Scale(domain=group_names,range=group_colors if timepoint == 'Pre-Tx' else group_colors_grey_control ),title='Cluster'),\n",
    "        shape=alt.Shape('Visual Group {}'.format(timepoint),scale=alt.Scale(domain=group_names,range=group_shapes)),\n",
    "        tooltip=['Name','Visual Group {}'.format(timepoint)]\n",
    "    )\n",
    "\n",
    "    charts.append(timepoint_pcoa.properties(title='PCoA based on {} Samples'.format(timepoint)))\n",
    "\n",
    "reduce(lambda x,y : x&y,charts).resolve_scale(color='independent',shape='independent').configure_axis(labelFontSize=15,titleFontSize=15,tickCount=7).configure_title(fontSize=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_frame = pd.DataFrame(bc_tuples,columns=['Sample X','Sample Y','BCD'])\n",
    "bc_frame[['Patient X','Time X']] = bc_frame['Sample X'].str.split('/',expand=True)\n",
    "bc_frame[['Patient Y','Time Y']] = bc_frame['Sample Y'].str.split('/',expand=True)\n",
    "\n",
    "#Determine most extreme timepoints for each patient\n",
    "\n",
    "extreme_points = pd.concat(\n",
    "    [bc_frame.groupby('Patient X')['Time X'].min().rename('Earliest Time'),\n",
    "    bc_frame.groupby('Patient X')['Time X'].max().rename('Latest Time')],axis=1\n",
    ").reset_index()\n",
    "\n",
    "#Eliminate rows where earliest is >= 0 or latest is <= 0\n",
    "eliminated = extreme_points[\n",
    "    (extreme_points['Earliest Time'].astype(int) < 0)&\n",
    "    (extreme_points['Latest Time'].astype(int) > 0)\n",
    "]\n",
    "\n",
    "print(\n",
    "    'For the following patients no valid pre/post pair could be generated: {} (They will be EXCLUDED from analysis)'.format(\n",
    "        \n",
    "        set(extreme_points['Patient X']).difference(set(eliminated['Patient X']))\n",
    "    )\n",
    ")\n",
    "\n",
    "bc_frame = bc_frame[\n",
    "    #(bc_frame['Patient X'].isin(eliminated['Patient X']))&\n",
    "    #(bc_frame['Patient Y'].isin(eliminated['Patient X']))&\n",
    "    (bc_frame['Patient X'] == bc_frame['Patient Y'])\n",
    "]\n",
    "\n",
    "eliminated = eliminated.set_index('Patient X')\n",
    "\n",
    "#bc_frame['Earliest X'] = bc_frame['Patient X'].apply(lambda x :  eliminated.loc[str(x)]['Earliest Time'])\n",
    "#bc_frame['Latest X'] = bc_frame['Patient X'].apply(lambda x :  eliminated.loc[str(x)]['Latest Time'])\n",
    "\n",
    "#bc_frame = bc_frame[\n",
    "#    (bc_frame['Time X'] == bc_frame['Earliest X'])&\n",
    "#    (bc_frame['Time Y'] == bc_frame['Latest X'])\n",
    "\n",
    "#]\n",
    "\n",
    "bc_frame['Time X'] = bc_frame['Time X'].astype(int)\n",
    "bc_frame['Time Y'] = bc_frame['Time Y'].astype(int)\n",
    "\n",
    "bc_frame = pd.merge(\n",
    "    bc_frame,\n",
    "    sample_statistics[['PatID','time','Startcluster']],\n",
    "    how='left',\n",
    "    left_on=['Patient X','Time X'],\n",
    "    right_on=['PatID','time']\n",
    ")\n",
    "\n",
    "bc_frame\n",
    "\n",
    "bc_frame = pd.merge(\n",
    "    bc_frame,\n",
    "    sample_statistics[['PatID','time','leukocytephase_cluster_2_kurz']],\n",
    "    how='left',\n",
    "    left_on=['Patient Y','Time Y'],\n",
    "    right_on=['PatID','time']\n",
    ")[['Sample X','Sample Y','BCD','Startcluster','leukocytephase_cluster_2_kurz']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(bc_frame).mark_boxplot().encode(\n",
    "    x='Startcluster:N',\n",
    "    y='BCD'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_frame.to_csv('BCPrePostPairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_frame.groupby('Startcluster')['BCD'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bray Curtis Pre/Reconst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_frame = pd.DataFrame(bc_tuples,columns=['Sample X','Sample Y','BCD'])\n",
    "bc_frame[['Patient X','Time X']] = bc_frame['Sample X'].str.split('/',expand=True)\n",
    "bc_frame[['Patient Y','Time Y']] = bc_frame['Sample Y'].str.split('/',expand=True)\n",
    "bc_frame['Time X'] = bc_frame['Time X'].astype(int)\n",
    "bc_frame['Time Y'] = bc_frame['Time Y'].astype(int)\n",
    "\n",
    "bc_frame = pd.merge(\n",
    "    bc_frame,\n",
    "    sample_statistics[['PatID','time','timephase']],\n",
    "    how='left',\n",
    "    left_on=['Patient X','Time X'],\n",
    "    right_on=['PatID','time']\n",
    ").drop(columns=['PatID','time']).rename(columns={'timephase':'Phase X'})\n",
    "bc_frame = pd.merge(\n",
    "    bc_frame,\n",
    "    sample_statistics[['PatID','time','timephase']],\n",
    "    how='left',\n",
    "    left_on=['Patient Y','Time Y'],\n",
    "    right_on=['PatID','time']\n",
    ").drop(columns=['PatID','time']).rename(columns={'timephase':'Phase Y'})\n",
    "\n",
    "bc_frame = bc_frame[\n",
    "    (bc_frame['Phase X'] == 'Pre TX')&\n",
    "    (bc_frame['Phase Y'] == 'Reconstitution')\n",
    "]\n",
    "\n",
    "bc_frame['Within Patient'] = bc_frame['Patient X'] == bc_frame['Patient Y']\n",
    "bc_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(bc_frame).mark_bar().encode(\n",
    "    x=alt.X('BCD',bin=True),\n",
    "    y='count()'\n",
    ").facet(column='Within Patient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Nadir less similar to Pre-TX than\" Reconstitution Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_frame = pd.DataFrame(bc_tuples,columns=['Sample X','Sample Y','BCD'])\n",
    "bc_frame[['Patient X','Time X']] = bc_frame['Sample X'].str.split('/',expand=True)\n",
    "bc_frame[['Patient Y','Time Y']] = bc_frame['Sample Y'].str.split('/',expand=True)\n",
    "bc_frame['Time X'] = bc_frame['Time X'].astype(int)\n",
    "bc_frame['Time Y'] = bc_frame['Time Y'].astype(int)\n",
    "\n",
    "bc_frame = pd.merge(\n",
    "    bc_frame,\n",
    "    sample_statistics[['PatID','time','timephase']],\n",
    "    how='left',\n",
    "    left_on=['Patient X','Time X'],\n",
    "    right_on=['PatID','time']\n",
    ").drop(columns=['PatID','time']).rename(columns={'timephase':'Phase X'})\n",
    "bc_frame = pd.merge(\n",
    "    bc_frame,\n",
    "    sample_statistics[['PatID','time','timephase']],\n",
    "    how='left',\n",
    "    left_on=['Patient Y','Time Y'],\n",
    "    right_on=['PatID','time']\n",
    ").drop(columns=['PatID','time']).rename(columns={'timephase':'Phase Y'})\n",
    "\n",
    "\n",
    "bc_frame['Within Patient'] = bc_frame['Patient X'] == bc_frame['Patient Y']\n",
    "\n",
    "bc_frame = bc_frame[\n",
    "    (bc_frame['Within Patient'] == True)&\n",
    "    (bc_frame['Phase X'] == 'Pre TX')\n",
    "]\n",
    "\n",
    "\n",
    "extreme_points = pd.concat(\n",
    "    [bc_frame.groupby(['Patient X','Phase X','Phase Y'])['Time X'].min().rename('Earliest Time'),\n",
    "    bc_frame.groupby(['Patient X','Phase X','Phase Y'])['Time Y'].max().rename('Latest Time')],axis=1\n",
    ").reset_index()\n",
    "\n",
    "bc_frame = pd.merge(bc_frame,extreme_points,on=['Patient X','Phase X','Phase Y'],how='left')\n",
    "bc_frame = bc_frame[\n",
    "    (bc_frame['Time X'] == bc_frame['Earliest Time'])&\n",
    "    (bc_frame['Time Y'] == bc_frame['Latest Time'])\n",
    "]\n",
    "bc_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(bc_frame).mark_boxplot().encode(\n",
    "    x='Phase Y',\n",
    "    y='BCD'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_frame = bc_frame.pivot(\n",
    "    index='Patient X',\n",
    "    columns='Phase Y',\n",
    "    values='BCD'\n",
    ").dropna()\n",
    "\n",
    "wilcoxon(bc_frame['Leukozytopenia'],bc_frame['Reconstitution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_frame['Diff'] = bc_frame['Leukozytopenia']-bc_frame['Reconstitution']\n",
    "bc_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(bc_frame).mark_boxplot().encode(\n",
    "    y='Diff'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Time Overview <a class=\"anchor\" id=\"sampletimes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_reduced = outcomes[\n",
    "    ['Pat ID',\n",
    "     'Day relative to HSCT',\n",
    "     'Day relative to HSCT.1',\n",
    "     'Day relative to HSCT.2',\n",
    "     'Day relative to HSCT.3',\n",
    "     'Day relative to HSCT.4',\n",
    "     'Day relative to HSCT.5',\n",
    "     'Day relative to HSCT.6',\n",
    "     'Day relative to HSCT.7',\n",
    "    ]\n",
    "         ].rename(\n",
    "    columns={\n",
    "        'Day relative to HSCT' : '1st Relapse',\n",
    "        'Day relative to HSCT.1' : '2nd Relapse',\n",
    "        'Day relative to HSCT.2' : '2nd HSCT',\n",
    "        'Day relative to HSCT.3' : 'Acute GvHD Grade 1-2',\n",
    "        'Day relative to HSCT.4' : 'Acute GvHD Grade 3-4',\n",
    "        'Day relative to HSCT.5' : 'Moderate cGvHD',\n",
    "        'Day relative to HSCT.6' : 'Severe cGvHD',\n",
    "        'Day relative to HSCT.7' : 'Death',\n",
    "    }\n",
    ")\n",
    "outcomes_reduced['No Adverse Event']=outcomes_reduced.apply( lambda x : 0 if x.count() <= 1 else np.NaN,axis=1)\n",
    "outcomes_reduced\n",
    "\n",
    "outcomes_reduced = outcomes_reduced.melt(id_vars=['Pat ID'])\n",
    "outcomes_reduced = outcomes_reduced[(outcomes_reduced['value']==outcomes_reduced['value'])]\n",
    "outcomes_reduced = outcomes_reduced[(outcomes_reduced['value']!='?')]\n",
    "outcomes_reduced.loc[\n",
    "    (outcomes_reduced['Pat ID']=='18.2'),'KI ID'\n",
    "] = '18.2'\n",
    "outcomes_reduced.loc[(outcomes_reduced['Pat ID']=='18.2'),'value'] -= OFFSET_PAT_18\n",
    "outcomes_reduced = outcomes_reduced[outcomes_reduced['variable'] != 'No Adverse Event']\n",
    "\n",
    "outcomes_reduced = outcomes_reduced.rename(columns={\n",
    "    'Pat ID' : 'PatID',\n",
    "    'variable' : 'Adverse Event',\n",
    "    'value' : 'time'\n",
    "})\n",
    "\n",
    "\n",
    "############\n",
    "\n",
    "sample_statistics['id'] = sample_statistics['PatID']+'/'+sample_statistics['time'].astype(str)\n",
    "overview = sample_statistics[sample_statistics['id'].isin(PAPER_SAMPLES)]\n",
    "overview = overview[~overview['PatID'].str.startswith('G')]\n",
    "\n",
    "\n",
    "combined = pd.concat([overview,outcomes_reduced])\n",
    "\n",
    "\n",
    "\n",
    "patientlist_sorted = []\n",
    "\n",
    "for startcluster in combined['Startcluster'].unique():\n",
    "    clusterlist = sorted(combined[combined['Startcluster'] == startcluster]['PatID'].unique())\n",
    "    patientlist_sorted += clusterlist\n",
    "\n",
    "        \n",
    "overview = (alt.Chart(combined[combined['Adverse Event'] == combined['Adverse Event']]).mark_point(size=38,color='black').encode(\n",
    "    x=alt.X('time',scale=alt.Scale(type='symlog'),axis=alt.Axis(grid=False)),\n",
    "    y=alt.Y('PatID',sort=patientlist_sorted),\n",
    "    shape=alt.Shape('Adverse Event:N',scale=alt.Scale(\n",
    "        domain=['2nd HSCT','Acute GvHD Grade 1-2','Acute GvHD Grade 3-4','Moderate cGvHD','Severe cGvHD','1st Relapse','2nd Relapse','Death'],\n",
    "        range=['circle','square','square','diamond','diamond','triangle','triangle','cross']))\n",
    ")+alt.Chart(combined,width=800).mark_circle().encode(\n",
    "    x=alt.X('time',scale=alt.Scale(type='symlog'),axis=alt.Axis(grid=False)),\n",
    "    y=alt.Y('PatID:N',sort=patientlist_sorted,axis=alt.Axis(grid=True)),\n",
    "    color='Startcluster:N'\n",
    ")+alt.Chart(combined).mark_text(dx=0,dy=-6).encode(\n",
    "    x=alt.X('time',scale=alt.Scale(type='symlog'),axis=alt.Axis(grid=False)),\n",
    "    y=alt.Y('PatID:N',sort=patientlist_sorted,axis=alt.Axis(grid=True)),\n",
    "    text='time'\n",
    "))\n",
    "\n",
    "overview.save('Output/Samples.html')\n",
    "\n",
    "overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zymo Std. Analysis <a class=\"anchor\" id=\"zymo\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_level = get_normalized_abundances(kraken_dataframe,samples=['Zymo_Power_-100'],level='S')\n",
    "\n",
    "with_validation = apply_validation(\n",
    "    species_level,\n",
    "    level = 'S',\n",
    "    validation_cutoff = 0.2,\n",
    "    readcount_cutoff = 5\n",
    ")\n",
    "with_validation['Read Fraction (%)'] = with_validation['readcount']/with_validation.groupby('samplename')['readcount'].transform('sum')\n",
    "with_validation=with_validation.rename(columns={\n",
    "    'samplename' : 'Sample ID',\n",
    "    'taxon' : 'Taxon'\n",
    "})\n",
    "\n",
    "sample_statistics['samplename']= sample_statistics['PatID']+'_'+sample_statistics['time'].astype(str)\n",
    "\n",
    "\n",
    "with_validation = pd.concat([with_validation,zymo_theory])[\n",
    "    ['Sample ID','Taxon','Read Fraction (%)']\n",
    "]\n",
    "\n",
    "melt = zymo_minimap.melt(id_vars='Index')\n",
    "\n",
    "melt = melt.rename(\n",
    "    columns={\n",
    "    'variable' : 'Taxon',\n",
    "        'Index' : 'Sample ID',\n",
    "        'value' : 'Read Fraction (%)'\n",
    "    }\n",
    ")\n",
    "\n",
    "#Replace underscores with spaces\n",
    "melt['Taxon'] = melt['Taxon'] = melt['Taxon'].apply(lambda x : ' '.join(x.split('_')))\n",
    "\n",
    "#Merge different E-Coli Strains\n",
    "melt.loc[melt['Taxon'].str.startswith('Escherichia coli'),'Taxon'] = 'Escherichia coli'\n",
    "melt.loc[melt['Taxon']=='Candida albican','Taxon'] = 'Candida albicans'\n",
    "\n",
    "melt = melt[melt['Sample ID'].isin(['Power/Reads','Power/Bases'])]\n",
    "\n",
    "melt['Sample ID'] = melt['Sample ID'].apply(lambda x : x + '_Minimap2')\n",
    "\n",
    "melt['Taxon'] = melt['Taxon'].apply(lambda x : 'Unmapped' if x == 'unmapped' else x)\n",
    "\n",
    "#Reduce to Species\n",
    "melt = melt.groupby(['Sample ID','Taxon'],as_index=False).sum()\n",
    "\n",
    "combined = pd.concat([melt,with_validation])\n",
    "\n",
    "taxa_we_look_at = list(combined[combined['Sample ID'] == 'Power/Reads_Minimap2']['Taxon'].unique())\n",
    "if 'Not validated' not in taxa_we_look_at:\n",
    "    taxa_we_look_at.append('Not validated')\n",
    "print('Determined the following taxa as relevant:',taxa_we_look_at)\n",
    "\n",
    "combined.loc[~combined['Taxon'].isin(taxa_we_look_at), 'Taxon'] = 'Other'\n",
    "combined = combined.groupby(['Taxon','Sample ID'],as_index=False).sum()\n",
    "\n",
    "\n",
    "colorMap = {}\n",
    "\n",
    "taxa = combined['Taxon'].unique()\n",
    "\n",
    "palette = cc.glasbey_light\n",
    "bright = palette[::2]\n",
    "muted = palette[1::2]\n",
    "palette = bright+muted\n",
    "\n",
    "for tax,col in zip(list(taxa),palette):\n",
    "    colorMap[tax] = col #colors.to_hex(col)\n",
    "    \n",
    "altdomain = []\n",
    "altrange = []\n",
    "\n",
    "for x in combined['Taxon'].unique():\n",
    "    \n",
    "    if x == 'Other' or x == 'Unmapped' or x == 'Not validated':\n",
    "        continue\n",
    "\n",
    "    c = colorMap[x]\n",
    "    altdomain.append(x)\n",
    "    color = colors.to_hex(c)\n",
    "    altrange.append(color)\n",
    "    \n",
    "altdomain.append('Other')\n",
    "altrange.append(colors.to_hex((1,1,1)))\n",
    "altdomain.append('Unmapped')\n",
    "altrange.append(colors.to_hex((0.45,0.45,0.45)))\n",
    "altdomain.append('Not validated')\n",
    "altrange.append(colors.to_hex((0.55,0.55,0.55)))\n",
    "\n",
    "combined['Sample ID'] = combined['Sample ID'].map({\n",
    "    'Zymo_Power_-100' : 'Kraken2',\n",
    "    'Power/Bases_Minimap2' : 'Minimap2 Bases',\n",
    "    'Power/Reads_Minimap2' : 'Minimap2 Reads',\n",
    "    'Zymo Theoretical Composition' : 'Theoretical Composition',\n",
    "    \n",
    "})\n",
    "\n",
    "c= alt.Chart(\n",
    "  combined\n",
    ").transform_calculate(\n",
    "order=f\"-indexof({altdomain}, datum.Taxon)\"\n",
    ").mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "    x=alt.X('Sample ID:N',title=None,sort=['Kraken2','Minimap2 Bases','Minimap2 Reads','Theoretical Composition']\n",
    "),\n",
    "    y=alt.Y('Read Fraction (%):Q',scale=alt.Scale(domain=(0,1)),title='Estimated Abundance'),\n",
    "    color=alt.Color('Taxon:N',legend=alt.Legend(columns=2,symbolLimit=0,labelLimit=0),scale=alt.Scale(domain=altdomain,range=altrange),title=None),\n",
    "    tooltip=['Read Fraction (%)','Taxon'],\n",
    "    order=alt.Order('order:Q')\n",
    ")\n",
    "c.save('Output/Zymo_Overview.html')\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.pivot(index='Taxon',columns='Sample ID',values='Read Fraction (%)').fillna(0).to_csv('Output/ZymoStdAbundances.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zymo Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.pivot(index='Taxon',columns='Sample ID',values='Read Fraction (%)').fillna(0).corr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crassphage <a class=\"anchor\" id=\"crassphage\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kraken_data = kraken_dataframe[kraken_dataframe['taxon'].isin(['uncultured crAssphage','unclassified Crassvirales','root'])]\n",
    "kraken_data = kraken_data[kraken_data['sample'].isin(PAPER_SAMPLES)]\n",
    "kraken_data = kraken_data.pivot(\n",
    "    index='sample',columns='taxon',values='readcount'\n",
    ").fillna(0)\n",
    "\n",
    "kraken_data['crassphage_detected'] = (kraken_data['uncultured crAssphage']+kraken_data['unclassified Crassvirales'])/kraken_data['root']\n",
    "\n",
    "kraken_data = kraken_data.reset_index()\n",
    "\n",
    "def rename(x):\n",
    "    split = x.rsplit('_',1)\n",
    "    return split[0]+'/'+split[1]\n",
    "\n",
    "minimap_data = pd.read_csv('Input/Crassphage/summary.csv')\n",
    "minimap_data['Sample'] = minimap_data['Sample'].apply(rename)\n",
    "\n",
    "\n",
    "minimap_data_aggregated = minimap_data.groupby('Sample')[['Fraction Mapped','Mapped Reads']].sum().reset_index()\n",
    "\n",
    "\n",
    "combined = pd.merge(kraken_data,minimap_data_aggregated,left_on='sample',right_on='Sample',how='left')\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = max(\n",
    "    combined['Fraction Mapped'].max(),combined['crassphage_detected'].max()\n",
    ")\n",
    "\n",
    "line = pd.DataFrame({\n",
    "    'X': [0, max_value],\n",
    "    'Y': [0, max_value],\n",
    "})\n",
    "\n",
    "\n",
    "(alt.Chart(combined,width=400,height=400).mark_point().encode(\n",
    "    y=alt.Y('Fraction Mapped',scale=alt.Scale(type='symlog',constant=0.001)),\n",
    "    x=alt.X('crassphage_detected',scale=alt.Scale(type='symlog',constant=0.001))\n",
    ")+alt.Chart(line,width=700,height=700).mark_line(color= 'lightgray').encode(\n",
    "        x= 'X',\n",
    "        y= 'Y'\n",
    "    )).interactive()#.save('Output/CrassphageKrakenVsMinimap.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap, which Crassphage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_X = 50\n",
    "\n",
    "heatmap_table = minimap_data.pivot(index='Sample',columns='Reference',values='Fraction Mapped').fillna(0).melt(ignore_index=False).reset_index().rename(columns={'value' : 'Fraction Mapped'})\n",
    "heatmap_table['Unambiguous'] = heatmap_table['Reference'] != 'Ambiguous'\n",
    "\n",
    "#For top bar charts\n",
    "heatmap_table_simplified = heatmap_table.groupby(['Sample','Unambiguous'],as_index=False)['Fraction Mapped'].sum()\n",
    "\n",
    "\n",
    "heatmap_table_filtered = heatmap_table[\n",
    "    (heatmap_table['Sample'].isin(PAPER_SAMPLES)  )\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "heatmap_table_filtered['Fraction Mapped']=heatmap_table_filtered['Fraction Mapped']/heatmap_table_filtered.groupby('Sample')['Fraction Mapped'].transform('sum')\n",
    "top_refs = heatmap_table_filtered.groupby('Reference')['Fraction Mapped'].sum().sort_values(ascending=False).keys().tolist()[:TOP_X]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "heatmap_table_filtered = heatmap_table_filtered[\n",
    "    \n",
    "    heatmap_table_filtered['Reference'].isin(top_refs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = (alt.Chart(heatmap_table_simplified[heatmap_table_simplified['Sample'].isin(PAPER_SAMPLES)]).mark_bar().encode(\n",
    "    x=alt.X('Sample',sort=sort_samples(PAPER_SAMPLES),axis=alt.Axis(orient='top')),\n",
    "    y=alt.Y('Fraction Mapped',stack=True),\n",
    "    color='Unambiguous'\n",
    ")&alt.Chart(heatmap_table_filtered).mark_rect().encode(\n",
    "    x=alt.X('Sample',sort=sort_samples(PAPER_SAMPLES),axis=None),\n",
    "    y=alt.Y('Reference',sort=max_refs),\n",
    "    color=alt.Color('Fraction Mapped:Q',title='Fraction of uniquely assigned reads'),\n",
    "    tooltip=['Fraction Mapped']\n",
    ")).resolve_scale(x='shared',color='independent',y='independent')\n",
    "\n",
    "chart.save('Output/CrassphageOverview.html')\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migration = pd.read_csv('Input/Crassphage/migration.csv',dtype={'Tax ID' : str})\n",
    "migration['Fraction'] = migration['Reads']/migration.groupby('Sample')['Reads'].transform('sum')\n",
    "migration['Taxon Name'] = migration['Tax ID'].map(idtonames)\n",
    "\n",
    "top_hits = migration.groupby('Tax ID')['Fraction'].mean().sort_values(ascending=False).keys()[:20]\n",
    "migration = migration[migration['Tax ID'].isin(top_hits)]\n",
    "\n",
    "def categorize(taxon_name):\n",
    "    if taxon_name in ['uncultured phage cr6_1','uncultured crAssphage','CrAss-like virus sp.']:\n",
    "        return 'crAssphage classification'\n",
    "    elif taxon_name != taxon_name:\n",
    "        return 'unclassified'\n",
    "    return 'other classification'\n",
    "\n",
    "migration['Category'] = migration['Taxon Name'].apply(categorize)\n",
    "migration['Taxon Name'] = migration['Taxon Name'].apply(lambda x : 'unclassified' if x != x else x)\n",
    "\n",
    "crassphages = {'uncultured phage cr6_1','uncultured crAssphage','CrAss-like virus sp.'}\n",
    "rest = set(migration['Taxon Name'])-(crassphages.union({'unclassified'}))\n",
    "sorted_taxa = ['unclassified']+list(sorted(rest))+list(crassphages)\n",
    "\n",
    "alt.Chart(migration).mark_boxplot().encode(\n",
    "    y='Fraction',\n",
    "    x=alt.X('Taxon Name:N',sort=sorted_taxa),\n",
    "    color='Category',\n",
    "    tooltip=['Reads','Sample','Fraction','Taxon Name']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = pd.read_csv('Input/crassphage_details.csv')\n",
    "\n",
    "charts = []\n",
    "\n",
    "for metric in ['Percentage Aligned','Mapping Quality','Identity']:\n",
    "\n",
    "    amount,edges = np.histogram(details[metric],bins=100)\n",
    "\n",
    "    centers = []\n",
    "    for x,y in zip(edges[:-1],edges[1:]):\n",
    "        centers.append((x+y)/2)\n",
    "\n",
    "    tuples = []\n",
    "    for x,y in zip(centers,amount):\n",
    "        tuples.append((x,y))\n",
    "\n",
    "    df = pd.DataFrame(tuples,columns=[metric,'Count'])\n",
    "\n",
    "    c = alt.Chart(df).mark_bar().encode(\n",
    "        x=metric,\n",
    "        y=alt.Y('Count',scale=alt.Scale(type='symlog'))\n",
    "    )\n",
    "    \n",
    "    charts.append(c)\n",
    "    \n",
    "reduce(lambda x,y : x&y, charts).save('Output/CrassphageAlignmentDetails.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marker Taxa Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIDENCE_INTERVAL_ALPHA = 0.05\n",
    "\n",
    "sample_statistics['samplename'] = sample_statistics['PatID']+'_'+sample_statistics['time'].astype(str)\n",
    "\n",
    "for level in ['S','G']:\n",
    "\n",
    "    MARKER_OTUS = MARKER_GENERA if level == 'G' else MARKER_SPECIES\n",
    "\n",
    "    otu_data = get_normalized_abundances(\n",
    "        kraken_dataframe,\n",
    "        samples=[s.replace('/','_') for s in PAPER_SAMPLES],\n",
    "        level=level,\n",
    "        excluded_taxa_filter=[CHORDATA,PLANTAE],\n",
    "        normalize=False\n",
    "    )\n",
    "\n",
    "    total_roots = otu_data.groupby('samplename')['readcount'].sum()\n",
    "\n",
    "    otu_data['Read Fraction'] =  otu_data['readcount'] / otu_data.groupby(\n",
    "        ['samplename']\n",
    "    )['readcount'].transform('sum')\n",
    "\n",
    "    PAPER_SAMPLES_UNDERSCORE = [x.replace('/','_') for x in PAPER_SAMPLES]\n",
    "    marker_data = validation_data[level]\n",
    "    marker_data = marker_data[marker_data['samplename'].isin(PAPER_SAMPLES_UNDERSCORE)]\n",
    "    marker_data['Validated'] = marker_data['Validation Rate'] >= 0.2\n",
    "\n",
    "    os.makedirs('marker_otus_overview',exist_ok=True)\n",
    "\n",
    "\n",
    "    def SORTFUNCTION(x):\n",
    "        return (\n",
    "            x[0] == 'G', #Sort by G or regular patient first\n",
    "            float(x.split('G')[-1])\n",
    "        )\n",
    "\n",
    "    patientlist_sorted = sorted(\n",
    "        marker_data['patientid'].unique(),key=SORTFUNCTION\n",
    "    )\n",
    "\n",
    "\n",
    "    for taxon in MARKER_OTUS:\n",
    "\n",
    "        subtable = marker_data[\n",
    "            marker_data['Taxon Name'] == taxon\n",
    "        ]\n",
    "\n",
    "        subtable = pd.merge(\n",
    "            otu_data,\n",
    "            subtable[['samplename','Taxon Name','Validated']],\n",
    "            left_on=['samplename','taxon'],\n",
    "            right_on=['samplename','Taxon Name'],\n",
    "            how='right'\n",
    "        )\n",
    "        \n",
    "        if len(subtable) == 0:\n",
    "            print('No data for taxon: {}'.format(taxon))\n",
    "            continue\n",
    "        subtable[['patientid','time']] = subtable['samplename'].str.split('_',expand=True)\n",
    "\n",
    "        substitutes = []\n",
    "        for sample in PAPER_SAMPLES_UNDERSCORE:\n",
    "            if sample not in subtable['samplename'].unique():\n",
    "                patientid,time = sample.rsplit('_',1)\n",
    "                substitutes.append(\n",
    "                    (patientid,int(time),'?',0,patientid+'_'+str(time),0)\n",
    "                )\n",
    "        subtable = pd.concat([subtable,pd.DataFrame(substitutes,columns=['patientid','time','Validated','readcount','samplename','Read Fraction'])]) \n",
    "\n",
    "\n",
    "        subtable[['Confidence Interval Low','Confidence Interval High']] = subtable.apply(\n",
    "            lambda row : proportion_confint(row['readcount'],total_roots[row['samplename']],alpha=CONFIDENCE_INTERVAL_ALPHA),\n",
    "            axis=1,\n",
    "            result_type='expand'\n",
    "        )   \n",
    "\n",
    "        subtable['Validated'] = subtable['Validated'].map(\n",
    "            {\n",
    "                '?' : 'Low abundance/Not validated',\n",
    "                False : 'Low abundance/Not validated',\n",
    "                True : 'Validated'\n",
    "            }\n",
    "        )\n",
    "\n",
    "        subtable['patientid'] = subtable['patientid'].astype(str)\n",
    "        subtable['day0'] = 0\n",
    "\n",
    "        c=alt.Chart(subtable[~subtable['patientid'].str.startswith('G')],height=40)\n",
    "        c2 = alt.Chart(subtable[subtable['patientid'].str.startswith('G')],height=40)\n",
    "        c = (c.mark_rule(size=20,strokeWidth=3).encode(\n",
    "                x=alt.X('time:Q',scale=alt.Scale(type='symlog'),axis=alt.Axis(grid=False),title='Day'),\n",
    "                color=alt.Color('Validated',scale=alt.Scale(domain=['Low abundance/Not validated','Validated'],range=['lightgrey','orange'])),\n",
    "                y=alt.Y('Confidence Interval Low:Q',title=None,axis=alt.Axis(tickCount=2)),\n",
    "                y2=alt.Y2('Confidence Interval High:Q',title=None)\n",
    "\n",
    "            )+c.mark_point(size=50,filled=True).encode(\n",
    "                x=alt.X('time:Q',scale=alt.Scale(type='symlog'),axis=alt.Axis(grid=False),title='Day'),\n",
    "                color=alt.Color('Validated',scale=alt.Scale(domain=['Low abundance/Not validated','Validated'],range=['lightgrey','orange'])),\n",
    "                y=alt.Y('Read Fraction:Q',title=None)\n",
    "\n",
    "            )+c.transform_calculate(time='0').mark_rule().encode(\n",
    "            x=alt.X('time:Q',scale=alt.Scale(type='symlog'),axis=alt.Axis(grid=False),title='Day')\n",
    "            )).resolve_scale(x='shared').facet(\n",
    "            row=alt.Row('patientid',sort=patientlist_sorted,title='Patient')\n",
    "        \n",
    "        ).resolve_scale(y='independent')|(\n",
    "            c2.mark_rule(size=20,strokeWidth=3).encode(\n",
    "                x=alt.X('patientid',axis=alt.Axis(grid=False),title='Control'),\n",
    "                color=alt.Color('Validated',scale=alt.Scale(domain=['Low abundance/Not validated','Validated'],range=['lightgrey','orange'])),\n",
    "                y=alt.Y('Confidence Interval Low:Q',title=None,axis=alt.Axis(tickCount=2)),\n",
    "                y2=alt.Y2('Confidence Interval High:Q',title=None)\n",
    "\n",
    "            )+c2.mark_point(size=50,filled=True).encode(\n",
    "                x=alt.X('patientid',axis=alt.Axis(grid=False),title='Control'),\n",
    "                color=alt.Color('Validated',scale=alt.Scale(domain=['Low abundance/Not validated','Validated'],range=['lightgrey','orange'])),\n",
    "                y=alt.Y('Read Fraction:Q',title=None),\n",
    "\n",
    "            )\n",
    "        )\n",
    "        c.properties(title=taxon).save('marker_otus_overview/{}.png'.format(taxon), engine='vl-convert') #Outcomment the engine argument if you have issues with saving\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data['S'].dropna()[validation_data['S'].dropna()['Taxon Name'].str.startswith('uncultured')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illumina Sequencing / Population Shifts <a class=\"anchor\" id=\"strains\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load samplesheet (To resolve illumina ids -> patient/time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesheetDictPatient = {}\n",
    "samplesheetDictTime = {}\n",
    "    \n",
    "samplesheet = pd.read_csv('Input/samples.tsv',sep='\\t')\n",
    "#Retain only entries that have illumina files\n",
    "samplesheet=samplesheet[samplesheet['illuminafile'] ==samplesheet['illuminafile']]\n",
    "for idx,row in samplesheet.iterrows():\n",
    "    samplesheetDictPatient[row['illuminafile']] = row['patientid']\n",
    "    samplesheetDictTime[row['illuminafile']] = row['time']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process/annotate distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGH_CONFIDENCE_CUTOFF = 50\n",
    "output_folder = 'Output/StrainAnalysis/GutTrSnp_{}'.format(HIGH_CONFIDENCE_CUTOFF)\n",
    "\n",
    "guttrsnp_distances = pd.read_csv('Input/GutTrSnp/RealData_{}/distances.csv'.format(\n",
    "   HIGH_CONFIDENCE_CUTOFF \n",
    "),dtype={\n",
    "    'Destination ID':str,\n",
    "    'Source ID' :str,\n",
    "    'Taxon' : str\n",
    "})\n",
    "\n",
    "#Kick out the stuff where there was no overlap and thus no distance\n",
    "guttrsnp_distances = guttrsnp_distances.dropna()\n",
    "guttrsnp_distances=guttrsnp_distances[guttrsnp_distances['Share Of Overlap'] >= 0.01]\n",
    "\n",
    "guttrsnp_distances['Taxon Name'] = guttrsnp_distances['Taxon'].map(idtonames)\n",
    "guttrsnp_distances['Source'] = guttrsnp_distances['Source ID']+'/'+guttrsnp_distances['Source Time'].astype(str)\n",
    "guttrsnp_distances['Destination'] = guttrsnp_distances['Destination ID']+'/'+guttrsnp_distances['Destination Time'].astype(str)\n",
    "guttrsnp_distances['PrePost Pair'] = (\n",
    "    guttrsnp_distances['Source ID'] == guttrsnp_distances['Destination ID']\n",
    ")&(\n",
    "guttrsnp_distances['Source Time'] < 0\n",
    ")&(\n",
    "guttrsnp_distances['Destination Time'] > 0\n",
    ")\n",
    "guttrsnp_distances['Within Patient'] = guttrsnp_distances['Source ID'] == guttrsnp_distances['Destination ID']\n",
    "#DoT\n",
    "guttrsnp_distances['Elapsed Time'] = guttrsnp_distances['Destination Time'] - guttrsnp_distances['Source Time']\n",
    "guttrsnp_distances['Distance over Time'] = guttrsnp_distances['GutTrSnp Distance'] / guttrsnp_distances['Elapsed Time']\n",
    "\n",
    "guttrsnp_distances['Distance Name'] = guttrsnp_distances['Source Time'].astype(str) + '->' + guttrsnp_distances['Destination Time'].astype(str)\n",
    "\n",
    "coverages = pd.read_csv('Input/GutTrSnp/RealData_{}/coverages.csv'.format(\n",
    "    HIGH_CONFIDENCE_CUTOFF\n",
    "),dtype={\n",
    "    'Patient ID' : str,\n",
    "    'Taxon ID' : str\n",
    "})\n",
    "guttrsnp_distances=pd.merge(guttrsnp_distances,coverages,how='left',left_on=['Source ID','Source Time','Taxon'],right_on=['Patient ID','Time','Taxon ID'])\n",
    "guttrsnp_distances=pd.merge(guttrsnp_distances,coverages,how='left',left_on=['Destination ID','Destination Time','Taxon'],right_on=['Patient ID','Time','Taxon ID'])\n",
    "\n",
    "\n",
    "\n",
    "guttrsnp_distances = guttrsnp_distances.drop(columns=[\n",
    "    'Patient ID_x','Patient ID_y',\n",
    "    'Time_x','Time_y',\n",
    "    'Taxon ID_x','Taxon ID_y'\n",
    "])\n",
    "\n",
    "\n",
    "bins=[0,10,20,50,100,200,500,1000,2000]\n",
    "\n",
    "\n",
    "guttrsnp_distances['VCG Source'] = pd.cut(guttrsnp_distances['Average Vertical Coverage_x'],bins)\n",
    "guttrsnp_distances['VCG Destination'] = pd.cut(guttrsnp_distances['Average Vertical Coverage_y'],bins)\n",
    "guttrsnp_distances = guttrsnp_distances.dropna()\n",
    "guttrsnp_distances['VCG Source']=guttrsnp_distances['VCG Source'].astype(str)\n",
    "guttrsnp_distances['VCG Destination']=guttrsnp_distances['VCG Destination'].astype(str)\n",
    "\n",
    "timepoints = {}\n",
    "\n",
    "for taxon in guttrsnp_distances['Taxon'].unique():\n",
    "    for source_id in guttrsnp_distances['Source ID'].unique():\n",
    "        subtable = guttrsnp_distances[\n",
    "            (\n",
    "                guttrsnp_distances['Taxon'] == taxon\n",
    "            )&(\n",
    "                guttrsnp_distances['Source ID'] == source_id\n",
    "                \n",
    "            )\n",
    "        ]\n",
    "        timepoints[(taxon,source_id)] = sorted(subtable['Source Time'].unique().tolist())\n",
    "        \n",
    "def sequential_test(row):\n",
    "    if row['Within Patient']:\n",
    "        if (row['Taxon'],row['Source ID']) in timepoints:\n",
    "            sequence = timepoints[(row['Taxon'],row['Source ID'])]\n",
    "            if sequence.index(row['Destination Time'])-sequence.index(row['Source Time']) == 1:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "guttrsnp_distances['Sequential'] = guttrsnp_distances.apply(sequential_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_to_simulation={\n",
    "    'Bacteroides vulgatus ATCC 8482' : 'PVulgatus',\n",
    "    'Enterococcus faecium' : 'EFaecium',\n",
    "    'Lactobacillus gasseri ATCC 33323 = JCM 1131' : 'LGasseri',\n",
    "    'Shigella sonnei 53G' : 'EColi',\n",
    "    \n",
    "}\n",
    "\n",
    "guttrsnp_distances_simulation = pd.read_csv('Input/GutTrSnp/SubspeciesSimulation/aggregatedDistances.csv',dtype={'Taxon' : str}).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Checks / Generic Analysis\n",
    "\n",
    "\n",
    "os.makedirs(output_folder,exist_ok=True)\n",
    "\n",
    "plots = []\n",
    "curation = []\n",
    "for taxon in guttrsnp_distances['Taxon Name'].unique():#['Bacteroides vulgatus ATCC 8482','Flavonifractor plautii','Bacteroides uniformis','Parabacteroides distasonis ATCC 8503','Parabacteroides merdae']:\n",
    "    \n",
    "    taxontable = guttrsnp_distances[\n",
    "        (guttrsnp_distances['Taxon Name'] == taxon)&\n",
    "        (guttrsnp_distances['Sequential'])&\n",
    "        (guttrsnp_distances['Share Of Overlap'] >= 0.5)\n",
    "    ]\n",
    "       \n",
    "    taxontable['PrePost Pair'] = taxontable['PrePost Pair'].map(\n",
    "        {\n",
    "            True : 'Yes',\n",
    "            False : 'No'\n",
    "        }\n",
    "    )\n",
    "   \n",
    "    if len(taxontable) < 3:\n",
    "        continue    \n",
    " \n",
    "    min_dist = taxontable['GutTrSnp Distance'].min()\n",
    "    max_dist = taxontable['GutTrSnp Distance'].max()\n",
    "    step = (max_dist-min_dist)/50\n",
    "    \n",
    "    max_line = alt.Chart(\n",
    "        pd.DataFrame(\n",
    "        [(max_dist)],columns=['Distance']\n",
    "    )\n",
    "    ).mark_rule().encode(\n",
    "        x='Distance',\n",
    "        size=alt.value(3)\n",
    "    )\n",
    "    \n",
    "    if taxon in mapping_to_simulation:\n",
    "        \n",
    "        subdata = guttrsnp_distances_simulation[\n",
    "            (guttrsnp_distances_simulation['Simulated Taxon'] == mapping_to_simulation[taxon])&\n",
    "            (guttrsnp_distances_simulation['Taxon'] == taxontable['Taxon'].values[0])\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        min_dist = min(subdata['GutTrSnp Distance'].min(),min_dist)\n",
    "        max_dist = min(subdata['GutTrSnp Distance'].max(),max_dist)\n",
    "        step = (max_dist-min_dist)/50 \n",
    "             \n",
    "    \n",
    "    plot = alt.Chart(taxontable,height=300,title='Sequential Samples Same Patients').mark_bar().encode(\n",
    "            x=alt.X('GutTrSnp Distance',bin=alt.Bin(extent=[min_dist,max_dist],step=step),title='Distance'),\n",
    "            y=alt.Y('count()',axis=alt.Axis(tickMinStep=1),stack=True),\n",
    "            color=alt.Color('PrePost Pair',title='Across Transplantation')\n",
    "        )\n",
    "    \n",
    "        \n",
    "    plot = (plot |alt.Chart(taxontable,height=300,title='Sequential Samples Same Patients').mark_point().encode(\n",
    "            y=alt.Y('GutTrSnp Distance',title='Distance'),\n",
    "            x='Elapsed Time',\n",
    "            tooltip=['Source','Destination','Share Of Overlap'],\n",
    "            color=alt.Color('PrePost Pair',title='Across Transplantation')\n",
    "        )          \n",
    "    )\n",
    "    \n",
    "    curation.append(\n",
    "        taxontable\n",
    "    )\n",
    "                   \n",
    "                \n",
    "    prepairs = guttrsnp_distances[\n",
    "        (guttrsnp_distances['Taxon Name'] == taxon)&\n",
    "        (guttrsnp_distances['Source ID'] != guttrsnp_distances['Destination ID'])&\n",
    "        (guttrsnp_distances['Source Time'] < 0)&\n",
    "        (guttrsnp_distances['Destination Time'] < 0)\n",
    "    ]\n",
    "    \n",
    "        \n",
    "    plot =  (plot | alt.Chart(\n",
    "        prepairs,height=300,title='Pre-Samples Different Patients'\n",
    "    ).mark_bar().encode(\n",
    "        x=alt.X('GutTrSnp Distance',bin=alt.Bin(extent=[min_dist,max_dist],step=step),title='Distance'),\n",
    "            y=alt.Y('count()',axis=alt.Axis(tickMinStep=1))\n",
    "    ) \n",
    "    )\n",
    "    \n",
    "    all_distances = guttrsnp_distances[\n",
    "        (guttrsnp_distances['Taxon Name'] == taxon)&\n",
    "        (guttrsnp_distances['Share Of Overlap'] >= 0.5)\n",
    "    ]\n",
    "    \n",
    "    plot =  (plot | (alt.Chart(\n",
    "            all_distances,height=300,title='All Distances'\n",
    "        ).mark_bar().encode(\n",
    "            x=alt.X('GutTrSnp Distance',bin=alt.Bin(maxbins=50),title='Distance'),\n",
    "            y=alt.Y('count()',axis=alt.Axis(tickMinStep=1))\n",
    "        ) +max_line)\n",
    "    )\n",
    "\n",
    "    if taxon in mapping_to_simulation:\n",
    "        \n",
    "        subdata = guttrsnp_distances_simulation[\n",
    "            (guttrsnp_distances_simulation['Simulated Taxon'] == mapping_to_simulation[taxon])&\n",
    "            (guttrsnp_distances_simulation['Taxon'] == taxontable['Taxon'].values[0])\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        plot = (plot |(alt.Chart(\n",
    "            subdata[subdata['GutTrSnp Distance'] != 0],height=300,title='Simulated Data'\n",
    "        ).mark_bar().encode(\n",
    "            x=alt.X('GutTrSnp Distance',bin=alt.Bin(maxbins=50),title='Distance'),\n",
    "            y=alt.Y('count()',axis=alt.Axis(tickMinStep=1))\n",
    "        )+max_line))\n",
    "        \n",
    "    plots.append(\n",
    "        plot.properties(title=taxon)\n",
    "    )\n",
    "\n",
    "\n",
    "          \n",
    "chart = reduce(lambda x,y : x& y,plots)\n",
    "\n",
    "chart.save(output_folder+'/Overview.html')\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of manually curated shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = pd.read_excel('Input/Annotations/Patient_Statistics (2).xlsx',dtype={'Pat ID' : str})\n",
    "outcomes_reduced = outcomes[\n",
    "    ['Pat ID',\n",
    "     'Day relative to HSCT',\n",
    "     'Day relative to HSCT.1',\n",
    "     'Day relative to HSCT.2',\n",
    "     'Day relative to HSCT.3',\n",
    "     'Day relative to HSCT.4',\n",
    "     'Day relative to HSCT.5',\n",
    "     'Day relative to HSCT.6',\n",
    "     'Day relative to HSCT.7',\n",
    "    ]\n",
    "         ].rename(\n",
    "    columns={\n",
    "        'Day relative to HSCT' : '1st Relapse',\n",
    "        'Day relative to HSCT.1' : '2nd Relapse',\n",
    "        'Day relative to HSCT.2' : '2nd HSCT',\n",
    "        'Day relative to HSCT.3' : 'Acute GvHD Grade 1-2',\n",
    "        'Day relative to HSCT.4' : 'Acute GvHD Grade 3-4',\n",
    "        'Day relative to HSCT.5' : 'Moderate cGvHD',\n",
    "        'Day relative to HSCT.6' : 'Severe cGvHD',\n",
    "        'Day relative to HSCT.7' : 'Death',\n",
    "    }\n",
    ")\n",
    "outcomes_reduced\n",
    "\n",
    "outcomes_reduced = outcomes_reduced.melt(id_vars=['Pat ID'])\n",
    "outcomes_reduced = outcomes_reduced[(outcomes_reduced['value']==outcomes_reduced['value'])]\n",
    "\n",
    "outcomes_reduced.loc[\n",
    "    (outcomes_reduced['Pat ID']=='18.2'),'KI ID'\n",
    "] = '18.2'\n",
    "outcomes_reduced.loc[(outcomes_reduced['Pat ID']=='18.2'),'value'] -= OFFSET_PAT_18\n",
    "outcomes_reduced = outcomes_reduced[['Pat ID','variable','value']]\n",
    "outcomes_reduced = outcomes_reduced[outcomes_reduced['value'] != '?']\n",
    "\n",
    "outcomes_reduced\n",
    "\n",
    "manual_curation = pd.read_csv('Input/curation_reduced.csv')\n",
    "manual_curation['Distance Name'] = manual_curation['Taxon Name']+':'+manual_curation['Source']+'->'+manual_curation['Destination']\n",
    "manual_curation['Time X'] = manual_curation['Source'].str.split('/',expand=True)[1].astype(int)\n",
    "manual_curation['Time Y'] = manual_curation['Destination'].str.split('/',expand=True)[1].astype(int)\n",
    "manual_curation['Pat ID'] = manual_curation['Source'].str.split('/',expand=True)[0]\n",
    "manual_curation['Any Annotation'] = manual_curation['Clinical Annotation']!='No events tracked'\n",
    "\n",
    "sample_times = (manual_curation.groupby(['Pat ID','Taxon Name'])['Time X'].apply(list)+manual_curation.groupby(['Pat ID','Taxon Name'])['Time Y'].apply(list)\n",
    ").reset_index().explode(0).rename(columns={0:'Time'}).drop_duplicates()\n",
    "\n",
    "charts=[]\n",
    "\n",
    "missing_samples = pd.DataFrame([(x) for x in PAPER_SAMPLES],columns=['Sample'])\n",
    "missing_samples[['Patient','Time']] = missing_samples['Sample'].str.split('/',expand=True)\n",
    "missing_samples\n",
    "\n",
    "for taxon in manual_curation['Taxon Name'].unique():\n",
    "    st = manual_curation[manual_curation['Taxon Name'] == taxon]\n",
    "    orm = outcomes_reduced[outcomes_reduced['Pat ID'].isin(st['Pat ID'].unique())]\n",
    "    \n",
    "    lm = missing_samples[\n",
    "        (missing_samples['Patient'].isin(st['Pat ID'].unique()))&\n",
    "        (~missing_samples['Sample'].isin(st['Source'].unique()))\n",
    "    ]\n",
    "    \n",
    "    if len(lm) != 0:\n",
    "        print(taxon,lm)\n",
    "    \n",
    "    c = (alt.Chart(st).mark_line().encode(\n",
    "        x=alt.X('Time X',title=None,scale=alt.Scale(type='symlog')),\n",
    "        x2=alt.X2('Time Y',title=None),\n",
    "        y=alt.Y('Pat ID',title='Patient'),\n",
    "        color=alt.Color('Manual Curation',scale=alt.Scale(\n",
    "            domain=['Shift','Stable'],range=['Orange','Grey']\n",
    "        ))\n",
    "    )+alt.Chart(orm).mark_point(color='black',size=56).encode(\n",
    "        x=alt.X('value',title='Day'),\n",
    "        y=alt.Y('Pat ID',title='Patient'),\n",
    "        shape=alt.Shape('variable',\n",
    "                       scale=alt.Scale(\n",
    "    domain=['2nd HSCT','Acute GvHD Grade 1-2','Acute GvHD Grade 3-4','Moderate cGvHD','Severe cGvHD','1st Relapse','2nd Relapse','Death'],\n",
    "        range=['circle','square','square','diamond','diamond','triangle','triangle','cross'])\n",
    "                       )\n",
    "    )+alt.Chart(pd.DataFrame([(0)],columns=['x'])).mark_rule().encode(x='x')+alt.Chart(\n",
    "    sample_times[sample_times['Taxon Name'] == taxon]\n",
    "\n",
    "    ).mark_point(color='black',filled=True).encode(\n",
    "        y='Pat ID',\n",
    "        x=alt.X('Time',title=None)\n",
    "    )+alt.Chart(lm).mark_point(color='grey',filled=True).encode(\n",
    "        y='Patient:N',\n",
    "        x=alt.X('Time:Q',title=None)\n",
    "    )).resolve_scale(x='shared')\n",
    "    charts.append(c.properties(title=taxon,width=800))\n",
    "    \n",
    "reduce(lambda x,y : x&y, charts).resolve_scale(x='shared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    len(manual_curation),\n",
    "    len(manual_curation['Taxon Name'].unique()),\n",
    "    len(manual_curation['Pat ID'].unique())\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXIMITY = 0\n",
    "\n",
    "def decide_proximity(row):\n",
    "\n",
    "    st = outcomes_reduced[outcomes_reduced['Pat ID'] == row['Pat ID']]\n",
    "    for idx,orow in st.iterrows():\n",
    "        if (orow['variable'] in ['1st Relapse','2nd Relapse']):\n",
    "            if row['Time X']-PROXIMITY <= orow['value'] and row['Time Y']+PROXIMITY >= orow['value']:\n",
    "                return True\n",
    "\n",
    "    if row['Across TX'] in ['True','Yes']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "manual_curation['Proximity to Relapse/aHSCT'] = manual_curation.apply(\n",
    "    decide_proximity,axis=1\n",
    ")\n",
    "\n",
    "manual_curation.groupby(['Proximity to Relapse/aHSCT','Manual Curation'])['Pat ID'].count().rename('Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_curation.to_csv('manual_cur_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency, fisher_exact\n",
    "\n",
    "ct = pd.crosstab(manual_curation['Proximity to Relapse/aHSCT'],manual_curation['Manual Curation'])\n",
    "c, p, dof, expected = chi2_contingency(ct)\n",
    "print(p)\n",
    "oddsratio,p = fisher_exact(ct)\n",
    "print(oddsratio,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = 'G11_-100'\n",
    "TAXON = PLANTAE\n",
    "EXCLUDE = []\n",
    "\n",
    "subtable = validation_data[validation_data['Sample'] == SAMPLE]\n",
    "subtable = subtable[subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,TAXON))]\n",
    "for ftaxon in EXCLUDE:\n",
    "    subtable = subtable[~subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,ftaxon))]\n",
    "subtable['Validated'] = (subtable['Validation Rate'].astype(float) >= 0.2)\n",
    "weighted_validation_rate= (\n",
    "    subtable['Validated']*subtable['Reads']\n",
    ").sum()/subtable['Reads'].sum()\n",
    "print('Weighted Validation Rate: {} (>= 80% validated)'.format(weighted_validation_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Rate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('Output/ValidationRatePlots',exist_ok=True)\n",
    "\n",
    "GROUPS = [\n",
    "    (VIRUSES,[]),\n",
    "    (EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]),\n",
    "    (FUNGI,[]),\n",
    "    (BACTERIA,[]),\n",
    "    (PLANTAE,[]),\n",
    "    (ARCHAEA,[]),\n",
    "    (CHORDATA,[]),\n",
    "    (PLANTAE,[]),\n",
    "]\n",
    "\n",
    "for group in GROUPS:\n",
    "    taxon,excludes = group\n",
    "    subtable = validation_data[validation_data['Taxon ID'].apply(lambda x : is_below_or_equal(x,taxon))]\n",
    "    for ftaxon in excludes:\n",
    "        subtable = subtable[~subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,ftaxon))]\n",
    "    topxtaxa = subtable.groupby('Taxon Name')['Reads'].sum().sort_values()[-30:].keys()\n",
    "    subtable = subtable[subtable['Taxon Name'].isin(topxtaxa)]\n",
    "    subtable['Validated'] = (subtable['Validation Rate'].astype(float) >= 0.2)\n",
    "    subtable['Validated Reads'] = subtable['Validated'] * subtable['Reads']\n",
    "\n",
    "    charts = []\n",
    "    charts.append(\n",
    "        alt.Chart(\n",
    "            subtable\n",
    "        ).mark_boxplot().encode(\n",
    "            x=alt.X('Taxon Name',sort=subtable.groupby('Taxon Name')['Reads'].sum().sort_values().keys().tolist()[::-1]),\n",
    "            y=alt.Y('Validation Rate')\n",
    "        )\n",
    "    )\n",
    "    for genus in topxtaxa[::-1]:\n",
    "        charts.append(\n",
    "            alt.Chart(\n",
    "                subtable[subtable['Taxon Name']==genus],title=genus\n",
    "            ).mark_point().encode(\n",
    "                x=alt.X('Fraction Estimate',scale=alt.Scale(type='log')),\n",
    "                y=alt.Y('Validation Rate',scale=alt.Scale(domain=[0,1])),\n",
    "                tooltip=['Sample','Validation Rate']\n",
    "            )\n",
    "        )\n",
    "\n",
    "    combined_title = idtonames[taxon]\n",
    "    if excludes != []:\n",
    "        combined_title += ' without ' + ','.join(idtonames[x] for x in excludes)\n",
    "        \n",
    "\n",
    "    reduce(lambda x,y : x&y, charts).properties(title=combined_title).save('Output/ValidationRatePlots/{}.html'.format(combined_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('Output/ValidationRatePlots',exist_ok=True)\n",
    "\n",
    "GROUPS = [\n",
    "    (VIRUSES,[]),\n",
    "    (EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]),\n",
    "    (FUNGI,[]),\n",
    "    (BACTERIA,[]),\n",
    "    (PLANTAE,[]),\n",
    "    (ARCHAEA,[]),\n",
    "    (CHORDATA,[]),\n",
    "    (PLANTAE,[])\n",
    "]\n",
    "\n",
    "PAPER_SAMPLES_UNDERSCORE = [x.replace('/','_') for x in PAPER_SAMPLES]\n",
    "\n",
    "print('Continuous')\n",
    "for group in GROUPS:\n",
    "    taxon,excludes = group\n",
    "    subtable = validation_data[validation_data['Taxon ID'].apply(lambda x : is_below_or_equal(x,taxon))]\n",
    "    #reduce to paper samples only\n",
    "    subtable = subtable[subtable['Sample'].isin(PAPER_SAMPLES_UNDERSCORE)]\n",
    "    for ftaxon in excludes:\n",
    "        subtable = subtable[~subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,ftaxon))]\n",
    "    subtable['Validated Reads'] = subtable['Validation Rate'] * subtable['Reads']\n",
    "\n",
    "    print(idtonames[group[0]],\n",
    "        (subtable.groupby('Sample')['Validated Reads'].sum()/subtable.groupby('Sample')['Reads'].sum()).median()\n",
    "         )\n",
    "print('Binary') \n",
    "for group in GROUPS:\n",
    "    taxon,excludes = group\n",
    "    subtable = validation_data[validation_data['Taxon ID'].apply(lambda x : is_below_or_equal(x,taxon))]\n",
    "    #reduce to paper samples only\n",
    "    subtable = subtable[subtable['Sample'].isin(PAPER_SAMPLES_UNDERSCORE)]\n",
    "    for ftaxon in excludes:\n",
    "        subtable = subtable[~subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,ftaxon))]\n",
    "    subtable['Validated Reads'] = (subtable['Validation Rate']>=0.2) * subtable['Reads']\n",
    "\n",
    "    print(idtonames[group[0]],\n",
    "        (subtable.groupby('Sample')['Validated Reads'].sum()/subtable.groupby('Sample')['Reads'].sum()).median()\n",
    "         )    \n",
    "subtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Fungal Eukaryota Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_table = get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    level='G',\n",
    "    samples=PAPER_SAMPLES,\n",
    "    included_taxa_filter=None,\n",
    "    excluded_taxa_filter=None,\n",
    "    normalize=False\n",
    ")\n",
    "input_table['Read Fraction'] = input_table['readcount'] / input_table.groupby(['time','patientid'])['readcount'].transform('sum')\n",
    "\n",
    "taxon,excludes = EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]\n",
    "subtable = validation_data[validation_data['Taxon ID'].apply(lambda x : is_below_or_equal(x,taxon))]\n",
    "#reduce to paper samples only\n",
    "subtable = subtable[subtable['Sample'].isin(PAPER_SAMPLES_UNDERSCORE)]\n",
    "for ftaxon in excludes:\n",
    "    subtable = subtable[~subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,ftaxon))]\n",
    "subtable['Validated Reads'] = (subtable['Validation Rate']>=0.2) * subtable['Reads']\n",
    "subtable = (subtable[subtable['Validated Reads'] > 0])\n",
    "subtable = pd.merge(\n",
    "    subtable[['patientid','time','Validation Rate','Taxon ID']],\n",
    "    input_table,\n",
    "    left_on=['patientid','time','Taxon ID'],\n",
    "    right_on=['patientid','time','taxonid'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtable_sub20 = subtable[subtable['readcount'] >= 20]\n",
    "subtable_sub20['sample'].unique()\n",
    "#subtable_sub20['taxon'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(subtable['sample'].unique()))\n",
    "print(subtable['taxon'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Plots Per Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUPS = {\n",
    "    'Bacteria':(BACTERIA,[]),\n",
    "    'Fungi':(FUNGI,[]),\n",
    "    'OtherEukaryota':(EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]),\n",
    "    'Viruses':(VIRUSES,[]),\n",
    "    'Archaea':(ARCHAEA,[]),\n",
    "    'Homo':(HOMO,[]),\n",
    "    'Plants':(PLANTAE,[]),\n",
    "}\n",
    "\n",
    "Gruppen = list(GROUPS.keys())\n",
    "\n",
    "\n",
    "x=alt.X(\"domain:O\", title=None, axis=alt.Axis(labels=False, ticks=False), scale=alt.Scale(paddingInner=1), sort=Gruppen),    \n",
    "y=alt.Y(\"value:Q\",title='Fraction [%]',scale=alt.Scale(type='symlog',domain=[0,100]), axis=alt.Axis(grid=False,minExtent=40, values=[0.1,0.5,1,5,10,20,50,100])), \n",
    "\n",
    "\n",
    "tables = []\n",
    "sample_statistics['samplename'] = sample_statistics['PatID']+'_'+sample_statistics['time'].astype(str)\n",
    "\n",
    "for groupname,group in GROUPS.items():\n",
    "    taxon,excludes = group\n",
    "    subtable = validation_data['S'][validation_data['S']['Taxon ID'].apply(lambda x : is_below_or_equal(x,taxon))]\n",
    "    for ftaxon in excludes:\n",
    "        subtable = subtable[~subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,ftaxon))]\n",
    "    \n",
    "\n",
    "    \n",
    "    subtable['Validated'] = (subtable['Validation Rate'].astype(float) >= 0.2)\n",
    "    subtable['Validated Reads Binary'] = subtable['Validated'] * subtable['Reads']\n",
    "    subtable['Validated Reads Continuous'] = subtable['Validation Rate'] * subtable['Reads']\n",
    "    \n",
    "    subtable = subtable.groupby('samplename',as_index=False)[['Validated Reads Binary','Validated Reads Continuous','Reads']].sum()\n",
    "\n",
    "    subtable['Validated Read Fraction Binary'] = subtable['Validated Reads Binary'] / subtable['Reads']\n",
    "    subtable['Validated Read Fraction Continuous'] = subtable['Validated Reads Continuous'] / subtable['Reads']\n",
    "\n",
    "        \n",
    "    subtable['Domain'] = groupname\n",
    "\n",
    "    tables.append(subtable)\n",
    "    \n",
    "combined = pd.merge(\n",
    "    pd.concat(tables),\n",
    "    sample_statistics[['samplename','timephase']],\n",
    "    on=['samplename'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "#filter null group\n",
    "combined = combined[combined['timephase']==combined['timephase']]\n",
    "\n",
    "\n",
    "(alt.Chart(combined,height=400,title='Binary').mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    x=alt.X('Domain',sort=list(GROUPS.keys())),\n",
    "    column=alt.Column(\n",
    "        'timephase:O',\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None,\n",
    "    ),    y=alt.Y('Validated Read Fraction Binary',scale=alt.Scale(type='linear',domain=[0,1.1])),\n",
    "        color=alt.Color(\n",
    "        \"Domain:N\",\n",
    "        sort=Gruppen,\n",
    "        legend=alt.Legend(title=None,orient='top'),\n",
    "        scale=alt.Scale(domain=list(GROUPS.keys()),range=[ '#EE6677','#fecc5c','#228833', '#66CCEE', '#AA3377','lightgrey', 'darkgrey'])\n",
    "    )\n",
    ")&alt.Chart(combined,height=400,title='Continuous').mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    x=alt.X('Domain',sort=list(GROUPS.keys())),\n",
    "    column=alt.Column(\n",
    "        'timephase:O',\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None\n",
    "    ),    y=alt.Y('Validated Read Fraction Continuous',scale=alt.Scale(type='linear',domain=[0,1.1])),\n",
    "    color=alt.Color(\n",
    "        \"Domain:N\",\n",
    "        sort=Gruppen,\n",
    "        legend=alt.Legend(title=None,orient='top'),\n",
    "        scale=alt.Scale(domain=list(GROUPS.keys()),range=[ '#EE6677','#fecc5c','#228833', '#66CCEE', '#AA3377','lightgrey', 'darkgrey'])\n",
    "    )\n",
    ")).save('Output/ValidatedReadsPerDomainOverview.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[combined['Domain']=='Viruses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.groupby('Domain')['Validated Read Fraction Continuous'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Validated Taxa Per Domain Stratified By PCoA Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_X = 20\n",
    "\n",
    "GROUPS = [\n",
    "    (VIRUSES,[]),\n",
    "    (EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]),\n",
    "    (FUNGI,[]),\n",
    "    (BACTERIA,[]),\n",
    "    (PLANTAE,[]),\n",
    "    (ARCHAEA,[]),\n",
    "    (CHORDATA,[]),\n",
    "    (PLANTAE,[]),\n",
    "    ('1',[CHORDATA,PLANTAE])\n",
    "]\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "os.makedirs('top_validated_taxa',exist_ok=True)\n",
    "\n",
    "tables = []\n",
    "\n",
    "for group in GROUPS:\n",
    "    taxon,excludes = group\n",
    "    subtable = validation_data[validation_data['Taxon ID'].apply(lambda x : is_not_below(x,taxon))]\n",
    "    for ftaxon in excludes:\n",
    "        subtable = subtable[~subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,ftaxon))]\n",
    "    subtable['Validated'] = (subtable['Validation Rate'].astype(float) >= 0.2)\n",
    "    subtable['Validated Reads Binary'] = subtable['Validated'] * subtable['Reads']\n",
    "    subtable['Validated Reads Continuous'] = subtable['Validation Rate'] * subtable['Reads']\n",
    "\n",
    "    subtable = subtable.groupby(['Sample','Taxon ID'],as_index=False)[['Validated Reads Binary','Validated Reads Continuous','Reads']].sum()\n",
    "\n",
    "    subtable['Validated Read Fraction Binary'] = subtable['Validated Reads Binary'] / subtable['Reads']\n",
    "    subtable['Validated Read Fraction Continuous'] = subtable['Validated Reads Continuous'] / subtable['Reads']\n",
    "    \n",
    "\n",
    "    combined_title = idtonames[taxon]\n",
    "    if excludes != []:\n",
    "        combined_title += ' without ' + ','.join(idtonames[x] for x in excludes)\n",
    "        \n",
    "    subtable['Domain'] = combined_title\n",
    "\n",
    "    tables.append(subtable)\n",
    "    \n",
    "combined = pd.concat(tables)\n",
    "\n",
    "sample_statistics['Sample'] = sample_statistics['PatID']+'_'+sample_statistics['time'].astype(str)\n",
    "combined = pd.merge(\n",
    "    combined,\n",
    "    sample_statistics[['Sample','leukocytephase_cluster_kurz']],\n",
    "    on=['Sample'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "for domain in combined['Domain'].unique():\n",
    "    subtable = combined[combined['Domain'] == domain]\n",
    "    subtable['Taxon Name'] = subtable['Taxon ID'].map(idtonames)\n",
    "    \n",
    "    taxa_we_look_at = list(subtable.groupby('Taxon Name')['Validated Read Fraction Binary'].sum().sort_values(ascending=False)[:(TOP_X+1)].keys())\n",
    "\n",
    "    taxatable = subtable[subtable['Taxon Name'].isin(taxa_we_look_at)]\n",
    "    c1=alt.Chart(taxatable,title='Binary').mark_rect().encode(\n",
    "        x='Taxon Name',\n",
    "        y='leukocytephase_cluster_kurz',\n",
    "        color='mean(Validated Read Fraction Binary)'\n",
    "    )\n",
    "\n",
    "    taxa_we_look_at = list(subtable.groupby('Taxon Name')['Validated Read Fraction Continuous'].sum().sort_values(ascending=False)[:(TOP_X+1)].keys())\n",
    "\n",
    "    taxatable = subtable[subtable['Taxon Name'].isin(taxa_we_look_at)]\n",
    "    c2=alt.Chart(taxatable,title='Continuous').mark_rect().encode(\n",
    "        x='Taxon Name',\n",
    "        y='leukocytephase_cluster_kurz',\n",
    "        color='mean(Validated Read Fraction Continuous)'\n",
    "    )\n",
    "    \n",
    "    (c1|c2).resolve_scale(color='independent').save('top_validated_taxa/{}_top_{}.png'.format(domain,TOP_X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Top Genera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GROUPS = [\n",
    "    (BACTERIA,[]),\n",
    "    (FUNGI,[]),\n",
    "    (EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]),\n",
    "    (VIRUSES,[]),\n",
    "    (ARCHAEA,[]),\n",
    "    (PLANTAE,[]),\n",
    "    (CHORDATA,[]),\n",
    "    (PLANTAE,[]),\n",
    "    ('1',[CHORDATA,FUNGI,PLANTAE])\n",
    "]\n",
    "\n",
    "PAPER_SAMPLES_UNDERSCORE = [x.replace('/','_') for x in PAPER_SAMPLES]\n",
    "\n",
    "os.makedirs('top_validated_taxa',exist_ok=True)\n",
    "sample_statistics['Sample'] = sample_statistics['PatID']+'_'+sample_statistics['time'].astype(str)\n",
    "\n",
    "tables = []\n",
    "\n",
    "total_reads = validation_data.groupby('Sample')['Reads'].sum()\n",
    "all_charts = []\n",
    "\n",
    "\n",
    "for group in GROUPS:\n",
    "    taxon,excludes = group\n",
    "    subtable = validation_data[validation_data['Taxon ID'].apply(lambda x : is_below_or_equal(x,taxon))]\n",
    "    for ftaxon in excludes:\n",
    "        subtable = subtable[~subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,ftaxon))]\n",
    "    subtable['Validated'] = (subtable['Validation Rate'].astype(float) >= 0.2)\n",
    "    subtable['Validated Reads Binary'] = subtable['Validated'] * subtable['Reads']\n",
    "    #subtable['Validated Reads Continuous'] = subtable['Validation Rate'] * subtable['Reads']\n",
    "\n",
    "    subtable = subtable.groupby(['Sample','Taxon ID'],as_index=False)[['Validated Reads Binary',\n",
    "                                                                       #'Validated Reads Continuous',\n",
    "                                                                       'Reads','Validated']].sum()\n",
    "\n",
    "    local_reads = subtable.groupby('Sample')['Reads'].sum()\n",
    "    \n",
    "    subtable['Validated Local Read Fraction Binary'] = subtable.apply(\n",
    "        lambda x:x['Validated Reads Binary']/local_reads[x['Sample']],axis=1\n",
    "    )\n",
    "    #subtable['Validated Local Read Fraction Continuous'] = subtable['Validated Reads Continuous'] / subtable['Reads']\n",
    "    \n",
    "    subtable['Validated Global Read Fraction Binary'] = subtable.apply(\n",
    "        lambda x:x['Validated Reads Binary']/total_reads[x['Sample']],axis=1\n",
    "    )\n",
    "    #subtable['Validated Global Read Fraction Continuous'] = subtable.apply(\n",
    "    #    lambda x:x['Validated Reads Continuous']/total_reads[x['Sample']],axis=1\n",
    "    #)        \n",
    "    \n",
    "    \n",
    "    combined_title = idtonames[taxon]\n",
    "    if excludes != []:\n",
    "        combined_title += ' without ' + ','.join(idtonames[x] for x in excludes)\n",
    "    \n",
    "    subtable['Validated Above 5% Binary Local'] = subtable['Validated Local Read Fraction Binary'] > 0.05\n",
    "    subtable['Validated Above 5% Binary Global'] = subtable['Validated Global Read Fraction Binary'] > 0.05\n",
    "    #subtable['Validated Above 5% Continuous Local'] = subtable['Validated Local Read Fraction Continuous'] > 0.05\n",
    "    #subtable['Validated Above 5% Continuous Global'] = subtable['Validated Global Read Fraction Continuous'] > 0.05\n",
    "    subtable = pd.merge(\n",
    "        subtable,\n",
    "        sample_statistics[['Sample','leukocytephase_cluster_2_kurz','Startcluster']],\n",
    "        on=['Sample'],\n",
    "        how='left'\n",
    "    )\n",
    "    subtable['Startcluster'] = subtable['Startcluster'].replace({'Gesund':'Control'})\n",
    "    subtable = subtable.rename(columns={'Validated' : 'Detected'})\n",
    "    \n",
    "    totals =subtable.groupby(['Taxon ID'])[[\n",
    "    'Detected',\n",
    "    ]].sum().rename(columns={'Detected' : 'Absolute Detected'})\n",
    "\n",
    "    short =(subtable.groupby(['Taxon ID'])[[\n",
    "    'Detected',\n",
    "    'Validated Above 5% Binary Local',\n",
    "    'Validated Above 5% Binary Global',\n",
    "    #'Validated Above 5% Continuous Local',\n",
    "    #'Validated Above 5% Continuous Global',\n",
    "    ]].sum()/len(validation_data['Sample'].unique())).add_prefix('Total ').fillna(0)\n",
    "\n",
    "    short = pd.concat([totals,short],axis=1)\n",
    "\n",
    "    long = subtable.groupby(['leukocytephase_cluster_2_kurz','Taxon ID'])[[\n",
    "    'Detected',\n",
    "    'Validated Above 5% Binary Local',\n",
    "    'Validated Above 5% Binary Global',\n",
    "    #'Validated Above 5% Continuous Local',\n",
    "    #'Validated Above 5% Continuous Global'\n",
    "    ]].sum().reset_index()\n",
    "\n",
    "    samples_per_group = sample_statistics.groupby(['leukocytephase_cluster_2_kurz'])[\n",
    "    'Sample'\n",
    "    ].count().reset_index()\n",
    "\n",
    "    long = pd.merge(long,samples_per_group,how='left',on='leukocytephase_cluster_2_kurz')\n",
    "\n",
    "    long['Detected'] = long['Detected'] / long['Sample']\n",
    "    long['Validated Above 5% Binary Local'] = long['Validated Above 5% Binary Local'] / long['Sample']\n",
    "    long['Validated Above 5% Binary Global'] = long['Validated Above 5% Binary Global'] / long['Sample']\n",
    "    #long['Validated Above 5% Continuous Local'] = long['Validated Above 5% Continuous Local'] / long['Sample']\n",
    "    #long['Validated Above 5% Continuous Global'] = long['Validated Above 5% Continuous Global'] / long['Sample']\n",
    "    long = long.drop(columns='Sample')\n",
    "\n",
    "    long = long.pivot(index='Taxon ID',columns='leukocytephase_cluster_2_kurz').fillna(0).swaplevel(0,1,1)\n",
    "\n",
    "    \n",
    "    long_2 = subtable.groupby(['Startcluster','Taxon ID'])[[\n",
    "    'Detected',\n",
    "    'Validated Above 5% Binary Local',\n",
    "    'Validated Above 5% Binary Global',\n",
    "    #'Validated Above 5% Continuous Local',\n",
    "    #'Validated Above 5% Continuous Global'\n",
    "    ]].sum().reset_index()\n",
    "\n",
    "    samples_per_group = sample_statistics.groupby(['Startcluster'])[\n",
    "    'Sample'\n",
    "    ].count().reset_index()\n",
    "\n",
    "    long_2 = pd.merge(long_2,samples_per_group,how='left',on='Startcluster')\n",
    "\n",
    "    long_2['Detected'] = long_2['Detected'] / long_2['Sample']\n",
    "    long_2['Validated Above 5% Binary Local'] = long_2['Validated Above 5% Binary Local'] / long_2['Sample']\n",
    "    long_2['Validated Above 5% Binary Global'] = long_2['Validated Above 5% Binary Global'] / long_2['Sample']\n",
    "    #long['Validated Above 5% Continuous Local'] = long['Validated Above 5% Continuous Local'] / long['Sample']\n",
    "    #long['Validated Above 5% Continuous Global'] = long['Validated Above 5% Continuous Global'] / long['Sample']\n",
    "    long_2 = long_2.drop(columns='Sample')\n",
    "\n",
    "    long_2 = long_2.pivot(index='Taxon ID',columns='Startcluster').fillna(0).swaplevel(0,1,1)\n",
    "\n",
    "    long=pd.merge(long,long_2,left_index=True,right_index=True,how='outer')\n",
    "        \n",
    "    combined = pd.merge(\n",
    "        short,long,left_index=True,right_index=True,how='outer'\n",
    "    ).sort_values(\n",
    "        by='Total Detected',ascending=False\n",
    "    )\n",
    "    combined = combined[[                    'Absolute Detected',\n",
    "                                                 'Total Detected',\n",
    "                          'Total Validated Above 5% Binary Local',\n",
    "                         'Total Validated Above 5% Binary Global',\n",
    "                                          ('Healthy', 'Detected'),\n",
    "                                        ('Healthy', 'Validated Above 5% Binary Local'),\n",
    "                  ('Healthy', 'Validated Above 5% Binary Global'),\n",
    "                                           ('Pre TX', 'Detected'),\n",
    "                    ('Pre TX', 'Validated Above 5% Binary Local'),\n",
    "                   ('Pre TX', 'Validated Above 5% Binary Global'),\n",
    "\n",
    "                                   ('Leukozytopenia', 'Detected'),\n",
    "                                 ('Leukozytopenia', 'Validated Above 5% Binary Local'),\n",
    "           ('Leukozytopenia', 'Validated Above 5% Binary Global'),\n",
    "\n",
    "                                   ('Reconstitution', 'Detected'),\n",
    "            ('Reconstitution', 'Validated Above 5% Binary Local'),\n",
    "           ('Reconstitution', 'Validated Above 5% Binary Global'),\n",
    "                                                  (1, 'Detected'),\n",
    "\n",
    "                           (1, 'Validated Above 5% Binary Local'),\n",
    "                                               (1, 'Validated Above 5% Binary Global'),\n",
    "\n",
    "                                                                       (2, 'Detected'),\n",
    "                                                (2, 'Validated Above 5% Binary Local'),\n",
    "                          (2, 'Validated Above 5% Binary Global'),\n",
    "\n",
    "                                                  (3, 'Detected'),\n",
    "                           (3, 'Validated Above 5% Binary Local'),\n",
    "                          (3, 'Validated Above 5% Binary Global')]\n",
    "    ]\n",
    "    for column in combined.columns:\n",
    "        if column != 'Absolute Detected':\n",
    "            combined[column] = combined[column].astype(float).map(\"{:.2%}\".format)\n",
    "    combined = combined[combined['Absolute Detected'] > 0]\n",
    "            \n",
    "    excel = combined[:]\n",
    "    excel.index = excel.index.map(idtonames)\n",
    "    excel.to_excel('top_validated_taxa/{}.xlsx'.format(combined_title))\n",
    "    \n",
    "    #Chart\n",
    "    \n",
    "    marker_data = validation_data\n",
    "    marker_data = marker_data[marker_data['Sample'].isin(PAPER_SAMPLES_UNDERSCORE)]\n",
    "    marker_data['Validated'] = marker_data['Validation Rate'] >= 0.2\n",
    "\n",
    "    os.makedirs('marker_genera_overview',exist_ok=True)\n",
    "\n",
    "\n",
    "    def SORTFUNCTION(x):\n",
    "        return (\n",
    "            x[0] == 'G', #Sort by G or regular patient first\n",
    "            float(x.split('G')[-1])\n",
    "        )\n",
    "\n",
    "    patientlist_sorted = sorted(\n",
    "        marker_data['patientid'].unique(),key=SORTFUNCTION\n",
    "    )\n",
    "\n",
    "\n",
    "    tables = []\n",
    "\n",
    "    for taxon in combined.index[:10]:\n",
    "        \n",
    "        subtable = marker_data[\n",
    "            marker_data['Taxon ID'] == taxon\n",
    "        ]\n",
    "\n",
    "\n",
    "        substitutes = []\n",
    "        for sample in PAPER_SAMPLES_UNDERSCORE:\n",
    "            if sample not in subtable['Sample'].unique():\n",
    "                patientid,time = sample.rsplit('_',1)\n",
    "                substitutes.append(\n",
    "                    (patientid,int(time),'?')\n",
    "                )\n",
    "        subtable = pd.concat([subtable,pd.DataFrame(substitutes,columns=['patientid','time','Validated'])]) \n",
    "\n",
    "\n",
    "        occurences = subtable[subtable['Validated']==True].groupby('patientid')['Validated'].count()\n",
    "        interesting_patients = occurences[occurences >= 1].keys()\n",
    "        subtable = subtable[subtable['patientid'].isin(interesting_patients)]\n",
    "\n",
    "        subtable['Validated'] = subtable['Validated'].map(\n",
    "            {\n",
    "                '?' : 'Not Validated',\n",
    "                False : 'Not Validated',\n",
    "                True : 'Validated'\n",
    "            }\n",
    "        )\n",
    "\n",
    "        subtable['Genus'] = idtonames[taxon] if taxon in idtonames else str(taxon) \n",
    "\n",
    "        #Filter patients without any presence\n",
    "\n",
    "        tables.append(subtable)\n",
    "    charttable = pd.concat(tables)\n",
    "    charts=[]\n",
    "    for genus in list(combined.index[:10]):\n",
    "        genustable = charttable[charttable['Taxon ID'] == genus]\n",
    "        if len(genustable) == 0:\n",
    "            continue\n",
    "        chart = (alt.Chart(genustable[~genustable['patientid'].str.startswith('G')],title=idtonames[genus]).mark_point().encode(\n",
    "            x=alt.X('time',scale=alt.Scale(type='symlog'),axis=alt.Axis(grid=False),title='Day'),\n",
    "                color=alt.Color(\n",
    "                    'Validated',scale=alt.Scale(domain=['Not Validated','Validated'],range=['lightgrey','orange'])),\n",
    "            y=alt.Y('patientid:N',sort=patientlist_sorted,axis=alt.Axis(grid=True),title=None)\n",
    "            )|\n",
    "\n",
    "            alt.Chart(genustable[genustable['patientid'].str.startswith('G')]).mark_point().encode(            color=alt.Color(\n",
    "                    'Validated',scale=alt.Scale(domain=['Not Validated','Validated'],range=['lightgrey','orange'])),\n",
    "            y=alt.Y('patientid:N',sort=patientlist_sorted,title=None)\n",
    "            )).resolve_scale(y='independent')   \n",
    "        \n",
    "        charts.append(chart)\n",
    "        if group in [\n",
    "            (BACTERIA,[]),\n",
    "            (FUNGI,[]),\n",
    "            (EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]),\n",
    "            (VIRUSES,[]),\n",
    "            (ARCHAEA,[]),\n",
    "            (PLANTAE,[]),\n",
    "            (CHORDATA,[]),\n",
    "            (PLANTAE,[])\n",
    "        ]:\n",
    "            all_charts.append((chart,len(genustable['patientid'].unique())))\n",
    "    reduce(lambda x,y : x&y,charts).save('top_validated_taxa/{}.svg'.format(combined_title))\n",
    "    reduce(lambda x,y : x&y,charts).save('top_validated_taxa/{}.png'.format(combined_title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_charts),sum(x[1] for x in all_charts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENT_MAX = 200\n",
    "FIXED_SPACE_PER_PLOT = 10\n",
    "\n",
    "cur = 0\n",
    "idx = 0\n",
    "\n",
    "column_charts = []\n",
    "column_chart = []\n",
    "\n",
    "while(idx < len(all_charts)):\n",
    "    chart,patients = all_charts[idx]\n",
    "    \n",
    "    column_chart.append(chart)\n",
    "    cur += patients+FIXED_SPACE_PER_PLOT\n",
    "    idx += 1\n",
    "    if cur >= PATIENT_MAX:\n",
    "        cur = 0\n",
    "        column_charts.append(\n",
    "            reduce(lambda x,y : x&y ,column_chart)\n",
    "        )\n",
    "        column_chart = []\n",
    "reduce(lambda x,y : x|y, column_charts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlation val rate abundance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_table = get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    level='G',\n",
    "    samples=PAPER_SAMPLES,\n",
    "    included_taxa_filter=None,\n",
    "    excluded_taxa_filter=None,\n",
    "    normalize=False\n",
    ")\n",
    "\n",
    "input_table['Read Fraction (%)'] = input_table['readcount'] / input_table.groupby(['time','patientid'])['readcount'].transform('sum')\n",
    "\n",
    "input_table['Sample'] = input_table['patientid']+'_'+input_table['time'].astype(str)\n",
    "\n",
    "\n",
    "DISCARD_CUTOFF = 20\n",
    "\n",
    "with_validation = pd.merge(\n",
    "    input_table,\n",
    "    validation_data[['Sample','Taxon ID','Validation Rate']],\n",
    "    left_on=['Sample','taxonid'],\n",
    "    right_on=['Sample','Taxon ID'],\n",
    "    how='left'\n",
    ").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_validation['Read Fraction Bin'] = pd.cut(with_validation['Read Fraction (%)'],[x*0.05 for x in range(round(1/0.05))]+[1]).astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.disable_max_rows()\n",
    "alt.Chart(\n",
    "    with_validation.dropna(),width=500\n",
    ").mark_boxplot(ticks=True).encode(\n",
    "    x=alt.X('Read Fraction Bin:O'),\n",
    "    y=alt.Y('Validation Rate')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_statistics['Sample'] = sample_statistics['PatID']+'_'+sample_statistics['time'].astype(str)\n",
    "\n",
    "t3 = pd.merge(\n",
    "    with_validation[with_validation['taxon'] == 'Candida'],\n",
    "    sample_statistics[['Sample','leukocytephase_cluster_2_kurz']],\n",
    "    how='left',\n",
    "    on='Sample'\n",
    ")\n",
    "\n",
    "t3['Validated'] = (t3['Validation Rate']>= 0.2)\n",
    "\n",
    "t3.groupby(['leukocytephase_cluster_2_kurz'])['Validated'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3.groupby(['leukocytephase_cluster_2_kurz'])['Validated'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lifelines Comparison Bacteroides Phocaeicola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_X = 30\n",
    "LEVEL = 'S'\n",
    "DISCARD_CUTOFF = 5\n",
    "\n",
    "EXCLUDE = [PLANTAE]\n",
    "INCLUDE = None\n",
    "\n",
    "SAMPLES = HEALTHY_SAMPLES+LIFELINES\n",
    "\n",
    "def SORTFUNCTION(x):\n",
    "    try:\n",
    "        return (\n",
    "            x[0] == 'G', #Sort by G or regular patient first\n",
    "            float(x.split('G')[-1].split('_')[0]), #Then Patient ID\n",
    "            int(x.split('G')[-1].split('_')[1]), #Then Time\n",
    "        )\n",
    "    except:\n",
    "        return (True,1,hash(x))\n",
    "    \n",
    "\n",
    "#################\n",
    "\n",
    "os.makedirs('Output/Lifelines',exist_ok=True)\n",
    "\n",
    "reset_frame = get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    level=LEVEL,\n",
    "    samples=[s.replace('/','_') for s in SAMPLES], #New sample format uses underscore '_' instead of '/'\n",
    "    included_taxa_filter=INCLUDE,\n",
    "    excluded_taxa_filter=EXCLUDE,\n",
    "    normalize=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_table = reset_frame.copy()\n",
    "\n",
    " #Phase 1: Kick out low abundance groups, assign to \"Not enough reads\"\n",
    "input_table.loc[input_table['readcount']<DISCARD_CUTOFF,'taxon'] = 'Not enough reads'\n",
    "input_table.loc[input_table['readcount']<DISCARD_CUTOFF,'taxonid'] = NOT_ENOUGH_READS\n",
    "\n",
    "#Readjust sum (group multiple \"Not enough reads\" entries together)\n",
    "input_table=  input_table.groupby(['taxon','taxonid','samplename'],as_index=False).sum()\n",
    "\n",
    "#Calculate Read Fractions\n",
    "input_table['Read Fraction'] = input_table['readcount']/input_table.groupby('samplename')['readcount'].transform('sum')\n",
    "\n",
    "# determine the top taxa based on means\n",
    "taxa_we_look_at = list(input_table.groupby('taxon')['Read Fraction'].sum().sort_values(ascending=False)[:(TOP_X+2)].keys())\n",
    "if 'Not enough reads' not in taxa_we_look_at:\n",
    "    taxa_we_look_at.append('Not enough reads')\n",
    "if 'Unassigned at Level' not in taxa_we_look_at:\n",
    "    taxa_we_look_at.append('Unassigned at Level')\n",
    "\n",
    "print('Determined the following taxa as relevant:',taxa_we_look_at)\n",
    "\n",
    "#assign everything else to the \"other\" group and readjust sum\n",
    "input_table.loc[~input_table['taxon'].isin(taxa_we_look_at), 'taxon'] = 'Other'\n",
    "input_table = input_table.groupby(['taxon','samplename'],as_index=False).sum()\n",
    "\n",
    "\n",
    "input_table['other'] = input_table['taxon'] == 'Other'\n",
    "\n",
    "input_table = input_table.rename(columns={\n",
    "    'samplename' : 'Sample ID',\n",
    "    'taxon' : 'Taxon'\n",
    "})\n",
    "\n",
    "colorMap = {}\n",
    "\n",
    "taxa = taxa_we_look_at+['Other']\n",
    "\n",
    "palette = cc.glasbey_light\n",
    "bright = palette[::2]\n",
    "muted = palette[1::2]\n",
    "palette = bright+muted\n",
    "\n",
    "taxa_we_look_at_assigned = taxa_we_look_at\n",
    "\n",
    "taxa_we_look_at_assigned.remove('Not enough reads')\n",
    "taxa_we_look_at_assigned.remove('Unassigned at Level')\n",
    "\n",
    "for tax,col in zip(taxa_we_look_at_assigned,palette):\n",
    "    colorMap[tax] = col #colors.to_hex(col)\n",
    "    \n",
    "altdomain = []\n",
    "altrange = []\n",
    "\n",
    "for x in taxa_we_look_at_assigned:\n",
    "\n",
    "    c = colorMap[x]\n",
    "    altdomain.append(x)\n",
    "    color = colors.to_hex(c)\n",
    "    altrange.append(color)\n",
    "    \n",
    "altdomain.append('Other')\n",
    "altrange.append(colors.to_hex((1,1,1)))\n",
    "altdomain.append('Not enough reads')\n",
    "altrange.append(colors.to_hex((0,0,0)))\n",
    "altdomain.append('Unassigned at Level')\n",
    "altrange.append(colors.to_hex((0.5,0.5,0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_table['Lifelines'] = input_table['Sample ID'].str.startswith('L')\n",
    "\n",
    "ll_table = input_table[input_table['Lifelines']]\n",
    "\n",
    "patientlist_sorted = sorted(\n",
    "    ll_table['Sample ID'].unique().tolist())\n",
    "\n",
    "lls = alt.Chart(\n",
    "  ll_table,title='Lifelines'\n",
    ").transform_calculate(\n",
    "order=f\"-indexof({altdomain}, datum.Taxon)\"\n",
    ").mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "    x=alt.X('Sample ID:N',sort=patientlist_sorted, axis=alt.Axis(labels=False,ticks=False),title=None),\n",
    "    y=alt.Y('Read Fraction:Q',scale=alt.Scale(\n",
    "        domain=(0,1)),title=None\n",
    "           ),\n",
    "    color=alt.Color('Taxon:N',\n",
    "                    legend=alt.Legend(columns=2,symbolLimit=0,labelLimit=0),\n",
    "                    sort=taxa,\n",
    "                    scale=alt.Scale(domain=altdomain,range=altrange)),\n",
    "    tooltip=['readcount','Read Fraction','Taxon','Sample ID'],\n",
    "    order=alt.Order('order:Q')\n",
    ")\n",
    "\n",
    "c_table = input_table[~input_table['Lifelines']]\n",
    "\n",
    "patientlist_sorted = sorted(\n",
    "    c_table['Sample ID'].unique().tolist(),\n",
    "    key=lambda x : SORTFUNCTION(x)\n",
    ")\n",
    "\n",
    "ccs = alt.Chart(\n",
    "  c_table,title='Control'\n",
    ").transform_calculate(\n",
    "order=f\"-indexof({altdomain}, datum.Taxon)\"\n",
    ").mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "    x=alt.X('Sample ID:N',sort=patientlist_sorted, axis=alt.Axis(labels=False,ticks=False),title=None),\n",
    "    y=alt.Y('Read Fraction:Q',scale=alt.Scale(\n",
    "        domain=(0,1)),title=['Estimated', 'Abundance']\n",
    "           ),\n",
    "    color=alt.Color('Taxon:N',\n",
    "                    legend=alt.Legend(columns=2,symbolLimit=0,labelLimit=0),\n",
    "                    sort=taxa,\n",
    "                    scale=alt.Scale(domain=altdomain,range=altrange)),\n",
    "    tooltip=['readcount','Read Fraction','Taxon','Sample ID'],\n",
    "    order=alt.Order('order:Q')\n",
    ")\n",
    "\n",
    "bac = input_table[input_table['Taxon'].str.startswith(('Phocaeicola','Bacteroides'))]\n",
    "bac['Healthy'] = bac['Sample ID'].str.startswith('G').map(\n",
    "    {True : 'Control',\n",
    "    False : 'Lifelines'}\n",
    ")\n",
    "bac = bac.groupby(['Healthy','Sample ID'])['Read Fraction'].sum().reset_index()\n",
    "bacc = alt.Chart(bac).mark_boxplot().encode(\n",
    "    x=alt.X('Healthy',sort=['Control','Lifelines'],title=None),\n",
    "    y=alt.Y('Read Fraction',title='Estimated Abundance Bacteroides+Phocaeicola')\n",
    ")\n",
    "\n",
    "(ccs|lls|bacc).configure_title(orient='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation to clinical outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCARD_CUTOFF = 20\n",
    "\n",
    "table = get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    samples=set(PAPER_SAMPLES)-set(DUPLICATES),\n",
    "    level='G',\n",
    "    normalize=False,\n",
    "    excluded_taxa_filter=[CHORDATA])\n",
    "\n",
    "table['Sample'] = table['patientid']+'_'+table['time'].astype(str)\n",
    "\n",
    "table = pd.merge(\n",
    "    table,\n",
    "    validation_data[['Sample','Taxon ID','Validation Rate']],\n",
    "    left_on=['Sample','taxonid'],\n",
    "    right_on=['Sample','Taxon ID'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "#Phase 1: Kick out low abundance groups, assign to \"Not enough reads\"\n",
    "table.loc[table['readcount']<DISCARD_CUTOFF,'taxon'] = 'Not enough reads'\n",
    "\n",
    "#Phase 2: Check for rest if validates, if not assign to \"Not validated\"\n",
    "table['Validated'] = (table['Validation Rate'] > 0.2)\n",
    "table.loc[(table['Validated']!=True)&~(table['taxon'] == 'Not enough reads'), 'taxon'] = 'Not validated'\n",
    "#Readjust sum (group multiple \"Not enough reads\" entries together)\n",
    "table = table.groupby(['taxon','time','patientid'],as_index=False).sum()\n",
    "table['Read Fraction (%)'] = table['readcount']*100 / table.groupby(['time','patientid'])['readcount'].transform('sum')\n",
    "table['Sample'] = table['patientid']+'/'+table['time'].astype(str)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXA_OF_INTEREST=[\n",
    "    'Bacteroides',\n",
    "    'Candida',\n",
    "    'Enterococcus',\n",
    "    'Saccharomyces',\n",
    "    'Lactobacillus',\n",
    "    'Methanosarcina',\n",
    "    'Pseudomonas',\n",
    "    'Methanobrevibacter',\n",
    "    'Blautia'\n",
    "]\n",
    "\n",
    "outer_charts = []\n",
    "\n",
    "for TAXON_OF_INTEREST in TAXA_OF_INTEREST:\n",
    "\n",
    "\n",
    "    taxon_table = table[table['taxon'] == TAXON_OF_INTEREST]\n",
    "    for sample in table['Sample'].unique():\n",
    "        #check if sample does not have the taxon\n",
    "        if sample not in taxon_table['Sample'].unique():\n",
    "            #print('Sample {} does not have the taxon, creating a dummy entry ...'.format(sample))\n",
    "            patientid,time = sample.split('/')\n",
    "            time = int(time)\n",
    "            taxon_table = pd.concat([\n",
    "                taxon_table,\n",
    "                pd.DataFrame(\n",
    "                    [(0,patientid,time,TAXON_OF_INTEREST,'???','???',sample,0)],\n",
    "                    columns=['readcount','patientid','time','taxon','taxonid','level','Sample','Read Fraction (%)']\n",
    "                )\n",
    "            ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    taxon_table = pd.merge(\n",
    "        taxon_table,\n",
    "        sample_statistics[['PatID','time','Startcluster','leukocytephase_cluster_2_kurz']],\n",
    "        how='left',\n",
    "        left_on=['patientid','time'],\n",
    "        right_on=['PatID','time']\n",
    "    ).drop(columns=['PatID'])\n",
    "    \n",
    "    taxon_table = taxon_table[\n",
    "        (taxon_table['leukocytephase_cluster_2_kurz'] == 'Leukozytopenia')|\n",
    "        (taxon_table['patientid'].str.startswith('G'))\n",
    "    ]\n",
    "\n",
    "    outcomes['Any GvHD'] = (\n",
    "        (outcomes['aGVHD Grade 1-2']==1) |\n",
    "        (outcomes['aGvHD Grade 3 - 4']==1) | \n",
    "        (outcomes['moderate cGVHD']==1) |\n",
    "        (outcomes['severe cGvHD']==1)\n",
    "    )\n",
    "\n",
    "    outcomes['Relapse'] = (\n",
    "        (outcomes['Replase_1']==1) |\n",
    "        (outcomes['Replase_2']==1)\n",
    "        )\n",
    "\n",
    "    taxon_table = pd.merge(\n",
    "        taxon_table,\n",
    "        outcomes[['Pat ID','Outcomes (non (0), adverse event (1))','Any GvHD','Death', 'Relapse']],\n",
    "        how='left',\n",
    "        left_on=['patientid'],\n",
    "        right_on=['Pat ID']\n",
    "    ).rename(columns={\n",
    "            'Outcomes (non (0), adverse event (1))' : 'Adverse Event',\n",
    "        'Any GvHD' : 'GvHD',\n",
    "        'Death' : 'Death',\n",
    "        'Relapse' : 'Relapse'\n",
    "    })\n",
    "\n",
    "    charts = []\n",
    "    \n",
    "    taxon_table.to_csv('{}_MARKER_GENERA_ANNA.csv'.format(TAXON_OF_INTEREST))\n",
    "\n",
    "    for category in ['Adverse Event','GvHD','Death', 'Relapse']:\n",
    "        \n",
    "        \n",
    "\n",
    "        yes_distrib = taxon_table[(taxon_table[category] == True)]['Read Fraction (%)']\n",
    "        no_distrib = taxon_table[(taxon_table[category] == False)]['Read Fraction (%)']\n",
    "        U1, p = mannwhitneyu(yes_distrib, no_distrib, method=\"exact\")    \n",
    "        '''\n",
    "        print('Comparing {} ({} samples) to no {} ({} samples), p-value: {:.2}'.format(\n",
    "            category,len(yes_distrib),\n",
    "                                                                                    category,\n",
    "                                                                                    len(no_distrib),\n",
    "                                                                                    p))\n",
    "        '''\n",
    "        chart = alt.Chart(taxon_table[~taxon_table['patientid'].str.startswith('G')],width=80, height=400, title='{} (p-Val: {:.2})'.format(\n",
    "            category,p\n",
    "        )).mark_boxplot(ticks=True).encode(\n",
    "            x=alt.X(category+':N',title=None),\n",
    "            y=alt.Y('Read Fraction (%)', axis=alt.Axis(format='.2f')),\n",
    "            tooltip=['Pat ID', 'time', 'Read Fraction (%)'],\n",
    "        )+alt.Chart(taxon_table[taxon_table['patientid'].str.startswith('G')]).mark_rule(color='red').encode(\n",
    "            y='mean(Read Fraction (%))'\n",
    "        )\n",
    "        charts.append(chart)\n",
    "\n",
    "    outer_charts.append(\n",
    "        reduce(lambda x,y : x|y , charts).resolve_scale(y='shared').properties(title=TAXON_OF_INTEREST)\n",
    "    )\n",
    "reduce (lambda x,y : x&y, outer_charts).configure_tick(thickness=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxon_table[taxon_table['Adverse Event']!=taxon_table['Adverse Event']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Calibration with Zymo Std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data[validation_data['Sample'].str.startswith('Zymo')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_validation = validation_data\n",
    "\n",
    "stats_validation['Validated'] = (stats_validation['Validation Rate'].astype(float) >= 0.2)\n",
    "stats_validation['Validated Reads Binary'] = stats_validation['Validated'] * stats_validation['Reads']\n",
    "stats_validation['Validated Reads Continuous'] = stats_validation['Validation Rate'] * stats_validation['Reads']\n",
    "\n",
    "stats_validation = stats_validation.groupby('Sample',as_index=False)[['Validated Reads Binary','Validated Reads Continuous','Reads']].sum()\n",
    "\n",
    "stats_validation['Validated Read Fraction Binary'] = stats_validation['Validated Reads Binary'] / stats_validation['Reads']\n",
    "stats_validation['Validated Read Fraction Continuous'] = stats_validation['Validated Reads Continuous'] / stats_validation['Reads']\n",
    "stats_validation[\n",
    "    stats_validation['Sample'].isin(\n",
    "        x.replace('/','_') for x in PAPER_SAMPLES\n",
    "    )\n",
    "                ].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVEL = 'G'\n",
    "\n",
    "#Filters\n",
    "INCLUDE = None\n",
    "EXCLUDE = [CHORDATA,PLANTAE]\n",
    "\n",
    "SAMPLES = PAPER_SAMPLES\n",
    "\n",
    "\n",
    "def SORTFUNCTION(x):\n",
    "    #return x\n",
    "    return (\n",
    "        x[0] == 'G', #Sort by G or regular patient first\n",
    "        float(x.split('G')[-1].split('/')[0]), #Then Patient ID\n",
    "        int(x.split('G')[-1].split('/')[1]), #Then Time\n",
    "    )\n",
    "\n",
    "GROUPING = 'leukocytephase_cluster_kurz'\n",
    "SORTING = 'Sample ID'\n",
    "#'leukocytephase_cluster_kurz'#'leukocytephase_cluster_kurz' # None if you don't want any Grouping, otherwise a column to group by\n",
    "\n",
    "NORMALIZE = False\n",
    "\n",
    "#################\n",
    "\n",
    "os.makedirs('Output/Composition',exist_ok=True)\n",
    "\n",
    "input_table = get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    level=LEVEL,\n",
    "    samples=SAMPLES,\n",
    "    included_taxa_filter=INCLUDE,\n",
    "    excluded_taxa_filter=EXCLUDE,\n",
    "    normalize=NORMALIZE\n",
    ")\n",
    "\n",
    "input_table['Read Fraction (%)'] = input_table['readcount']*100 / input_table.groupby(['time','patientid'])['readcount'].transform('sum')\n",
    "\n",
    "input_table['Sample'] = input_table['patientid']+'_'+input_table['time'].astype(str)\n",
    "\n",
    "\n",
    "DISCARD_CUTOFF = 20\n",
    "\n",
    "with_validation = pd.merge(\n",
    "    input_table,\n",
    "    validation_data[['Sample','Taxon ID','Validation Rate']],\n",
    "    left_on=['Sample','taxonid'],\n",
    "    right_on=['Sample','Taxon ID'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "total_reads = with_validation.groupby('sample',as_index=False)['readcount'].sum()\n",
    "sample_statistics['sample']= sample_statistics['PatID']+'/'+sample_statistics['time'].astype(str)\n",
    "total_reads = pd.merge(total_reads,sample_statistics[['sample','Classified Reads']],on='sample',how='left')\n",
    "total_reads['Total Fraction (%)'] = total_reads['readcount']/total_reads['Classified Reads']\n",
    "total_reads['Sample ID'] = total_reads['sample']\n",
    "\n",
    "#Phase 1: Kick out low abundance groups, assign to \"Not enough reads\"\n",
    "with_validation.loc[with_validation['readcount']<DISCARD_CUTOFF,'taxon'] = 'Not enough reads'\n",
    "\n",
    "#Phase 2: Check for rest if validates, if not assign to \"Not validated\"\n",
    "with_validation['Validated'] = (with_validation['Validation Rate'] > 0.2)\n",
    "with_validation.loc[(with_validation['Validated']!=True)&~(with_validation['taxon'] == 'Not enough reads'), 'taxonid'] = '-3'\n",
    "#Readjust sum (group multiple \"Not enough reads\" entries together)\n",
    "with_validation = with_validation.groupby(['taxonid','sample'],as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_validation['Ingroup'] = with_validation['taxonid'].apply(lambda x : is_below_or_equal(x,'1224'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_statistics['sample']= sample_statistics['PatID']+'/'+sample_statistics['time'].astype(str)\n",
    "table = pd.merge(\n",
    "    with_validation[with_validation['Ingroup']].groupby(['sample'],as_index=False)['Read Fraction (%)'].sum(),\n",
    "    sample_statistics[['sample','leukocytephase_cluster_kurz']],on='sample',how='left'\n",
    ")\n",
    "\n",
    "print(table.groupby('leukocytephase_cluster_kurz')['Read Fraction (%)'].median())\n",
    "alt.Chart(table).mark_boxplot().encode(\n",
    "    x='leukocytephase_cluster_kurz',\n",
    "    y='Read Fraction (%)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_statistics.groupby('leukocytephase_cluster_2_kurz')['Total Reads'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check combined abundance and validation presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_table = get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    level='G',\n",
    "    samples=PAPER_SAMPLES,\n",
    "    included_taxa_filter=VIRUSES,\n",
    "    excluded_taxa_filter=None,\n",
    "    normalize=False\n",
    ")\n",
    "\n",
    "input_table['Read Fraction'] = input_table['readcount'] / input_table.groupby(['time','patientid'])['readcount'].transform('sum')\n",
    "input_table.loc[input_table['taxon']=='Crassphage Pseudo-Genus', 'taxon'] = 'uncultured crAssphage'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxon = 'uncultured crAssphage'\n",
    "abundance = 0.1\n",
    "\n",
    "subtable = validation_data[\n",
    "    (validation_data['Taxon Name'] == taxon)&\n",
    "    (validation_data['sample'].isin(PAPER_SAMPLES))\n",
    "]\n",
    "\n",
    "subtable = pd.merge(\n",
    "    subtable,\n",
    "    sample_statistics,\n",
    "    on='sample',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "subtable = pd.merge(\n",
    "    subtable,\n",
    "    input_table,\n",
    "    left_on=['sample','Taxon ID'],\n",
    "    right_on=['sample','taxonid'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "subtable = subtable[subtable['leukocytephase_cluster_kurz'].isin(['pre_1','pre_2','pre_3'])]\n",
    "subtable = subtable[subtable['readcount'] >= 20]\n",
    "\n",
    "print(\n",
    "    len(subtable[subtable['Validation Rate'] >= 0.2]),\n",
    "    len(subtable[\n",
    "        (subtable['Validation Rate'] >= 0.2)&\n",
    "        (subtable['Read Fraction'] >= abundance)\n",
    "    ])\n",
    ")\n",
    "subtable[['sample','readcount','Validation Rate','Read Fraction','leukocytephase_cluster_kurz']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saturation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_table = get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    level='G',\n",
    "    samples=PAPER_SAMPLES,\n",
    "    included_taxa_filter=None,\n",
    "    excluded_taxa_filter=None,\n",
    "    normalize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_table['Sample'] = input_table['patientid']+'_'+input_table['time'].astype(str)\n",
    "\n",
    "with_validation = pd.merge(\n",
    "    input_table,\n",
    "    validation_data[['Sample','Taxon ID','Validation Rate']],\n",
    "    left_on=['Sample','taxonid'],\n",
    "    right_on=['Sample','Taxon ID'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "with_validation['Validated'] = (with_validation['Validation Rate'] > 0.2)\n",
    "with_validation.loc[(with_validation['Validated']!=True)&~(with_validation['taxon'] == 'Not enough reads'), 'taxonid'] = '-3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(pd.concat([\n",
    "    with_validation.groupby('sample')['taxon'].count(),\n",
    "    with_validation.groupby('sample')['readcount'].sum()\n",
    "],axis=1)).mark_point().encode(\n",
    "    x=alt.X('readcount',scale=alt.Scale(type='symlog')),\n",
    "    y='taxon'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
