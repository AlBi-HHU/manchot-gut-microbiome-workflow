{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages / imports\n",
    "\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import math\n",
    "import altair as alt\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as colors\n",
    "import glob\n",
    "from skbio.stats import ordination\n",
    "from scipy.stats import mannwhitneyu, wilcoxon\n",
    "from functools import lru_cache,reduce\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "os.makedirs('Output',exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Definitions / Helper Functions](#defines)\n",
    "* [Load Data](#loaddata)\n",
    "* [Normalization Function](#normalize)\n",
    "* Analysis\n",
    "    * [Top Level Statistics](#toplevel)\n",
    "    * [P-Values Top Level Statistics](#pvalues)\n",
    "    * [Overview Top Taxa per Level](#toptaxa)\n",
    "    * [Unmapped and Human Fraction](#human)\n",
    "    * [AMR Gene Counts](#amr)\n",
    "    * [Taxa Counts (Diversity)](#taxacount)\n",
    "    * [Barplots Compositions](#barplots)\n",
    "    * [Bray-Curtis Based PCoA](#pcoa)\n",
    "    * [Sampling Times Overview](#sampletimes)\n",
    "    * [Zymo Std Analysis](#zymo)\n",
    "    * [Eukaryotes Analysis](#eukaryotes)\n",
    "    * [Crassphage Analysis](#crassphage)\n",
    "    * [Population Level Analysis / Illumina](#strains)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions / Helper Functions <a class=\"anchor\" id=\"defines\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePrePostPairs(table : pd.DataFrame, patientcolumn: str = 'patientid' , timecolumn: str ='time' ):\n",
    "    result = []\n",
    "    counts_of_common_occurences = table.groupby([patientcolumn,timecolumn]).size().reset_index()\n",
    "    for patient in counts_of_common_occurences[patientcolumn].unique():\n",
    "        subtable = counts_of_common_occurences[counts_of_common_occurences[patientcolumn] == patient]\n",
    "        for time1,time2 in it.combinations(subtable[timecolumn].unique(),2):\n",
    "            if time1 < 0 and time2 > 0:\n",
    "                result.append((patient,time1,time2))\n",
    "            elif time1 > 0 and time2 < 0:\n",
    "                result.append((patient,time2,time1))\n",
    "    return result\n",
    "\n",
    "\n",
    "#Helper Function that returns a preceding timepoint (if one exists) for each row in a table\n",
    "def find_previous_sample(table,row):\n",
    "    #print (row ['PatID'])\n",
    "    timepoints_patient = list(table[table['PatID']==row['PatID']]['time'].unique())\n",
    "    timepoints_filtered = [timepoint for timepoint in timepoints_patient if timepoint < row['time']]\n",
    "    if len(timepoints_filtered) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return sorted(timepoints_filtered)[-1]\n",
    "\n",
    "#Helper function that recalculates some relative days and relabels some patients\n",
    "def adjust_table(df):\n",
    "\n",
    "    df.loc[(df['patientid'] == '999') & (df['time'] == 0), 'patientid'] = 'G1'\n",
    "    df.loc[df['patientid'] == 'G1', 'time'] = -100\n",
    "    df.loc[df['patientid'] == '132', 'patientid'] = '45'\n",
    "    df.loc[df['patientid'] == '133', 'patientid'] = '14'\n",
    "    df.loc[df['patientid'] == '134', 'patientid'] = '46'\n",
    "\n",
    "def sort_samples(samples):\n",
    "    return sorted(\n",
    "        samples,\n",
    "        key=lambda x : ('G' in x,\n",
    "                        int(x.split('/')[0].replace('G','').split('.')[0]),\n",
    "                        int(x.split('/')[0].replace('G','').split('.')[-1]),\n",
    "                        int(x.split('/')[1]))\n",
    "    )\n",
    "\n",
    "\n",
    "def is_below_or_equal(x,y):\n",
    "    if x in ['-2','-3']:\n",
    "        return False\n",
    "    if x == y:\n",
    "        return True\n",
    "    \n",
    "    parent_node = taxonomy[x]\n",
    "    if parent_node == y: #Parent was the node we looked for\n",
    "        return True\n",
    "    elif parent_node == x: #This is only the case at the root node\n",
    "        return False\n",
    "    else: #We need to keep looking\n",
    "        return is_below_or_equal(parent_node,y)\n",
    "\n",
    "#Constants, frequently used\n",
    "CHORDATA = '7711'\n",
    "FUNGI = '4751'\n",
    "VIRUSES = '10239'\n",
    "BACTERIA = '2'\n",
    "ARCHAEA = '2157'\n",
    "EUKARYOTA = '2759'\n",
    "PROTOZOA = '1891100'\n",
    "FIRMICUTES = '1239'\n",
    "BACTEROIDETES = '976'\n",
    "ACTINOBACTERIA = '201174'\n",
    "PROTEOBACTERIA = '1224'\n",
    "VERRUCOMICROBIA = '74201'\n",
    "BACTEROIDES = '816'\n",
    "PREVOTELLA = '838'\n",
    "ALISTIPES = '239759'\n",
    "PARABACTEROIDES = '375288'\n",
    "PLANTAE = '33090'\n",
    "HOMO='9605'\n",
    "\n",
    "\n",
    "OFFSET_PAT_18 = 161\n",
    "\n",
    "PAPER_SAMPLES = ['G1/-100','G2/-100','G3/-100','G4/-100','G5/-100','G6/-100',\n",
    "             'G7/-100','G8/-100','G9/-100','G10/-100','G11/-100',\n",
    "             '4/-7','7/-6','11/-2','12/-6','14/-7','15/-6','16/-5','17/-7','19/-2',\n",
    "             '21/-9','24/-10','28/-5','29/-5','32/-5','34/-6','38/-6','39/-6','42/-8',\n",
    "             '13/-7','18/-3','22/-4','23/-9','25/-62','37/-5','40/-1','41/-6','20/-4',\n",
    "             '26/-3','31/-6','33/-2','36/-6',\n",
    "             '4/1','11/0','15/6','16/2','17/9','17/13','24/0','24/9','28/1','29/1','34/10','39/1','42/6',\n",
    "             '18/13','22/0','23/12','25/1','37/1','41/14','26/12','31/6','33/7',\n",
    "             '15/16','28/21','29/15','34/18','38/15','39/15','42/18','42/30',\n",
    "             '25/15','37/17','37/22','40/17','26/27',\n",
    "             '12/34','24/49','28/63','34/41',\n",
    "             '18/91','21/50','22/91','23/61','25/31','31/58','33/47','36/38','36/63',\n",
    "             '11/182','12/342','14/558','15/104','15/189','16/163','16/171','17/154',\n",
    "             '19/153','24/127','28/237','29/186','39/115',\n",
    "             '13/298','18.2/-9','18.2/16','18.2/71','21/120','23/130','37/138','26/185','31/178']\n",
    "\n",
    "DUPLICATES = ['15/104',\n",
    " '16/163',\n",
    " '18.2/71',\n",
    " '21/50',\n",
    " '23/61',\n",
    " '29/15',\n",
    " '31/58',\n",
    " '34/10',\n",
    " '34/18',\n",
    " '36/38',\n",
    " '37/17',\n",
    " '42/18']\n",
    "\n",
    "\n",
    "HEALTHY_SAMPLES = ['G1/-100','G2/-100','G3/-100','G4/-100','G5/-100','G6/-100',\n",
    "             'G7/-100','G8/-100','G9/-100','G10/-100','G11/-100']\n",
    "\n",
    "ZYMO_SAMPLES_NEW = ['Zymo_Zymo/-100','Zymo_EZ1/-100','Zymo_Power/-100','Zymo_Pro/-100']+['Stool_Power_11/-100',\n",
    " 'Stool_Power_21/-100',\n",
    " 'Stool_Pro_11/-100',\n",
    " 'Stool_Pro_21/-100',\n",
    " 'Stool_Zymo_11/-100',\n",
    " 'Stool_Zymo_21/-100',\n",
    " 'Stool_EZ1_12/-100',\n",
    " 'Stool_EZ1_21/-100']\n",
    "\n",
    "LIFELINES = ['LL81_E07_7627/-777', 'LL91_A04_8564/-777',\n",
    "       'LL93_H07_8785/-777', 'LL87_A09_8215/-777', 'LL80_A10_7551/-777',\n",
    "       'LL47_D03_4330/-777', 'LL63_B04_5872/-777', 'LL83_F03_7788/-777',\n",
    "       'LL66_C11_6217/-777', 'LL80_D01_7482/-777', 'LL92_D01_8637/-777',\n",
    "       'LL59_F11_5548/-777', 'LL83_G11_7853/-777', 'LL72_C02_6721/-777',\n",
    "       'LL84_D03_7882/-777', 'LL45_F06_4164/-777', 'LL89_E11_8427/-777',\n",
    "       'LL60_B04_5584/-777', 'LL45_F11_4204/-777', 'LL70_E06_6539/-777'\n",
    "    \n",
    "]\n",
    "\n",
    "MARKER_GENERA = [\n",
    "    'Akkermansia',\n",
    "    'Prevotella',\n",
    "    'Citrobacter',\n",
    "    'Enterobacter',\n",
    "    'Faecalibacterium',\n",
    "    'Escherichia',\n",
    "    'Klebsiella',\n",
    "    'Providencia',\n",
    "    'Pseudomonas',\n",
    "    'Serratia',\n",
    "    'Sutterella',\n",
    "    'Enterococcus',\n",
    "    'Clostridium',\n",
    "    'Candida',\n",
    "    'Geotrichum',\n",
    "    'Nakaseomyces',\n",
    "    'Saccharomyces',\n",
    "    'Plasmodium',\n",
    "    'Toxoplasma',\n",
    "    'Blautia',\n",
    "    'Bacteroides',\n",
    "    'Skunavirus',\n",
    "    'uncultured crAssphage'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data <a class=\"anchor\" id=\"loaddata\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kraken2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This data can be collected from the snakemake workflow\n",
    "kraken_dataframe = pd.read_csv('Input/KrakenFullDump.csv',usecols=[1,2,3,4,5,6],dtype={'patientid' : str,'taxonid':str})\n",
    "adjust_table(kraken_dataframe)\n",
    "kraken_dataframe = pd.concat([kraken_dataframe,\n",
    "pd.read_csv('Input/KrakenLifelines.csv',usecols=[0,1,2,3,4,5],dtype={'patientid' : str,'taxonid':str}\n",
    "            )]\n",
    "                            \n",
    "                            )\n",
    "kraken_dataframe['sample'] = kraken_dataframe['patientid']+'/'+kraken_dataframe['time'].astype(str) \n",
    "\n",
    "crassphages_species = kraken_dataframe[kraken_dataframe['taxon'] == 'uncultured crAssphage']\n",
    "crassphages_species['level'] = 'G'\n",
    "crassphages_species['taxon'] = 'Crassphage Pseudo-Genus'\n",
    "\n",
    "kraken_dataframe = pd.concat([kraken_dataframe,crassphages_species])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMR Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can be extracted from the snakemake workflow, used to normalize the AMR read counts\n",
    "sequencing_stats = pd.read_csv('Input/sampleStats.filtered.csv',dtype={'patientid' : str})\n",
    "\n",
    "sequencing_stats['sample'] = sequencing_stats['patientid']+'/'+sequencing_stats['time'].astype(str)\n",
    "\n",
    "adjust_table(sequencing_stats)\n",
    "sequencing_stats = sequencing_stats[sequencing_stats['sample'].isin(PAPER_SAMPLES)]\n",
    "\n",
    "readcount_min = sequencing_stats['nReads'].min()\n",
    "\n",
    "sequencing_stats = sequencing_stats.set_index(['sample'])\n",
    "\n",
    "def calc_fraction(x):\n",
    "    if x['sample'] in sequencing_stats.index:\n",
    "        return x['readcount'] / sequencing_stats.loc[x['sample']]['nReads']\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "#can be extracted from the snakemake workflow\n",
    "amr = pd.read_csv('Input/fulldump_amr_170322.csv',dtype={'Patient' : str}).rename(\n",
    "columns={'Patient' : 'patientid','Time' : 'time'})\n",
    "\n",
    "amr['sample'] = amr['patientid'].astype(str)+'/'+amr['time'].astype(str)\n",
    "amr['readcount'] = amr.apply(\n",
    "    lambda x : 1/x['Ambiguous Assignments'] if x['Ambiguous Assignments'] != 0 else 1,axis=1\n",
    ")\n",
    "adjust_table(amr)\n",
    "amr = amr[amr['sample'].isin(PAPER_SAMPLES)]\n",
    "\n",
    "amr['Gene Pro Read'] =amr.apply(lambda x : calc_fraction(x) ,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kraken2/Metamaps Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idtonames = {}\n",
    "\n",
    "with open('Input/Taxonomy/new_10_10_22/names.dmp','r') as f:\n",
    "    for l in f.read().splitlines():\n",
    "        d=[x.strip() for x in l.split('|')]\n",
    "        if d[3] == 'scientific name':\n",
    "            idtonames[d[0]] = d[1]\n",
    "\n",
    "\n",
    "taxonomy = {}\n",
    "\n",
    "levels = {}\n",
    "\n",
    "with open('Input/Taxonomy/new_10_10_22/nodes.dmp','r') as f:\n",
    "    for l in f.read().splitlines():\n",
    "        d=[x.strip() for x in l.split('|')]\n",
    "        taxonomy[d[0]] = d[1]\n",
    "        levels[d[0]] = d[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zymo Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zymo_theory = pd.read_csv('Input/zymo_theory.csv',\n",
    "                          header=None,\n",
    "                          names=['Taxon','Read Fraction']\n",
    "                         )\n",
    "\n",
    "zymo_theory_taxa = zymo_theory['Taxon'].tolist()\n",
    "zymo_theory['Read Fraction (%)'] = zymo_theory['Read Fraction']/100\n",
    "zymo_theory['Sample ID'] = 'Zymo Theoretical Composition'\n",
    "\n",
    "zymo_minimap = pd.read_csv('Input/zymo_minimap_verification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sample Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PatID is specified as String (Text) so IDs like 18.2 don't get confused as decimal numbers\n",
    "\n",
    "#Tables contain annotations regarding the individual samples\n",
    "sample_statistics = pd.read_excel('Input/Annotations/SampleStatistics.xlsx',dtype={'PatID' : str})\n",
    "\n",
    "#Outcomes\n",
    "outcomes = pd.read_excel('Input/Annotations/PatientStatistics.xlsx',dtype={'Pat ID' : str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv('Input/raw_data_validation.csv',dtype={'Taxon ID':str})\n",
    "\n",
    "validation_data[['patientid','time']] = validation_data['Sample'].str.rsplit('_',1,expand=True)\n",
    "validation_data['time'] = validation_data['time'].astype(int)\n",
    "adjust_table(validation_data)\n",
    "validation_data['Sample'] = validation_data['patientid']+'_'+validation_data['time'].astype(str)\n",
    "validation_data['Taxon Name'] = validation_data['Taxon ID'].map(idtonames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization Function <a class=\"anchor\" id=\"normalize\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    level, #No default value here to avoid accidental mistakes!\n",
    "    samples = None,\n",
    "    random_seed = (4+8+15+16+23+42),\n",
    "    normalize=True,\n",
    "    excluded_taxa_filter = None, #Can be a list of taxa\n",
    "    included_taxa_filter = None, #Only one taxa, becomes new root\n",
    "):\n",
    "    \n",
    "    ### Helper Functions for Filtering\n",
    "    \n",
    "    not_found_taxa = set()\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def is_not_below(x,y):\n",
    "        parent_node = taxonomy[x]\n",
    "        if parent_node == y: #Parent was the node we looked for\n",
    "            return False\n",
    "        elif parent_node == x: #This is only the case at the root node\n",
    "            return True\n",
    "        else: #We need to keep looking\n",
    "            return is_not_below(parent_node,y)\n",
    "\n",
    "\n",
    "    def filter_function(row,taxon):\n",
    "        if row['taxonid'] == '0': #Root Node\n",
    "            return False\n",
    "        if row['taxonid'] not in taxonomy:\n",
    "            #print('Warning: TaxID {} is not in the taxonomy, this is (potentially) bad!'.format(row['taxonid']))\n",
    "            return False\n",
    "        if row['taxonid'] == taxon: #This is the taxon itself, remove\n",
    "            return False\n",
    "        #Otherwise we will check if the taxon is below our target taxon in the taxonomy\n",
    "        return is_not_below(row['taxonid'],taxon)\n",
    "\n",
    "\n",
    "    ###\n",
    "    \n",
    "    #We begin by assuming the raw kraken dataframe as input\n",
    "    working_table = kraken_dataframe\n",
    "\n",
    "    \n",
    "    #####################\n",
    "    #   SELECT SAMPLES\n",
    "    #\n",
    "    #####################    \n",
    "    \n",
    "    #Filter to target samples\n",
    "    if samples != None:\n",
    "        working_table = working_table[working_table['sample'].isin(samples)]    \n",
    "\n",
    "\n",
    "    #####################\n",
    "    #   CALCULATE UNASSIGNED READS\n",
    "    #\n",
    "    #####################    \n",
    "\n",
    "    #Determine root readcounts\n",
    "    root_readcounts_kr = None\n",
    "\n",
    "    if included_taxa_filter != None:\n",
    "        root_readcounts_kr = working_table[\n",
    "            working_table['taxonid']==included_taxa_filter\n",
    "        ].groupby(\n",
    "            ['sample']\n",
    "        )['readcount'].sum()\n",
    "\n",
    "    else: #If we don't filter we take all reads -> R\n",
    "        root_readcounts_kr = working_table[\n",
    "            working_table['level']=='R'\n",
    "        ].groupby(\n",
    "            ['sample']\n",
    "        )['readcount'].sum()\n",
    "\n",
    "\n",
    "    if excluded_taxa_filter != None:\n",
    "            root_readcounts_kr -= working_table[\n",
    "            working_table['taxonid'].isin(excluded_taxa_filter)\n",
    "        ].groupby(\n",
    "            ['sample']\n",
    "        )['readcount'].sum()\n",
    "\n",
    "    readcounts_at_level=None\n",
    "\n",
    "    if included_taxa_filter != None:\n",
    "        include = working_table[\n",
    "            working_table['level'] == level\n",
    "        ].apply(lambda x : filter_function(x,included_taxa_filter),axis=1)\n",
    "\n",
    "        readcounts_at_level = working_table[\n",
    "            working_table['level'] == level\n",
    "        ][~include]\n",
    "\n",
    "        readcounts_at_level = readcounts_at_level.groupby(\n",
    "            ['sample']\n",
    "        )['readcount'].sum()\n",
    "    else:\n",
    "        #Determine total read counts at level\n",
    "        readcounts_at_level = working_table[\n",
    "            working_table['level'] == level\n",
    "        ].groupby(\n",
    "            ['sample']\n",
    "        )['readcount'].sum()\n",
    "\n",
    "\n",
    "    if excluded_taxa_filter != None:\n",
    "        for taxon in excluded_taxa_filter:\n",
    "            include = working_table[\n",
    "                working_table['level'] == level\n",
    "            ].apply(lambda x : filter_function(x,taxon),axis=1)\n",
    "\n",
    "            excluded_taxon_sum = working_table[\n",
    "                working_table['level'] == level\n",
    "            ][~include]\n",
    "\n",
    "\n",
    "            readcounts_at_level -= excluded_taxon_sum.groupby(\n",
    "            ['sample']\n",
    "                )['readcount'].sum()\n",
    "\n",
    "    #Add unassigned at level\n",
    "    unassigned_entries = []\n",
    "    for sample in readcounts_at_level.keys():\n",
    "        \n",
    "        patientid,time = sample.split('/')\n",
    "        time = int(time)\n",
    "\n",
    "        difference = root_readcounts_kr[sample]-readcounts_at_level[sample]\n",
    "        \n",
    "        addon_table = pd.DataFrame([\n",
    "            (difference,patientid,time,'Unassigned at Level','-2',level,sample)\n",
    "        ],columns=[\n",
    "            'readcount','patientid','time','taxon','taxonid','level','sample'\n",
    "        ])\n",
    "        \n",
    "        unassigned_entries.append(addon_table)\n",
    "                \n",
    "    #Combine into one table\n",
    "    if len(unassigned_entries) != 0:\n",
    "        unassigned_table = pd.concat(unassigned_entries)\n",
    "        working_table = pd.concat([unassigned_table,working_table])\n",
    "    \n",
    "    #####################\n",
    "    #   DOWNSAMPLING\n",
    "    #\n",
    "    #####################\n",
    "    \n",
    "    #Filter to target level\n",
    "    downsampled_table = working_table[working_table['level'] == level]  \n",
    "    \n",
    "    #Filter for taxon if required\n",
    "    if included_taxa_filter != None:\n",
    "        print('Reducing composition to subtree below taxon: {}'.format(idtonames[included_taxa_filter]))\n",
    "        include = downsampled_table.apply(lambda x : filter_function(x,included_taxa_filter),axis=1)\n",
    "        downsampled_table = downsampled_table[~include]\n",
    "           \n",
    "    if normalize:\n",
    "    \n",
    "        # Identify lowest read count\n",
    "        readAnzahlen = downsampled_table.groupby('sample')['readcount'].sum()\n",
    "        minimaleReadAnzahl = readAnzahlen.min()\n",
    "        print('The minimal read count across all samples is [Taxonomic Level {}]: {}'.format(level,minimaleReadAnzahl))\n",
    "\n",
    "        frames = []\n",
    "\n",
    "        # Draw new counts for each sample\n",
    "        for sample in downsampled_table['sample'].unique():\n",
    "\n",
    "            sample_table = downsampled_table[downsampled_table['sample'] == sample]\n",
    "            sample = sample_table.sample(\n",
    "                n=round(minimaleReadAnzahl),\n",
    "                random_state=random_seed,\n",
    "                weights='readcount',\n",
    "                replace=True\n",
    "            )\n",
    "\n",
    "            sample = sample.groupby([\n",
    "                'taxon',\n",
    "                'taxonid',\n",
    "                'sample',\n",
    "                'patientid',\n",
    "                'time',\n",
    "                'level'\n",
    "            ],as_index=False).count()\n",
    "\n",
    "            frames.append(sample)\n",
    "\n",
    "        #Overwrite table with downsampled entries\n",
    "        downsampled_table = pd.concat(frames)   \n",
    "    \n",
    "    #####################\n",
    "    #   FILTERING II\n",
    "    #\n",
    "    #####################\n",
    "    \n",
    "    unassigned_downsampled_table = downsampled_table[downsampled_table['taxonid'] == '-2']\n",
    "    assigned_downsampled_table = downsampled_table[downsampled_table['taxonid'] != '-2']\n",
    "    \n",
    "    \n",
    "\n",
    "    if excluded_taxa_filter != None:\n",
    "        for taxon in excluded_taxa_filter:\n",
    "            include = assigned_downsampled_table.apply(lambda x : filter_function(x,taxon),axis=1)\n",
    "            assigned_downsampled_table = assigned_downsampled_table[include]\n",
    "   \n",
    "\n",
    "    tables=[unassigned_downsampled_table,assigned_downsampled_table]\n",
    "    \n",
    "    #Create dummy entries for patients that have nothing\n",
    "\n",
    "\n",
    "    if samples != None:\n",
    "        dummy_entries = []\n",
    "        for sample_id in samples:\n",
    "            if sample_id not in assigned_downsampled_table['sample'].unique():\n",
    "                patientid,time = sample_id.split('/')\n",
    "                time = int(time)\n",
    "                print('Creating a dummy entry for sample {} (No reads with the selected parameters)'.format(sample_id))\n",
    "                addon_table = pd.DataFrame([\n",
    "                            (0,patientid,time,'Absolutely Nothing','Nothing','N',sample_id)\n",
    "                        ],columns=[\n",
    "                            'readcount','patientid','time','taxon','taxonid','level','sample'\n",
    "                        ]) \n",
    "                dummy_entries.append(addon_table)\n",
    "                tables+=dummy_entries\n",
    "\n",
    "    downsampled_table = pd.concat(tables)\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "    return downsampled_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Level Statistics & Visualization <a class=\"anchor\" id=\"toplevel\"></a>\n",
    "\n",
    "Here we generate top-level plots for the different timepoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats kraken\n",
    "classified = kraken_dataframe[kraken_dataframe['level'].isin(['R','U'])].groupby(['patientid','time','taxon','level'],as_index=False).sum().pivot(\n",
    "    index=['patientid','time'],columns=['level'],values=['readcount']\n",
    ")\n",
    "\n",
    "classified = classified.rename(columns={'R' : 'Classified','U' : 'Unclassified'})\n",
    "\n",
    "classified.columns = classified.columns.get_level_values(1)\n",
    "\n",
    "total = kraken_dataframe[kraken_dataframe['level'].isin(['R','U'])].groupby(['patientid','time'],as_index=False).sum()\n",
    "total = total.rename(columns={'readcount' : 'Total'})\n",
    "\n",
    "combined = pd.merge(total,classified,how='left',on=['patientid','time'])\n",
    "\n",
    "for taxon in [CHORDATA,BACTERIA,FUNGI,VIRUSES,EUKARYOTA,ARCHAEA,PLANTAE]:\n",
    "    taxonOnly = kraken_dataframe[kraken_dataframe['taxonid'] == taxon].groupby(['patientid','time'],as_index=False).sum()\n",
    "    taxonOnly = taxonOnly.rename(columns={'readcount' : idtonames[taxon]})\n",
    "    combined = pd.merge(combined,taxonOnly,how='left',on=['patientid','time'])\n",
    "    \n",
    "combined = combined.set_index(['patientid','time'])\n",
    "\n",
    "for column in combined.columns:\n",
    "    print(column)\n",
    "    combined[column+'_Prozent'] = (combined[column]/combined['Total'])*100\n",
    "\n",
    "    \n",
    "combined.to_csv('stats_kraken.csv')\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_validated = pd.read_csv('Input/Outliers_With_Validation.csv',dtype={'PatID':str})\n",
    "\n",
    "domain_like_groups=['Unclassified','Human','Plants','Microbiome']\n",
    "x = sample_statistics[\n",
    "    ['timephase',\n",
    "     'Unclassified_%','Human_%','Plants_%','Microbiome_%',\n",
    "     'PatID','time']\n",
    "]\n",
    "\n",
    "x = x.melt(id_vars=['PatID','time','timephase'])\n",
    "\n",
    "x[['domain','percent_sign']] = x['variable'].str.split('_',expand=True)\n",
    "x = x.drop(columns=['percent_sign'])\n",
    "x = x.pivot(index=['PatID','time','timephase'],columns='domain',values='value')\n",
    "x = x.reset_index()\n",
    "x = x.melt(id_vars=['PatID','time','timephase'])\n",
    "x = pd.merge(x,outlier_validated[['PatID','time','validated','domain']],on=['PatID','time','domain'],how='left').fillna('Not an outlier')\n",
    "\n",
    "c1 = alt.Chart(sample_statistics,width=100,height=100).mark_boxplot(\n",
    "    ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'timephase:O',\n",
    "        header=alt.Header(labels=False),\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None\n",
    "    ),\n",
    "    y=alt.Y('Microgram DNA per g Stool', axis=alt.Axis(grid=False,minExtent=40), title='DNA [µg] Per g Stool'),\n",
    "\n",
    "    )\n",
    "\n",
    "c2 = alt.Chart(sample_statistics,width=100,height=100).mark_boxplot(\n",
    "    ticks=True,median={'color':'black'}\n",
    ").encode(\n",
    "    column=alt.Column(\n",
    "        'timephase:O',\n",
    "        header=alt.Header(labels=False),\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None\n",
    "    ),\n",
    "     y=alt.Y('Total Reads per DNA Library:Q',scale=alt.Scale(type='log'),\n",
    "             axis=alt.Axis(grid=False,tickCount=10, format='.1e',minExtent=40),title='Total Reads')\n",
    "    )\n",
    "\n",
    "c2_2 = alt.Chart(sample_statistics,width=100,height=100).mark_boxplot(\n",
    "    ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'timephase:O',\n",
    "        header=alt.Header(labels=False),\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None\n",
    "    ),\n",
    "     y=alt.Y('Median Read Length:Q',scale=alt.Scale(type='log'),\n",
    "             axis=alt.Axis(grid=False,tickCount=10, format='.1e',minExtent=40),title='Median Readlength')\n",
    "    )\n",
    "\n",
    "c3 = alt.Chart(x,width=100).mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'timephase:O',\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None,\n",
    "        header=alt.Header(labelOrient='bottom', labelPadding=313)\n",
    "    ),\n",
    "     x=alt.X(\"domain:O\", title=None, axis=alt.Axis(labels=False, ticks=False), \n",
    "             scale=alt.Scale(paddingInner=1), sort=domain_like_groups),    \n",
    "    y=alt.Y(\"value:Q\",title='Fraction [%]',\n",
    "            scale=alt.Scale(type='symlog',domain=[0,100]),\n",
    "            axis=alt.Axis(grid=False,minExtent=40,values=[0.1,0.5,1,5,10,20,50,100])), \n",
    "    color=alt.condition(\n",
    "        (alt.datum['validated'] != 'False'),\n",
    "        alt.Color(\"domain:N\",sort=domain_like_groups,legend=alt.Legend(title=None,orient='top'),\n",
    "                    scale=alt.Scale(domain=domain_like_groups,\n",
    "                                    range=['#dd1c77', '#fecc5c', '#006837', '#253494'])),\n",
    "                alt.value('lightgrey')\n",
    "    )\n",
    "    ,tooltip=['PatID','time', 'value']\n",
    ")\n",
    "\n",
    "\n",
    "chart1=(c1&c2&c2_2&c3).configure_tick(thickness=2)\n",
    "\n",
    "chart1.save('Output/Composition/High_Level_Metrics.html')\n",
    "\n",
    "chart1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_like_groups=['Bacteria','Fungi','OtherEukaryota','Viruses','Archaea']\n",
    "z = sample_statistics[\n",
    "    ['timephase',\n",
    "     'Bacteria_%','Fungi_%','OtherEukaryota_%','Viruses_%','Archaea_%',\n",
    "     'PatID','time']\n",
    "]\n",
    "z = z.melt(id_vars=['PatID','time','timephase'])\n",
    "z[['domain','variable']] = z['variable'].str.split('_',expand=True)\n",
    "z = z.drop(columns=['variable'])\n",
    "z = z.pivot(index=['PatID','time','timephase'],columns='domain',values='value')\n",
    "z = z.reset_index()\n",
    "z = z.melt(id_vars=['PatID','time','timephase'])\n",
    "z = pd.merge(\n",
    "    z,outlier_validated[['PatID','time','validated','domain']],\n",
    "             on=['PatID','time','domain'],how='left')\n",
    "\n",
    "\n",
    "\n",
    "c5 = alt.Chart(sample_statistics,width=100,height=100).mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'timephase:O',\n",
    "        header=alt.Header(labels=False),\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None\n",
    "    ),\n",
    "    y=alt.Y('Detected Genera', axis=alt.Axis(grid=False,minExtent=40), title='No. Genera')\n",
    "    )\n",
    "\n",
    "c6 =alt.Chart(sample_statistics,width=100,height=100).mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'timephase:O',\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None,\n",
    "        header=alt.Header(labels=False)\n",
    "    ),\n",
    "    y=alt.Y('ARG Reads per 10000 Reads:Q', \n",
    "            title=['ARG-carrying Reads','Per 10,000 Reads'],\n",
    "            scale=alt.Scale(type='symlog'), axis=alt.Axis(grid=False,minExtent=40))\n",
    "    )\n",
    "\n",
    "\n",
    "c7 = alt.Chart(z,width=100).mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    column=alt.Column(\n",
    "        'timephase:O',\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None,\n",
    "        header=alt.Header(labelOrient='bottom', labelPadding=313)\n",
    "    ),\n",
    "    x=alt.X(\"domain:O\", title=None, \n",
    "            axis=alt.Axis(labels=False, ticks=False),\n",
    "            scale=alt.Scale(paddingInner=1), sort=domain_like_groups),    \n",
    "    y=alt.Y(\"value:Q\",title='Fraction [%]',scale=alt.Scale(type='symlog',domain=[0,100]),\n",
    "            axis=alt.Axis(grid=False,minExtent=40, values=[0.1,0.5,1,5,10,20,50,100])), \n",
    "    color=alt.condition(\n",
    "        (alt.datum['validated'] != 'False'),\n",
    "        alt.Color(\n",
    "            \"domain:N\",\n",
    "            sort=domain_like_groups,\n",
    "            legend=alt.Legend(title=None,orient='top'),\n",
    "            scale=alt.Scale(domain=domain_like_groups,range=['#EE6677','#fecc5c','#228833', '#66CCEE', '#AA3377'])\n",
    "        ),\n",
    "        alt.value('lightgrey')\n",
    "    )\n",
    ")\n",
    "\n",
    "chart2=(c5&c6&c7).configure_tick(thickness=2)\n",
    "\n",
    "chart2.save(\n",
    "    'Output/Composition/High_Level_Metrics_2.html')\n",
    "\n",
    "chart2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.groupby(['domain','timephase'])['value'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Level Statistics P-Values <a class=\"anchor\" id=\"pvalues\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare each group with respect to a specific property and calculate a mannwhitney-u test to determine if one distribution is \"larger\" than the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_column = 'Microgram DNA per g Stool'\n",
    "group_column = 'Startcluster'\n",
    "\n",
    "tuples_a = []\n",
    "for group1,group2 in it.combinations(sample_statistics[group_column].unique(),2):\n",
    "    result = mannwhitneyu(\n",
    "        sample_statistics[sample_statistics[group_column] == group1][test_column],\n",
    "        sample_statistics[sample_statistics[group_column] == group2][test_column]  \n",
    "    )\n",
    "    tuples_a.append((group1,group2,result.pvalue))\n",
    "    \n",
    "df_a = pd.DataFrame(tuples_a,columns=['Group A','Group B','Whitney'])\n",
    "df_a.to_csv('Output/Whitney_{}_{}.csv'.format(test_column,group_column))\n",
    "df_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the column which was tested we will also track Mean/Min/Max + Std. Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples_b = []\n",
    "for group in sample_statistics[group_column].unique():\n",
    "    tuples_b.append((\n",
    "        group,\n",
    "        sample_statistics[sample_statistics[group_column] == group][test_column].mean(),\n",
    "        sample_statistics[sample_statistics[group_column] == group][test_column].std(),\n",
    "        sample_statistics[sample_statistics[group_column] == group][test_column].min(),\n",
    "        sample_statistics[sample_statistics[group_column] == group][test_column].max()\n",
    "        \n",
    "    ))\n",
    "df_b = pd.DataFrame(tuples_b,columns=['Group','Mean','Std. Deviation','Minimum','Maximum'])\n",
    "df_b.to_csv('Output/MeanAndStd_{}_{}.csv'.format(test_column,group_column))\n",
    "df_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxa Counts (Diversity) Per Level <a class=\"anchor\" id=\"taxacount\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES = PAPER_SAMPLES\n",
    "EXCLUDE = [CHORDATA, PLANTAE]\n",
    "############################\n",
    "\n",
    "os.makedirs('Output/Diversity',exist_ok=True)\n",
    "\n",
    "charts = []\n",
    "\n",
    "for taxonomic_level in ['S','G','P']:\n",
    "    table = get_normalized_abundances(\n",
    "        kraken_dataframe,\n",
    "        level=taxonomic_level,\n",
    "        excluded_taxa_filter=EXCLUDE,\n",
    "        samples=SAMPLES,\n",
    "        normalize=True #Important: To get comparable numbers we use normalization here!\n",
    "    )\n",
    "    \n",
    "    table = table[table['taxonid']!='-2']\n",
    "     \n",
    "    taxaCounts = table.groupby(['patientid','time'],as_index=False)['taxon'].count().rename(\n",
    "        columns={'taxon' : '{}_{}'.format(taxonomic_level,'Normalized')}\n",
    "    )\n",
    "    \n",
    "    charts.append(taxaCounts)\n",
    "\n",
    "\n",
    "reduce(lambda x,y : pd.merge(x,y,how='left',on=['patientid','time']),charts).to_csv(\n",
    "    'Output/Diversity/taxa_counts_{}-Without_{}_Normalized.csv'.format(\n",
    "        hash(str(SAMPLES)),\n",
    "        str(EXCLUDE)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "charts = []\n",
    "\n",
    "\n",
    "for taxonomic_level in ['S','G','P']:\n",
    "    table = get_normalized_abundances(\n",
    "        kraken_dataframe,\n",
    "        level=taxonomic_level,excluded_taxa_filter=EXCLUDE,normalize=False, samples=SAMPLES)\n",
    "    \n",
    "    table = table[table['taxonid']!='-2']\n",
    "    \n",
    "    taxaCounts = table.groupby(['patientid','time'],as_index=False)['taxon'].count().rename(\n",
    "        columns={'taxon' : '{}_{}'.format(taxonomic_level,'Raw')}\n",
    "    )\n",
    "    \n",
    "    charts.append(taxaCounts)\n",
    "        \n",
    "combined_chart = reduce(lambda x,y : pd.merge(x,y,how='left',on=['patientid','time']),charts)\n",
    "\n",
    "combined_chart.to_csv(\n",
    "    'Output/Diversity/taxa_counts_{}-Without_{}_RAW.csv'.format(\n",
    "        hash(str(SAMPLES)),\n",
    "        str(EXCLUDE)\n",
    "    )\n",
    ")\n",
    "combined_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize the precalculated diversity values (calculated above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table= sample_statistics[sample_statistics['timephase']=='Pre TX']\n",
    "print(table.groupby('Startcluster')['Detected Genera'].median())\n",
    "alt.Chart(table).mark_boxplot().encode(\n",
    "    x='Startcluster',\n",
    "    y='Detected Genera'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barplots Compositions <a class=\"anchor\" id=\"barplots\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_X = 20\n",
    "LEVEL = 'G'\n",
    "DISCARD_CUTOFF = 20\n",
    "\n",
    "#Filters\n",
    "INCLUDE = None\n",
    "EXCLUDE = [CHORDATA,PLANTAE]\n",
    "\n",
    "SAMPLES = None\n",
    "\n",
    "OPTIONAL_ANNOTATIONS = None #['ARG Reads pro Eingabe Reads *10000']\n",
    "\n",
    "def SORTFUNCTION(x):\n",
    "    try:\n",
    "        return (\n",
    "            x[0] == 'G', #Sort by G or regular patient first\n",
    "            float(x.split('G')[-1].split('/')[0]), #Then Patient ID\n",
    "            int(x.split('G')[-1].split('/')[1]), #Then Time\n",
    "        )\n",
    "    except:\n",
    "        return (True,1,hash(x))\n",
    "    \n",
    "GROUPING = 'timephase_and_cluster'\n",
    "SORTING = 'Sample ID'\n",
    "\n",
    "NORMALIZE = False\n",
    "\n",
    "#################\n",
    "\n",
    "os.makedirs('Output/Composition',exist_ok=True)\n",
    "\n",
    "input_table = get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    level=LEVEL,\n",
    "    samples=SAMPLES,\n",
    "    included_taxa_filter=INCLUDE,\n",
    "    excluded_taxa_filter=EXCLUDE,\n",
    "    normalize=NORMALIZE\n",
    ")\n",
    "\n",
    "input_table['Read Fraction'] = input_table['readcount'] / input_table.groupby(['time','patientid'])['readcount'].transform('sum')\n",
    "\n",
    "input_table['Sample'] = input_table['patientid']+'_'+input_table['time'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "with_validation = pd.merge(\n",
    "    input_table,\n",
    "    validation_data[['Sample','Taxon ID','Validation Rate']],\n",
    "    left_on=['Sample','taxonid'],\n",
    "    right_on=['Sample','Taxon ID'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "total_reads = with_validation.groupby('sample',as_index=False)['readcount'].sum()\n",
    "sample_statistics['sample']= sample_statistics['PatID']+'/'+sample_statistics['time'].astype(str)\n",
    "total_reads = pd.merge(total_reads,sample_statistics[['sample','Classified Reads']],on='sample',how='left').fillna(0)\n",
    "total_reads['Total Fraction'] = total_reads['readcount']/total_reads['Classified Reads']\n",
    "total_reads['Sample ID'] = total_reads['sample']\n",
    "\n",
    "#Phase 1: Kick out low abundance groups, assign to \"Not enough reads\"\n",
    "with_validation.loc[with_validation['readcount']<DISCARD_CUTOFF,'taxon'] = 'Not enough reads'\n",
    "\n",
    "#Phase 2: Check for rest if validates, if not assign to \"Not validated\"\n",
    "with_validation['Validated'] = (with_validation['Validation Rate'] > 0.2)\n",
    "with_validation.loc[(with_validation['Validated']!=True)&~(with_validation['taxon'] == 'Not enough reads'), 'taxon'] = 'Not validated'\n",
    "#Readjust sum (group multiple \"Not enough reads\" entries together)\n",
    "with_validation = with_validation.groupby(['taxon','time','patientid'],as_index=False).sum()\n",
    "\n",
    "# determine the top taxa based on means\n",
    "taxa_we_look_at = list(with_validation.groupby('taxon')['Read Fraction'].sum().sort_values(ascending=False)[:(TOP_X+1)].keys())\n",
    "if 'Not enough reads' not in taxa_we_look_at:\n",
    "    taxa_we_look_at.append('Not enough reads')\n",
    "if 'Not validated' not in taxa_we_look_at:\n",
    "    taxa_we_look_at.append('Not validated')\n",
    "print('Determined the following taxa as relevant:',taxa_we_look_at)\n",
    "\n",
    "#assign everything else to the \"other\" group and readjust sum\n",
    "with_validation.loc[~with_validation['taxon'].isin(taxa_we_look_at), 'taxon'] = 'Other'\n",
    "with_validation = with_validation.groupby(['taxon','time','patientid'],as_index=False).sum()\n",
    "\n",
    "with_validation['id'] = (\n",
    "    with_validation['patientid'].astype(str)+'/'+with_validation['time'].astype(str)\n",
    ")\n",
    "\n",
    "\n",
    "with_validation['other'] = with_validation['taxon'] == 'Other'\n",
    "\n",
    "with_validation = with_validation.rename(columns={\n",
    "    'id' : 'Sample ID',\n",
    "    'taxon' : 'Taxon'\n",
    "})\n",
    "\n",
    "colorMap = {}\n",
    "\n",
    "taxa = taxa_we_look_at+['Other']\n",
    "\n",
    "palette = sns.color_palette(\"tab20\",n_colors=len(taxa)-3)\n",
    "bright = palette[::2]\n",
    "muted = palette[1::2]\n",
    "palette = bright+muted\n",
    "\n",
    "taxa_we_look_at_assigned = taxa_we_look_at\n",
    "\n",
    "taxa_we_look_at_assigned.remove('Not validated')\n",
    "taxa_we_look_at_assigned.remove('Not enough reads')\n",
    "\n",
    "for tax,col in zip(taxa_we_look_at_assigned,palette):\n",
    "    colorMap[tax] = col #colors.to_hex(col)\n",
    "    \n",
    "altdomain = []\n",
    "altrange = []\n",
    "\n",
    "for x in taxa_we_look_at_assigned:\n",
    "\n",
    "    c = colorMap[x]\n",
    "    altdomain.append(x)\n",
    "    color = colors.to_hex(c)\n",
    "    altrange.append(color)\n",
    "    \n",
    "altdomain.append('Other')\n",
    "altrange.append(colors.to_hex((1,1,1)))\n",
    "altdomain.append('Not enough reads')\n",
    "altrange.append(colors.to_hex((0.15,0.15,0.15)))\n",
    "altdomain.append('Not validated')\n",
    "altrange.append(colors.to_hex((0.55,0.55,0.55)))\n",
    "\n",
    "with_validation = pd.merge(with_validation,sample_statistics,how='left',left_on=['patientid','time'],right_on=['PatID','time'])\n",
    "with_validation[GROUPING]=with_validation[GROUPING].fillna('Unknown Group')\n",
    "maxfraction = total_reads['Total Fraction'].max()\n",
    "\n",
    "charts = []\n",
    "\n",
    "\n",
    "\n",
    "groups = with_validation[GROUPING].unique() if GROUPING != None else [None]\n",
    "\n",
    "for group in groups:\n",
    "    \n",
    "    grouptable = None\n",
    "    if group != None:\n",
    "        #Reduce to required columns to keep output reasonably small\n",
    "        grouptable = with_validation[with_validation[GROUPING] == group]\n",
    "    else:\n",
    "        grouptable = with_validation\n",
    "    \n",
    "    patientlist_sorted = sorted(\n",
    "        grouptable[SORTING].unique().tolist(),\n",
    "        key=lambda x : SORTFUNCTION(x)\n",
    "    )\n",
    "    \n",
    "    total_reads_sub = total_reads[total_reads['Sample ID'].isin(grouptable['Sample ID'].unique())]\n",
    "    \n",
    "    chart =  (\n",
    "        alt.Chart(total_reads_sub,height=100).mark_bar().encode(x=alt.X('{}:N'.format(SORTING),sort=patientlist_sorted),y=alt.Y('Total Fraction',scale=alt.Scale(domain=[0,maxfraction]))) +\n",
    "        alt.Chart(total_reads_sub,height=100).mark_point(shape='diamond',color='black').encode(alt.X('{}:N'.format(SORTING),sort=patientlist_sorted),y=alt.Y('readcount',scale=alt.Scale(type='linear')))\n",
    "    ).resolve_scale(y='independent')& alt.Chart(\n",
    "          grouptable,title=str(group)\n",
    "      ).transform_calculate(\n",
    "      order=f\"-indexof({altdomain}, datum.Taxon)\"\n",
    "        ).mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "            x=alt.X('{}:N'.format(SORTING),sort=patientlist_sorted),\n",
    "            y=alt.Y('Read Fraction:Q',scale=alt.Scale(\n",
    "                domain=(0,1))\n",
    "                   ),\n",
    "            color=alt.Color('Taxon:N',\n",
    "                            legend=alt.Legend(columns=1,symbolLimit=0,labelLimit=0),\n",
    "                            sort=taxa,\n",
    "                            scale=alt.Scale(domain=altdomain,range=altrange)),\n",
    "            tooltip=['readcount','Read Fraction','Taxon'],\n",
    "            order=alt.Order('order:Q')\n",
    "      )\n",
    "    \n",
    "    if OPTIONAL_ANNOTATIONS != None:\n",
    "        for optional_annotation in OPTIONAL_ANNOTATIONS:\n",
    "            chart &= alt.Chart(grouptable,title=str(group)).mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "                x=alt.X('{}:N'.format(SORTING),sort=patientlist_sorted),\n",
    "                y=alt.Y('{}'.format(optional_annotation))\n",
    "            )\n",
    "\n",
    "    charts.append(\n",
    "        chart\n",
    "         \n",
    "        \n",
    "    )\n",
    "\n",
    "chart = reduce(lambda x,y : x&y, charts).configure_axis(\n",
    "    labelFontSize=16, titleFontSize=16\n",
    ").configure_title(fontSize=20).configure_legend(titleFontSize=20, labelFontSize=16)\n",
    "\n",
    "chart.save(\n",
    "    'Output/Composition/Barplots-Top_{}-{}-Only_{}-Without_{}-{}_{}_{}_{}.html'.format(\n",
    "        TOP_X,\n",
    "        LEVEL,\n",
    "        INCLUDE,\n",
    "        str(EXCLUDE),\n",
    "        hash(str(SAMPLES)),\n",
    "        hash(str(OPTIONAL_ANNOTATIONS)),\n",
    "        SORTING,\n",
    "        GROUPING\n",
    "    )\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SORTFUNCTION_TABLE(x):\n",
    "    try:\n",
    "        return (\n",
    "            x[0][0] == 'G', #Sort by G or regular patient first\n",
    "            float(x[0].split('G')[-1]), #Then Patient ID\n",
    "            int(x[1]), #Then Time\n",
    "        )\n",
    "    except:\n",
    "        return (hash(x),True,True)\n",
    "\n",
    "with_validation_pivot = with_validation.pivot(index=['patientid','time'],columns='Taxon',values='Read Fraction')\n",
    "\n",
    "with_validation_pivot.transpose()\n",
    "\n",
    "with_validation_pivot=with_validation_pivot.fillna(0).transpose()\n",
    "\n",
    "with_validation_pivot = with_validation_pivot[sorted(with_validation_pivot.columns,key=SORTFUNCTION_TABLE)]\n",
    "with_validation_pivot\n",
    "\n",
    "\n",
    "with_validation_pivot.to_csv(\n",
    "    'Output/Composition/Compositions-Top_{}-{}-Only_{}-Without_{}-{}-{}_{}_{}.csv'.format(\n",
    "        TOP_X,\n",
    "        LEVEL,\n",
    "        INCLUDE,\n",
    "        str(EXCLUDE),\n",
    "        hash(str(SAMPLES)),\n",
    "        #'Relative' if RELATIVE else 'Absolute',\n",
    "        hash(str(OPTIONAL_ANNOTATIONS)),\n",
    "        SORTING,\n",
    "        GROUPING\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional analysis: We check for Pre-TX samples in cluster 2 the highest abundance per taxon and subtract the highest abundance per taxon for the other patient clusters to identify unique features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c13 = with_validation[with_validation['timephase_and_cluster'].isin(['pre_1','pre_3'])]\n",
    "c2 = with_validation[with_validation['timephase_and_cluster'].isin(['pre_2'])]\n",
    "c2.groupby('Taxon')['Read Fraction'].max()-c13.groupby('Taxon')['Read Fraction'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We track for all taxa that have an abundance over 1/4 in how many samples they occur in cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2[c2['Read Fraction'] > 0.25].groupby('Taxon')['sample'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionaly the maximum read fractions are displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2.groupby('Taxon')['Read Fraction'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leukopenia-Loss\n",
    "We analyze here how much of each taxon is lost comparing leukozytopenia -> pre tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(with_validation[\n",
    "    with_validation['timephase']=='Leukozytopenia'\n",
    "].groupby('Taxon')['Read Fraction'].sum()/len(with_validation[\n",
    "    with_validation['timephase']=='Leukozytopenia'\n",
    "]['sample'].unique())-with_validation[\n",
    "    with_validation['timephase']=='Pre TX'\n",
    "].groupby('Taxon')['Read Fraction'].sum()/len(with_validation[\n",
    "    with_validation['timephase']=='Pre TX'\n",
    "]['sample'].unique())).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in it.permutations(with_validation['timephase'].unique(),2):\n",
    "    ca = with_validation[with_validation['timephase']==a]\n",
    "    cb = with_validation[with_validation['timephase']==b]\n",
    "    diffs = (ca.groupby('Taxon')['Read Fraction'].max()-cb.groupby('Taxon')['Read Fraction'].max()).sort_values(ascending=True)\n",
    "    combined = pd.concat([\n",
    "        ca[ca['Read Fraction'] > 0.25].groupby('Taxon')['sample'].count().rename('Above 25% {}'.format(a)),\n",
    "        ca.groupby('Taxon')['sample'].count().rename('Any% {}'.format(a))\n",
    "  \n",
    "    ],axis=1).fillna(0).reset_index()\n",
    "    combined['Above 25% {}'.format(a)] = combined['Above 25% {}'.format(a)].astype(int)\n",
    "    combined = pd.merge(diffs,combined,how='outer',on='Taxon')\n",
    "    combined = combined.rename(columns={'Read Fraction':'Diff Max {} Minus {}'.format(a,b)})\n",
    "    combined.to_excel('{}_Minus_{}.xlsx'.format(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occurence of Genera across Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_X = 20\n",
    "LEVEL = 'G'\n",
    "\n",
    "DISCARD_CUTOFF = 20\n",
    "PRESENCE_CUTOFF = 0.05\n",
    "\n",
    "#Filters\n",
    "GROUPS = [\n",
    "    (BACTERIA,[]),\n",
    "    (FUNGI,[]),\n",
    "    (EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]),\n",
    "    (VIRUSES,[]),\n",
    "    (ARCHAEA,[]),\n",
    "    (PLANTAE,[]),\n",
    "    (CHORDATA,[]),\n",
    "    ('1',[CHORDATA,PLANTAE])\n",
    "]\n",
    "\n",
    "SAMPLES = PAPER_SAMPLES\n",
    "\n",
    "def SORTFUNCTION(x):\n",
    "    #return x\n",
    "    return (\n",
    "        x[0] == 'G', #Sort by G or regular patient first\n",
    "        float(x.split('G')[-1].split('/')[0]), #Then Patient ID\n",
    "        int(x.split('G')[-1].split('/')[1]), #Then Time\n",
    "    )\n",
    "\n",
    "NORMALIZE = False\n",
    "\n",
    "#################\n",
    "\n",
    "for INCLUDE,EXCLUDE in GROUPS:\n",
    "\n",
    "    input_table = get_normalized_abundances(\n",
    "        kraken_dataframe,\n",
    "        level=LEVEL,\n",
    "        samples=SAMPLES,\n",
    "        included_taxa_filter=INCLUDE,\n",
    "        excluded_taxa_filter=EXCLUDE,\n",
    "        normalize=NORMALIZE\n",
    "    )\n",
    "\n",
    "    input_table['Read Fraction'] = input_table['readcount'] / input_table.groupby(['time','patientid'])['readcount'].transform('sum')\n",
    "\n",
    "    input_table['Sample'] = input_table['patientid']+'_'+input_table['time'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "    with_validation = pd.merge(\n",
    "        input_table,\n",
    "        validation_data[['Sample','Taxon ID','Validation Rate']],\n",
    "        left_on=['Sample','taxonid'],\n",
    "        right_on=['Sample','Taxon ID'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "\n",
    "    total_reads = with_validation.groupby('sample',as_index=False)['readcount'].sum()\n",
    "    sample_statistics['sample']= sample_statistics['PatID']+'/'+sample_statistics['time'].astype(str)\n",
    "    total_reads = pd.merge(total_reads,sample_statistics[['sample','Classified Reads']],on='sample',how='left')\n",
    "    total_reads['Total Fraction'] = total_reads['readcount']/total_reads['Classified Reads']\n",
    "    total_reads['Sample ID'] = total_reads['sample']\n",
    "\n",
    "    #Phase 1: Kick out low abundance groups, assign to \"Not enough reads\"\n",
    "    with_validation.loc[with_validation['readcount']<DISCARD_CUTOFF,'taxon'] = 'Not enough reads'\n",
    "\n",
    "    #Phase 2: Check for rest if validates, if not assign to \"Not validated\"\n",
    "    with_validation['Validated'] = (with_validation['Validation Rate'] > 0.2)\n",
    "    with_validation.loc[(with_validation['Validated']!=True)&~(with_validation['taxon'] == 'Not enough reads'), 'taxon'] = 'Not validated'\n",
    "    #Readjust sum (group multiple \"Not enough reads\" entries together)\n",
    "    with_validation = with_validation.groupby(['taxon','time','patientid'],as_index=False).sum()\n",
    "\n",
    "    merged = pd.merge(with_validation,sample_statistics,how='left',left_on=['patientid','time'],right_on=['PatID','time'])\n",
    "\n",
    "    subset_taxa = merged[merged['Read Fraction'] >= PRESENCE_CUTOFF]['taxon'].unique()\n",
    "    subset = merged[\n",
    "        (merged['taxon'].isin(subset_taxa))&\n",
    "        (merged['Read Fraction'] >= PRESENCE_CUTOFF)\n",
    "    ]\n",
    "    \n",
    "    subset=subset[~subset['taxon'].isin(['Not enough reads','Not validated'])]\n",
    "    \n",
    "    charts = []\n",
    "    \n",
    "    for group in subset['timephase'].unique():\n",
    "        grouptable = (subset[\n",
    "            subset['timephase']==group\n",
    "        ].groupby(['taxon'])['sample'].count()/len(\n",
    "                sample_statistics[\n",
    "                sample_statistics['timephase'] == group\n",
    "            ]\n",
    "        )).rename('Fraction {} (n={})'.format(\n",
    "            group,\n",
    "            len(sample_statistics[sample_statistics['timephase']==group])\n",
    "        ))\n",
    "        charts.append(grouptable)\n",
    "        \n",
    "    for group in subset['timephase_and_cluster'].unique():\n",
    "        grouptable = (subset[\n",
    "            subset['timephase_and_cluster']==group\n",
    "        ].groupby(['taxon'])['sample'].count()/len(\n",
    "                sample_statistics[\n",
    "                sample_statistics['timephase_and_cluster'] == group\n",
    "            ]\n",
    "        )).rename('Fraction {} (n={})'.format(\n",
    "            group,\n",
    "            len(sample_statistics[sample_statistics['timephase_and_cluster']==group])\n",
    "        ))\n",
    "        charts.append(grouptable)\n",
    "        \n",
    "    totaltable = (subset.groupby(\n",
    "        ['taxon'])['sample'].count()/112\n",
    "    ).rename('Fraction Total (n=112)')\n",
    "    charts.append(totaltable)\n",
    "    pd.concat(charts,axis=1).sort_values(by='Fraction Total (n=112)',ascending=False).to_excel('Presence_Above_{}_{}_WITHOUT_{}.xlsx'.format(PRESENCE_CUTOFF,\n",
    "        idtonames[INCLUDE],\n",
    "        list(idtonames[x] for x in EXCLUDE)\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bray-Curtis Distance based PCoA <a class=\"anchor\" id=\"pcoa\"></a>\n",
    "$$\n",
    "B_{i,j} = 1 - \\frac{2C_{i,j}}{S_i+S_j}\n",
    "$$\n",
    "\n",
    "For relative abundances, this becomes:\n",
    "\n",
    "$1-C_{i,j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtable = get_normalized_abundances(kraken_dataframe,\n",
    "                                        level='G',\n",
    "                                        samples=PAPER_SAMPLES,\n",
    "                                        excluded_taxa_filter=[CHORDATA,PLANTAE],\n",
    "                                        included_taxa_filter=None,\n",
    "                                       normalize=False)\n",
    "\n",
    "\n",
    "\n",
    "subtable['readAnteil'] = subtable['readcount'] / subtable.groupby(['time','patientid'])['readcount'].transform('sum')\n",
    "\n",
    "subtable = subtable.drop(columns=['level','readcount'])\n",
    "\n",
    "pivot_table = subtable.pivot(index=['patientid','time'],columns=['taxonid'],values=['readAnteil']).fillna(0)\n",
    "data = pivot_table.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pivot_table.index.tolist()\n",
    "taxa_count = len(data[0])\n",
    "\n",
    "def braycurtis(indexA,indexB):\n",
    "    cij = 0\n",
    "    for taxonIndex in range(taxa_count):\n",
    "        cij += min(\n",
    "            data[indexA][taxonIndex],\n",
    "            data[indexB][taxonIndex]\n",
    "        )\n",
    "    if (1-cij) > 1:\n",
    "        print(1-cij)\n",
    "    return 1-cij\n",
    "\n",
    "bc_tuples = []\n",
    "\n",
    "distance_matrix = np.zeros(shape=(len(samples),len(samples)))\n",
    "\n",
    "for x in range(len(samples)):\n",
    "    for y in range(x+1,len(samples)):\n",
    "        print('Calculating all distancesfor sample {} / {}'.format(x+1,len(samples)),end='\\r')\n",
    "        distance = braycurtis(x,y)\n",
    "        distance_matrix[x][y] = distance\n",
    "        distance_matrix[y][x] = distance\n",
    "        bc_tuples.append((samples[x][0]+'/'+str(samples[x][1]),samples[y][0]+'/'+str(samples[y][1]),distance))\n",
    "        \n",
    "INVERT_X = True\n",
    "\n",
    "PCoA = ordination.pcoa(distance_matrix,number_of_dimensions=2)\n",
    "\n",
    "tuples = []\n",
    "\n",
    "for idx,s in PCoA.samples.iterrows():\n",
    "    sample = samples[int(idx)]\n",
    "    patient = sample[0]\n",
    "    time = sample[1]\n",
    "\n",
    "    tuples.append((s.PC1,s.PC2,patient,time))\n",
    "\n",
    "pcoa_table = pd.DataFrame(\n",
    "    tuples,\n",
    "    columns=['x','y','patient','time']\n",
    ")\n",
    "\n",
    "pcoa_table['Name'] = pcoa_table['patient'].astype(str)+'/'+pcoa_table['time'].astype(str)\n",
    "\n",
    "\n",
    "pcoa_table = pd.merge(sample_statistics,pcoa_table,left_on=['time','PatID'],right_on=['time','patient'],how='right')\n",
    "\n",
    "pcoa_table['Sample Type'] = pcoa_table['Name'].apply(lambda x : 'Control' if x.startswith('G') else 'patient')\n",
    "\n",
    "if INVERT_X:\n",
    "    pcoa_table['x'] = -pcoa_table['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = alt.selection_multi(fields=['leukocytephase_cluster_kurz'])\n",
    "color = alt.condition(selection,\n",
    "                      alt.Color('Startcluster:N', legend=None),\n",
    "                      alt.value('lightgray'))\n",
    "\n",
    "\n",
    "pcoa_combined = (alt.Chart(pcoa_table).transform_calculate(\n",
    "    order = f\"indexof({selection.name}.Startcluster || [],datum.Startcluster)\"\n",
    ").mark_point(filled=True).encode(\n",
    "    x='x:Q',\n",
    "    y='y:Q',\n",
    "    color=color,\n",
    "    order=alt.Order('order:N',sort='ascending'),\n",
    "    shape=alt.condition(\n",
    "        alt.datum['Sample Type'] == 'Control',\n",
    "        alt.value('square'),\n",
    "        alt.value('circle'),\n",
    "\n",
    "    ),\n",
    "    tooltip=['Name']\n",
    ").interactive() | alt.Chart(pcoa_table).mark_rect().encode(\n",
    "    y=alt.Y('leukocytephase_cluster_kurz:N',title=None),\n",
    "    color=color\n",
    ").add_selection(selection))\n",
    "\n",
    "pcoa_combined.save(\n",
    "    'Output/PCOA.html'\n",
    ")\n",
    "pcoa_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charts = []\n",
    "\n",
    "for timepoint in pcoa_table['leukocytephase_cluster_2_kurz'].unique():\n",
    "\n",
    "    timepoint_pcoa = alt.Chart(pcoa_table[pcoa_table['leukocytephase_cluster_2_kurz'] != timepoint]).mark_point(filled=True).encode(\n",
    "        x='x:Q',\n",
    "        y='y:Q',\n",
    "        color=alt.value('lightgray'),\n",
    "        shape=alt.condition(\n",
    "            alt.datum['Sample Type'] == 'Control',\n",
    "            alt.value('square'),\n",
    "            alt.value('diamond')\n",
    "        ),\n",
    "        tooltip=['Name']\n",
    "    )+alt.Chart(pcoa_table[\n",
    "        (pcoa_table['leukocytephase_cluster_2_kurz'] == timepoint)\n",
    "        |((pcoa_table['leukocytephase_cluster_2_kurz'] == 'Healthy')&(timepoint=='Pre TX'))\n",
    "    ]).mark_point(filled=True).encode(\n",
    "        x='x:Q',\n",
    "        y='y:Q',\n",
    "        color='Startcluster:N',\n",
    "        shape=alt.condition(\n",
    "            alt.datum['Sample Type'] == 'Control',\n",
    "            alt.value('square'),\n",
    "            alt.value('circle')\n",
    "        ),\n",
    "        tooltip=['Name']\n",
    "    )\n",
    "\n",
    "    charts.append(timepoint_pcoa.properties(title=timepoint))\n",
    "                      \n",
    "reduce(lambda x,y : x&y,charts).resolve_scale(color='shared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_frame = pd.DataFrame(bc_tuples,columns=['Sample X','Sample Y','BCD'])\n",
    "bc_frame[['Patient X','Time X']] = bc_frame['Sample X'].str.split('/',expand=True)\n",
    "bc_frame[['Patient Y','Time Y']] = bc_frame['Sample Y'].str.split('/',expand=True)\n",
    "\n",
    "#Determine most extreme timepoints for each patient\n",
    "\n",
    "extreme_points = pd.concat(\n",
    "    [bc_frame.groupby('Patient X')['Time X'].min().rename('Earliest Time'),\n",
    "    bc_frame.groupby('Patient X')['Time X'].max().rename('Latest Time')],axis=1\n",
    ").reset_index()\n",
    "\n",
    "#Eliminate rows where earliest is >= 0 or latest is <= 0\n",
    "eliminated = extreme_points[\n",
    "    (extreme_points['Earliest Time'].astype(int) < 0)&\n",
    "    (extreme_points['Latest Time'].astype(int) > 0)\n",
    "]\n",
    "\n",
    "print(\n",
    "    'For the following patients no valid pre/post pair could be generated: {} (They will be EXCLUDED from analysis)'.format(\n",
    "        \n",
    "        set(extreme_points['Patient X']).difference(set(eliminated['Patient X']))\n",
    "    )\n",
    ")\n",
    "\n",
    "bc_frame = bc_frame[\n",
    "    #(bc_frame['Patient X'].isin(eliminated['Patient X']))&\n",
    "    #(bc_frame['Patient Y'].isin(eliminated['Patient X']))&\n",
    "    (bc_frame['Patient X'] == bc_frame['Patient Y'])\n",
    "]\n",
    "\n",
    "eliminated = eliminated.set_index('Patient X')\n",
    "\n",
    "#bc_frame['Earliest X'] = bc_frame['Patient X'].apply(lambda x :  eliminated.loc[str(x)]['Earliest Time'])\n",
    "#bc_frame['Latest X'] = bc_frame['Patient X'].apply(lambda x :  eliminated.loc[str(x)]['Latest Time'])\n",
    "\n",
    "#bc_frame = bc_frame[\n",
    "#    (bc_frame['Time X'] == bc_frame['Earliest X'])&\n",
    "#    (bc_frame['Time Y'] == bc_frame['Latest X'])\n",
    "\n",
    "#]\n",
    "\n",
    "bc_frame['Time X'] = bc_frame['Time X'].astype(int)\n",
    "bc_frame['Time Y'] = bc_frame['Time Y'].astype(int)\n",
    "\n",
    "bc_frame = pd.merge(\n",
    "    bc_frame,\n",
    "    sample_statistics[['PatID','time','Startcluster']],\n",
    "    how='left',\n",
    "    left_on=['Patient X','Time X'],\n",
    "    right_on=['PatID','time']\n",
    ")\n",
    "\n",
    "bc_frame\n",
    "\n",
    "bc_frame = pd.merge(\n",
    "    bc_frame,\n",
    "    sample_statistics[['PatID','time','leukocytephase_cluster_2_kurz']],\n",
    "    how='left',\n",
    "    left_on=['Patient Y','Time Y'],\n",
    "    right_on=['PatID','time']\n",
    ")[['Sample X','Sample Y','BCD','Startcluster','leukocytephase_cluster_2_kurz']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(bc_frame).mark_boxplot().encode(\n",
    "    x='Startcluster:N',\n",
    "    y='BCD'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_frame.to_csv('BCPrePostPairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bray Curtis Pre/Reconst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_frame = pd.DataFrame(bc_tuples,columns=['Sample X','Sample Y','BCD'])\n",
    "bc_frame[['Patient X','Time X']] = bc_frame['Sample X'].str.split('/',expand=True)\n",
    "bc_frame[['Patient Y','Time Y']] = bc_frame['Sample Y'].str.split('/',expand=True)\n",
    "bc_frame['Time X'] = bc_frame['Time X'].astype(int)\n",
    "bc_frame['Time Y'] = bc_frame['Time Y'].astype(int)\n",
    "\n",
    "bc_frame = pd.merge(\n",
    "    bc_frame,\n",
    "    sample_statistics[['PatID','time','timephase']],\n",
    "    how='left',\n",
    "    left_on=['Patient X','Time X'],\n",
    "    right_on=['PatID','time']\n",
    ").drop(columns=['PatID','time']).rename(columns={'timephase':'Phase X'})\n",
    "bc_frame = pd.merge(\n",
    "    bc_frame,\n",
    "    sample_statistics[['PatID','time','timephase']],\n",
    "    how='left',\n",
    "    left_on=['Patient Y','Time Y'],\n",
    "    right_on=['PatID','time']\n",
    ").drop(columns=['PatID','time']).rename(columns={'timephase':'Phase Y'})\n",
    "\n",
    "bc_frame = bc_frame[\n",
    "    (bc_frame['Phase X'] == 'Pre TX')&\n",
    "    (bc_frame['Phase Y'] == 'Reconstitution')\n",
    "]\n",
    "\n",
    "bc_frame['Within Patient'] = bc_frame['Patient X'] == bc_frame['Patient Y']\n",
    "bc_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(bc_frame).mark_bar().encode(\n",
    "    x=alt.X('BCD',bin=True),\n",
    "    y='count()'\n",
    ").facet(column='Within Patient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Nadir less similar to Pre-TX than\" Reconstitution Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_frame = pd.DataFrame(bc_tuples,columns=['Sample X','Sample Y','BCD'])\n",
    "bc_frame[['Patient X','Time X']] = bc_frame['Sample X'].str.split('/',expand=True)\n",
    "bc_frame[['Patient Y','Time Y']] = bc_frame['Sample Y'].str.split('/',expand=True)\n",
    "bc_frame['Time X'] = bc_frame['Time X'].astype(int)\n",
    "bc_frame['Time Y'] = bc_frame['Time Y'].astype(int)\n",
    "\n",
    "bc_frame = pd.merge(\n",
    "    bc_frame,\n",
    "    sample_statistics[['PatID','time','timephase']],\n",
    "    how='left',\n",
    "    left_on=['Patient X','Time X'],\n",
    "    right_on=['PatID','time']\n",
    ").drop(columns=['PatID','time']).rename(columns={'timephase':'Phase X'})\n",
    "bc_frame = pd.merge(\n",
    "    bc_frame,\n",
    "    sample_statistics[['PatID','time','timephase']],\n",
    "    how='left',\n",
    "    left_on=['Patient Y','Time Y'],\n",
    "    right_on=['PatID','time']\n",
    ").drop(columns=['PatID','time']).rename(columns={'timephase':'Phase Y'})\n",
    "\n",
    "\n",
    "bc_frame['Within Patient'] = bc_frame['Patient X'] == bc_frame['Patient Y']\n",
    "\n",
    "bc_frame = bc_frame[\n",
    "    (bc_frame['Within Patient'] == True)&\n",
    "    (bc_frame['Phase X'] == 'Pre TX')\n",
    "]\n",
    "\n",
    "\n",
    "extreme_points = pd.concat(\n",
    "    [bc_frame.groupby(['Patient X','Phase X','Phase Y'])['Time X'].min().rename('Earliest Time'),\n",
    "    bc_frame.groupby(['Patient X','Phase X','Phase Y'])['Time Y'].max().rename('Latest Time')],axis=1\n",
    ").reset_index()\n",
    "\n",
    "bc_frame = pd.merge(bc_frame,extreme_points,on=['Patient X','Phase X','Phase Y'],how='left')\n",
    "bc_frame = bc_frame[\n",
    "    (bc_frame['Time X'] == bc_frame['Earliest Time'])&\n",
    "    (bc_frame['Time Y'] == bc_frame['Latest Time'])\n",
    "]\n",
    "bc_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(bc_frame).mark_boxplot().encode(\n",
    "    x='Phase Y',\n",
    "    y='BCD'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_frame = bc_frame.pivot(\n",
    "    index='Patient X',\n",
    "    columns='Phase Y',\n",
    "    values='BCD'\n",
    ").dropna()\n",
    "\n",
    "wilcoxon(bc_frame['Leukozytopenia'],bc_frame['Reconstitution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_frame['Diff'] = bc_frame['Leukozytopenia']-bc_frame['Reconstitution']\n",
    "bc_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(bc_frame).mark_boxplot().encode(\n",
    "    y='Diff'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Time Overview <a class=\"anchor\" id=\"sampletimes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_reduced = outcomes[\n",
    "    ['Pat ID',\n",
    "     'Day relative to HSCT',\n",
    "     'Day relative to HSCT.1',\n",
    "     'Day relative to HSCT.2',\n",
    "     'Day relative to HSCT.3',\n",
    "     'Day relative to HSCT.4',\n",
    "     'Day relative to HSCT.5',\n",
    "     'Day relative to HSCT.6',\n",
    "     'Day relative to HSCT.7',\n",
    "    ]\n",
    "         ].rename(\n",
    "    columns={\n",
    "        'Day relative to HSCT' : '1st Relapse',\n",
    "        'Day relative to HSCT.1' : '2nd Relapse',\n",
    "        'Day relative to HSCT.2' : '2nd HSCT',\n",
    "        'Day relative to HSCT.3' : 'Acute GvHD Grade 1-2',\n",
    "        'Day relative to HSCT.4' : 'Acute GvHD Grade 3-4',\n",
    "        'Day relative to HSCT.5' : 'Moderate cGvHD',\n",
    "        'Day relative to HSCT.6' : 'Severe cGvHD',\n",
    "        'Day relative to HSCT.7' : 'Death',\n",
    "    }\n",
    ")\n",
    "outcomes_reduced['No Adverse Event']=outcomes_reduced.apply( lambda x : 0 if x.count() <= 1 else np.NaN,axis=1)\n",
    "outcomes_reduced\n",
    "\n",
    "outcomes_reduced = outcomes_reduced.melt(id_vars=['Pat ID'])\n",
    "outcomes_reduced = outcomes_reduced[(outcomes_reduced['value']==outcomes_reduced['value'])]\n",
    "outcomes_reduced = outcomes_reduced[(outcomes_reduced['value']!='?')]\n",
    "outcomes_reduced.loc[\n",
    "    (outcomes_reduced['Pat ID']=='18.2'),'KI ID'\n",
    "] = '18.2'\n",
    "outcomes_reduced.loc[(outcomes_reduced['Pat ID']=='18.2'),'value'] -= OFFSET_PAT_18\n",
    "outcomes_reduced = outcomes_reduced[outcomes_reduced['variable'] != 'No Adverse Event']\n",
    "\n",
    "outcomes_reduced = outcomes_reduced.rename(columns={\n",
    "    'Pat ID' : 'PatID',\n",
    "    'variable' : 'Adverse Event',\n",
    "    'value' : 'time'\n",
    "})\n",
    "\n",
    "\n",
    "############\n",
    "\n",
    "sample_statistics['id'] = sample_statistics['PatID']+'/'+sample_statistics['time'].astype(str)\n",
    "overview = sample_statistics[sample_statistics['id'].isin(PAPER_SAMPLES)]\n",
    "overview = overview[~overview['PatID'].str.startswith('G')]\n",
    "\n",
    "\n",
    "combined = pd.concat([overview,outcomes_reduced])\n",
    "\n",
    "\n",
    "\n",
    "patientlist_sorted = []\n",
    "\n",
    "for startcluster in combined['Startcluster'].unique():\n",
    "    clusterlist = sorted(combined[combined['Startcluster'] == startcluster]['PatID'].unique())\n",
    "    patientlist_sorted += clusterlist\n",
    "\n",
    "        \n",
    "overview = (alt.Chart(combined[combined['Adverse Event'] == combined['Adverse Event']]).mark_point(size=38,color='black').encode(\n",
    "    x=alt.X('time',scale=alt.Scale(type='symlog'),axis=alt.Axis(grid=False)),\n",
    "    y=alt.Y('PatID',sort=patientlist_sorted),\n",
    "    shape=alt.Shape('Adverse Event:N',scale=alt.Scale(\n",
    "        domain=['2nd HSCT','Acute GvHD Grade 1-2','Acute GvHD Grade 3-4','Moderate cGvHD','Severe cGvHD','1st Relapse','2nd Relapse','Death'],\n",
    "        range=['circle','square','square','diamond','diamond','triangle','triangle','cross']))\n",
    ")+alt.Chart(combined,width=800).mark_circle().encode(\n",
    "    x=alt.X('time',scale=alt.Scale(type='symlog'),axis=alt.Axis(grid=False)),\n",
    "    y=alt.Y('PatID:N',sort=patientlist_sorted,axis=alt.Axis(grid=True)),\n",
    "    color='Startcluster:N'\n",
    ")+alt.Chart(combined).mark_text(dx=0,dy=-6).encode(\n",
    "    x=alt.X('time',scale=alt.Scale(type='symlog'),axis=alt.Axis(grid=False)),\n",
    "    y=alt.Y('PatID:N',sort=patientlist_sorted,axis=alt.Axis(grid=True)),\n",
    "    text='time'\n",
    "))\n",
    "\n",
    "overview.save('Output/Samples.html')\n",
    "\n",
    "overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zymo Std. Analysis <a class=\"anchor\" id=\"zymo\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_level = get_normalized_abundances(kraken_dataframe,samples=['Zymo_Power/-100'],level='G',excluded_taxa_filter=None,normalize=False)\n",
    "\n",
    "species_level['Read Fraction (%)'] = species_level['readcount'] / species_level.groupby(['sample'])['readcount'].transform('sum')\n",
    "species_level = species_level.rename(columns={'sample' : 'Sample ID','taxon' : 'Taxon'})\n",
    "validation_data['sample']=validation_data['patientid']+'/'+validation_data['time'].astype(str)\n",
    "with_validation = pd.merge(\n",
    "    species_level,\n",
    "    validation_data[['sample','Taxon ID','Validation Rate']],\n",
    "    left_on=['Sample ID','taxonid'],\n",
    "    right_on=['sample','Taxon ID'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "sample_statistics['sample']= sample_statistics['PatID']+'/'+sample_statistics['time'].astype(str)\n",
    "\n",
    "#Phase 1: Kick out low abundance groups, assign to \"Not enough reads\"\n",
    "with_validation.loc[with_validation['readcount']<DISCARD_CUTOFF,'taxon'] = 'Not enough reads'\n",
    "\n",
    "#Phase 2: Check for rest if validates, if not assign to \"Not validated\"\n",
    "with_validation['Validated'] = (with_validation['Validation Rate'] > 0.2)\n",
    "with_validation.loc[\n",
    "    (with_validation['Validated']!=True)&~(with_validation['taxon'] == 'Not enough reads'), 'Taxon'\n",
    "] = 'Not validated'\n",
    "#Readjust sum (group multiple \"Not enough reads\" entries together)\n",
    "with_validation = with_validation.groupby(['Taxon','time','patientid','Sample ID'],as_index=False).sum()\n",
    "\n",
    "zymo_theory['Taxon'] = zymo_theory['Taxon'].apply( lambda x : x.split()[0])\n",
    "zymo_theory = zymo_theory.groupby(['Sample ID','Taxon'],as_index=False).sum()\n",
    "\n",
    "\n",
    "with_validation = pd.concat([with_validation,zymo_theory])[['Sample ID','Taxon','Read Fraction (%)']]\n",
    "melt = zymo_minimap.melt(id_vars='Index')\n",
    "\n",
    "def clean(x):\n",
    "    x = x.replace('_',' ').replace('albican','albicans')  \n",
    "    if x.startswith('Escherichia coli'):\n",
    "        return 'Escherichia coli'\n",
    "    return x\n",
    "\n",
    "melt['variable'] = melt['variable'].apply(\n",
    "    clean\n",
    ")\n",
    "\n",
    "\n",
    "melt = melt.rename(\n",
    "    columns={\n",
    "    'variable' : 'Taxon',\n",
    "        'Index' : 'Sample ID',\n",
    "        'value' : 'Read Fraction (%)'\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "melt = melt[melt['Sample ID'].isin(['Power/Reads','Power/Bases'])]\n",
    "\n",
    "\n",
    "melt['Sample ID'] = melt['Sample ID'].apply(lambda x : x + '_Minimap2')\n",
    "\n",
    "melt['Taxon'] = melt['Taxon'].apply(lambda x : 'Unmapped' if x == 'unmapped' else x)\n",
    "\n",
    "#Reduce to Genus\n",
    "melt['Taxon'] = melt['Taxon'].apply( lambda x : x.split()[0])\n",
    "melt = melt.groupby(['Sample ID','Taxon'],as_index=False).sum()\n",
    "\n",
    "combined = pd.concat([melt,with_validation])\n",
    "\n",
    "taxa_we_look_at = list(combined[combined['Sample ID'] == 'Power/Reads_Minimap2']['Taxon'].unique())\n",
    "if 'Not validated' not in taxa_we_look_at:\n",
    "    taxa_we_look_at.append('Not validated')\n",
    "print('Determined the following taxa as relevant:',taxa_we_look_at)\n",
    "\n",
    "combined.loc[~combined['Taxon'].isin(taxa_we_look_at), 'Taxon'] = 'Other'\n",
    "combined = combined.groupby(['Taxon','Sample ID'],as_index=False).sum()\n",
    "\n",
    "\n",
    "colorMap = {}\n",
    "\n",
    "taxa = combined['Taxon'].unique()\n",
    "\n",
    "palette = sns.color_palette(\"tab20\",n_colors=len(taxa))\n",
    "bright = palette[::2]\n",
    "muted = palette[1::2]\n",
    "palette = bright+muted\n",
    "\n",
    "for tax,col in zip(list(taxa),palette):\n",
    "    colorMap[tax] = col #colors.to_hex(col)\n",
    "    \n",
    "altdomain = []\n",
    "altrange = []\n",
    "\n",
    "for x in combined['Taxon'].unique():\n",
    "    \n",
    "    if x == 'Other' or x == 'Unmapped' or x == 'Not validated':\n",
    "        continue\n",
    "\n",
    "    c = colorMap[x]\n",
    "    altdomain.append(x)\n",
    "    color = colors.to_hex(c)\n",
    "    altrange.append(color)\n",
    "    \n",
    "altdomain.append('Other')\n",
    "altrange.append(colors.to_hex((1,1,1)))\n",
    "altdomain.append('Unmapped')\n",
    "altrange.append(colors.to_hex((0.45,0.45,0.45)))\n",
    "altdomain.append('Not validated')\n",
    "altrange.append(colors.to_hex((0.55,0.55,0.55)))\n",
    "\n",
    "c= alt.Chart(\n",
    "  combined\n",
    ").transform_calculate(\n",
    "order=f\"-indexof({altdomain}, datum.Taxon)\"\n",
    ").mark_bar(stroke='black',strokeWidth=0.5,strokeOpacity=0.9).encode(\n",
    "    x=alt.X('Sample ID:N',sort=['Zymo_Power/-100']\n",
    "),\n",
    "    y=alt.Y('Read Fraction (%):Q',scale=alt.Scale(domain=(0,1))),\n",
    "    color=alt.Color('Taxon:N',legend=alt.Legend(columns=2,symbolLimit=0,labelLimit=0),scale=alt.Scale(domain=altdomain,range=altrange)),\n",
    "    tooltip=['Read Fraction (%)','Taxon'],\n",
    "    order=alt.Order('order:Q')\n",
    ")\n",
    "c.save('Output/Zymo_Overview.html')\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.pivot(index='Taxon',columns='Sample ID',values='Read Fraction (%)').fillna(0).to_csv('ZymoStdAbundances_{}.csv'.format(LEVEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zymo Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.pivot(index='Taxon',columns='Sample ID',values='Read Fraction (%)').fillna(0).corr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Illumina Congruence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This data can be collected from the snakemake workflow\n",
    "kdf_illumina = pd.read_csv('Input/fulldumpKrakenIllumina_2022-01-13.csv',usecols=[1,2,3,4,5,6],dtype={'patientid' : str,'taxonid':str})\n",
    "adjust_table(kdf_illumina)         \n",
    "kdf_illumina['sample'] = kdf_illumina['patientid']+'/'+kdf_illumina['time'].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eukaryota_illumina = get_normalized_abundances(kdf_illumina,normalize=False,level='S',included_taxa_filter=EUKARYOTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eukaryota_nanopore = get_normalized_abundances(kraken_dataframe,normalize=False,level='S',included_taxa_filter=EUKARYOTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_samples = set(eukaryota_illumina['sample'].unique()).intersection(eukaryota_nanopore['sample'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eukaryota_illumina = eukaryota_illumina[eukaryota_illumina['sample'].isin(common_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eukaryota_nanopore = eukaryota_nanopore[eukaryota_nanopore['sample'].isin(common_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eukaryota_nanopore['Read Fraction'] = eukaryota_nanopore['readcount'] / eukaryota_nanopore.groupby(['time','patientid'])['readcount'].transform('sum')\n",
    "eukaryota_illumina['Read Fraction'] = eukaryota_illumina['readcount'] / eukaryota_illumina.groupby(['time','patientid'])['readcount'].transform('sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eukaryota_nanopore=eukaryota_nanopore.rename(columns={'Read Fraction' : 'Read Fraction Nanopore'})\n",
    "eukaryota_illumina=eukaryota_illumina.rename(columns={'Read Fraction' : 'Read Fraction Illumina'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(\n",
    "    eukaryota_illumina,\n",
    "    eukaryota_nanopore,\n",
    "    how='outer',\n",
    "    on=['patientid','time','taxon']\n",
    ").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(merged).mark_point().encode(\n",
    "    x=alt.X('Read Fraction Nanopore',scale=alt.Scale(type='symlog')),\n",
    "    y=alt.Y('Read Fraction Illumina',scale=alt.Scale(type='symlog')),\n",
    "    color='taxon',\n",
    "    tooltip=['taxon']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(merged[~merged['taxon'].isin(['Unassigned at Level','Homo sapiens'])]).mark_point().encode(\n",
    "    x=alt.X('Read Fraction Nanopore',scale=alt.Scale(type='symlog')),\n",
    "    y=alt.Y('Read Fraction Illumina',scale=alt.Scale(type='symlog')),\n",
    "    color='taxon',\n",
    "    tooltip=['taxon']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crassphage <a class=\"anchor\" id=\"crassphage\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kraken_data = kraken_dataframe[kraken_dataframe['taxon'].isin(['uncultured crAssphage','unclassified Crassvirales','root'])]\n",
    "kraken_data = kraken_data[kraken_data['sample'].isin(PAPER_SAMPLES)]\n",
    "kraken_data = kraken_data.pivot(\n",
    "    index='sample',columns='taxon',values='readcount'\n",
    ").fillna(0)\n",
    "\n",
    "kraken_data['crassphage_detected'] = (kraken_data['uncultured crAssphage']+kraken_data['unclassified Crassvirales'])/kraken_data['root']\n",
    "\n",
    "kraken_data = kraken_data.reset_index()\n",
    "\n",
    "def rename(x):\n",
    "    split = x.rsplit('_',1)\n",
    "    return split[0]+'/'+split[1]\n",
    "\n",
    "minimap_data = pd.read_csv('Input/Crassphage/summary.csv')\n",
    "minimap_data['Sample'] = minimap_data['Sample'].apply(rename)\n",
    "\n",
    "\n",
    "minimap_data_aggregated = minimap_data.groupby('Sample')[['Fraction Mapped','Mapped Reads']].sum().reset_index()\n",
    "\n",
    "\n",
    "combined = pd.merge(kraken_data,minimap_data_aggregated,left_on='sample',right_on='Sample',how='left')\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = max(\n",
    "    combined['Fraction Mapped'].max(),combined['crassphage_detected'].max()\n",
    ")\n",
    "\n",
    "line = pd.DataFrame({\n",
    "    'X': [0, max_value],\n",
    "    'Y': [0, max_value],\n",
    "})\n",
    "\n",
    "\n",
    "(alt.Chart(combined,width=400,height=400).mark_point().encode(\n",
    "    y=alt.Y('Fraction Mapped',scale=alt.Scale(type='symlog',constant=0.001)),\n",
    "    x=alt.X('crassphage_detected',scale=alt.Scale(type='symlog',constant=0.001))\n",
    ")+alt.Chart(line,width=700,height=700).mark_line(color= 'lightgray').encode(\n",
    "        x= 'X',\n",
    "        y= 'Y'\n",
    "    )).interactive()#.save('Output/CrassphageKrakenVsMinimap.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap, which Crassphage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_X = 50\n",
    "\n",
    "heatmap_table = minimap_data.pivot(index='Sample',columns='Reference',values='Fraction Mapped').fillna(0).melt(ignore_index=False).reset_index().rename(columns={'value' : 'Fraction Mapped'})\n",
    "heatmap_table['Unambiguous'] = heatmap_table['Reference'] != 'Ambiguous'\n",
    "\n",
    "#For top bar charts\n",
    "heatmap_table_simplified = heatmap_table.groupby(['Sample','Unambiguous'],as_index=False)['Fraction Mapped'].sum()\n",
    "\n",
    "\n",
    "heatmap_table_filtered = heatmap_table[\n",
    "    (heatmap_table['Sample'].isin(PAPER_SAMPLES)  )\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "heatmap_table_filtered['Fraction Mapped']=heatmap_table_filtered['Fraction Mapped']/heatmap_table_filtered.groupby('Sample')['Fraction Mapped'].transform('sum')\n",
    "top_refs = heatmap_table_filtered.groupby('Reference')['Fraction Mapped'].sum().sort_values(ascending=False).keys().tolist()[:TOP_X]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "heatmap_table_filtered = heatmap_table_filtered[\n",
    "    \n",
    "    heatmap_table_filtered['Reference'].isin(top_refs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = (alt.Chart(heatmap_table_simplified[heatmap_table_simplified['Sample'].isin(PAPER_SAMPLES)]).mark_bar().encode(\n",
    "    x=alt.X('Sample',sort=sort_samples(PAPER_SAMPLES),axis=alt.Axis(orient='top')),\n",
    "    y=alt.Y('Fraction Mapped',stack=True),\n",
    "    color='Unambiguous'\n",
    ")&alt.Chart(heatmap_table_filtered).mark_rect().encode(\n",
    "    x=alt.X('Sample',sort=sort_samples(PAPER_SAMPLES),axis=None),\n",
    "    y=alt.Y('Reference',sort=max_refs),\n",
    "    color=alt.Color('Fraction Mapped:Q',title='Fraction of uniquely assigned reads'),\n",
    "    tooltip=['Fraction Mapped']\n",
    ")).resolve_scale(x='shared',color='independent',y='independent')\n",
    "\n",
    "chart.save('Output/CrassphageOverview.html')\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migration = pd.read_csv('Input/Crassphage/migration.csv',dtype={'Tax ID' : str})\n",
    "migration['Fraction'] = migration['Reads']/migration.groupby('Sample')['Reads'].transform('sum')\n",
    "migration['Taxon Name'] = migration['Tax ID'].map(idtonames)\n",
    "\n",
    "top_hits = migration.groupby('Tax ID')['Fraction'].mean().sort_values(ascending=False).keys()[:20]\n",
    "migration = migration[migration['Tax ID'].isin(top_hits)]\n",
    "\n",
    "def categorize(taxon_name):\n",
    "    if taxon_name in ['uncultured phage cr6_1','uncultured crAssphage','CrAss-like virus sp.']:\n",
    "        return 'crAssphage classification'\n",
    "    elif taxon_name != taxon_name:\n",
    "        return 'unclassified'\n",
    "    return 'other classification'\n",
    "\n",
    "migration['Category'] = migration['Taxon Name'].apply(categorize)\n",
    "migration['Taxon Name'] = migration['Taxon Name'].apply(lambda x : 'unclassified' if x != x else x)\n",
    "\n",
    "crassphages = {'uncultured phage cr6_1','uncultured crAssphage','CrAss-like virus sp.'}\n",
    "rest = set(migration['Taxon Name'])-(crassphages.union({'unclassified'}))\n",
    "sorted_taxa = ['unclassified']+list(sorted(rest))+list(crassphages)\n",
    "\n",
    "alt.Chart(migration).mark_boxplot().encode(\n",
    "    y='Fraction',\n",
    "    x=alt.X('Taxon Name:N',sort=sorted_taxa),\n",
    "    color='Category',\n",
    "    tooltip=['Reads','Sample','Fraction','Taxon Name']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = pd.read_csv('Input/crassphage_details.csv')\n",
    "\n",
    "charts = []\n",
    "\n",
    "for metric in ['Percentage Aligned','Mapping Quality','Identity']:\n",
    "\n",
    "    amount,edges = np.histogram(details[metric],bins=100)\n",
    "\n",
    "    centers = []\n",
    "    for x,y in zip(edges[:-1],edges[1:]):\n",
    "        centers.append((x+y)/2)\n",
    "\n",
    "    tuples = []\n",
    "    for x,y in zip(centers,amount):\n",
    "        tuples.append((x,y))\n",
    "\n",
    "    df = pd.DataFrame(tuples,columns=[metric,'Count'])\n",
    "\n",
    "    c = alt.Chart(df).mark_bar().encode(\n",
    "        x=metric,\n",
    "        y=alt.Y('Count',scale=alt.Scale(type='symlog'))\n",
    "    )\n",
    "    \n",
    "    charts.append(c)\n",
    "    \n",
    "reduce(lambda x,y : x&y, charts).save('Output/CrassphageAlignmentDetails.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marker Species Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIDENCE_INTERVAL_ALPHA = 0.05\n",
    "\n",
    "genus_data = get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    samples=PAPER_SAMPLES,\n",
    "    level='G',\n",
    "    excluded_taxa_filter=[CHORDATA,PLANTAE],\n",
    "    normalize=False\n",
    ")\n",
    "\n",
    "total_roots = genus_data.groupby('sample')['readcount'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genus_data['Read Fraction'] =  genus_data['readcount'] / genus_data.groupby(['sample'])['readcount'].transform('sum')\n",
    "genus_data.loc[genus_data['taxon']=='Crassphage Pseudo-Genus', 'taxon'] = 'uncultured crAssphage'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_SAMPLES_UNDERSCORE = [x.replace('/','_') for x in PAPER_SAMPLES]\n",
    "marker_data = validation_data\n",
    "marker_data = marker_data[marker_data['Sample'].isin(PAPER_SAMPLES_UNDERSCORE)]\n",
    "marker_data['Validated'] = marker_data['Validation Rate'] >= 0.2\n",
    "marker_data['sample'] = marker_data['patientid']+'/'+validation_data['time'].astype(str)\n",
    "\n",
    "os.makedirs('marker_genera_overview',exist_ok=True)\n",
    "\n",
    "\n",
    "def SORTFUNCTION(x):\n",
    "    return (\n",
    "        x[0] == 'G', #Sort by G or regular patient first\n",
    "        float(x.split('G')[-1])\n",
    "    )\n",
    "\n",
    "patientlist_sorted = sorted(\n",
    "    marker_data['patientid'].unique(),key=SORTFUNCTION\n",
    ")\n",
    "\n",
    "\n",
    "for taxon in MARKER_GENERA:\n",
    "    \n",
    "    subtable = marker_data[\n",
    "        marker_data['Taxon Name'] == taxon\n",
    "    ]\n",
    "\n",
    "    subtable = pd.merge(\n",
    "        genus_data,\n",
    "        subtable[['sample','Taxon Name','Validated','Sample']],\n",
    "        left_on=['sample','taxon'],\n",
    "        right_on=['sample','Taxon Name'],\n",
    "        how='right'\n",
    "    )\n",
    "\n",
    "    substitutes = []\n",
    "    for sample in PAPER_SAMPLES_UNDERSCORE:\n",
    "        if sample not in subtable['Sample'].unique():\n",
    "            patientid,time = sample.rsplit('_',1)\n",
    "            substitutes.append(\n",
    "                (patientid,int(time),'?',0,patientid+'/'+str(time),0)\n",
    "            )\n",
    "    subtable = pd.concat([subtable,pd.DataFrame(substitutes,columns=['patientid','time','Validated','readcount','sample','Read Fraction'])]) \n",
    "\n",
    "\n",
    "    subtable[['Confidence Interval Low','Confidence Interval High']] = subtable.apply(\n",
    "        lambda row : proportion_confint(row['readcount'],total_roots[row['sample']],alpha=CONFIDENCE_INTERVAL_ALPHA),\n",
    "        axis=1,\n",
    "        result_type='expand'\n",
    "    )   \n",
    "    \n",
    "    subtable['Validated'] = subtable['Validated'].map(\n",
    "        {\n",
    "            '?' : 'Low abundance/Not validated',\n",
    "            False : 'Low abundance/Not validated',\n",
    "            True : 'Validated'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    subtable['patientid'] = subtable['patientid'].astype(str)\n",
    "      \n",
    "\n",
    "    c=alt.Chart(subtable[~subtable['patientid'].str.startswith('G')],height=40)\n",
    "    c2 = alt.Chart(subtable[subtable['patientid'].str.startswith('G')],height=40)\n",
    "    c = (c.mark_rule(size=20,strokeWidth=3).encode(\n",
    "            x=alt.X('time',scale=alt.Scale(type='symlog'),axis=alt.Axis(grid=False)),\n",
    "            color=alt.Color('Validated',scale=alt.Scale(domain=['Low abundance/Not validated','Validated'],range=['lightgrey','orange'])),\n",
    "            y=alt.Y('Confidence Interval Low:Q',title=None,axis=alt.Axis(tickCount=2)),\n",
    "            y2=alt.Y2('Confidence Interval High:Q',title=None)\n",
    "\n",
    "        )+c.mark_point(size=50,filled=True).encode(\n",
    "            x=alt.X('time',scale=alt.Scale(type='symlog'),axis=alt.Axis(grid=False)),\n",
    "            color=alt.Color('Validated',scale=alt.Scale(domain=['Low abundance/Not validated','Validated'],range=['lightgrey','orange'])),\n",
    "            y=alt.Y('Read Fraction:Q',title=None),\n",
    "\n",
    "        )).facet(row=alt.Row('patientid',sort=patientlist_sorted)).resolve_scale(y='independent')|(\n",
    "        c2.mark_rule(size=20,strokeWidth=3).encode(\n",
    "            x=alt.X('patientid',axis=alt.Axis(grid=False)),\n",
    "            color=alt.Color('Validated',scale=alt.Scale(domain=['Low abundance/Not validated','Validated'],range=['lightgrey','orange'])),\n",
    "            y=alt.Y('Confidence Interval Low:Q',title=None,axis=alt.Axis(tickCount=2)),\n",
    "            y2=alt.Y2('Confidence Interval High:Q',title=None)\n",
    "\n",
    "        )+c2.mark_point(size=50,filled=True).encode(\n",
    "            x=alt.X('patientid',axis=alt.Axis(grid=False)),\n",
    "            color=alt.Color('Validated',scale=alt.Scale(domain=['Low abundance/Not validated','Validated'],range=['lightgrey','orange'])),\n",
    "            y=alt.Y('Read Fraction:Q',title=None),\n",
    "\n",
    "        )\n",
    "    )\n",
    "    c.save('marker_genera_overview/{}.svg'.format(taxon))\n",
    "    for time2 in ['Leukozytopenia','Reconstitution']:\n",
    "\n",
    "        annotated_subtable = pd.merge(\n",
    "            subtable,\n",
    "            sample_statistics[['sample','leukocytephase_cluster_2_kurz']],\n",
    "            on='sample'\n",
    "        )\n",
    "\n",
    "        tuples = []\n",
    "\n",
    "        for patient in annotated_subtable['patientid'].unique():\n",
    "            if patient.startswith('G'):\n",
    "                continue\n",
    "\n",
    "            if not ((\n",
    "                'Pre TX' in annotated_subtable[\n",
    "                    (annotated_subtable['patientid'] == patient)\n",
    "                ]['leukocytephase_cluster_2_kurz'].unique()\n",
    "            ) and (\n",
    "                time2 in annotated_subtable[\n",
    "                    (annotated_subtable['patientid'] == patient)\n",
    "                ]['leukocytephase_cluster_2_kurz'].unique()\n",
    "            )   ):\n",
    "                continue\n",
    "                #print('Patient {} does not have both a Pre-TX and a {} sample'.format(patient,time2))\n",
    "            else:\n",
    "                pass\n",
    "                #print('Sufficient data for patient {}'.format(patient))\n",
    "\n",
    "            pre_tx_presence = False\n",
    "\n",
    "            if len(\n",
    "                annotated_subtable[\n",
    "                    (annotated_subtable['patientid'] == patient)&\n",
    "                    (annotated_subtable['leukocytephase_cluster_2_kurz'] == 'Pre TX')&\n",
    "                    (annotated_subtable['Validated'] == 'Validated')\n",
    "                ]\n",
    "            ) > 0:\n",
    "                pre_tx_presence = True\n",
    "\n",
    "            leukopenia_presence = False\n",
    "\n",
    "            if len(\n",
    "                annotated_subtable[\n",
    "                    (annotated_subtable['patientid'] == patient)&\n",
    "                    (annotated_subtable['leukocytephase_cluster_2_kurz'] == time2)&\n",
    "                    (annotated_subtable['Validated'] == 'Validated')\n",
    "                ]\n",
    "            ) > 0:\n",
    "                leukopenia_presence = True       \n",
    "            tuples.append((patient,pre_tx_presence,leukopenia_presence))\n",
    "\n",
    "        pres_table = pd.DataFrame(tuples,columns=['Patient','Pre-TX Presence','Time2 Presence'])\n",
    "\n",
    "        ct = pd.crosstab(pres_table['Pre-TX Presence'],pres_table['Time2 Presence'])\n",
    "        c, p, dof, expected = chi2_contingency(ct)\n",
    "        print('Based on {} pairs we get a p-value of {} ({}/{})'.format(\n",
    "            len(pres_table),\n",
    "            p,\n",
    "            taxon,\n",
    "            time2\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illumina Sequencing / Population Shifts <a class=\"anchor\" id=\"strains\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load samplesheet (To resolve illumina ids -> patient/time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesheetDictPatient = {}\n",
    "samplesheetDictTime = {}\n",
    "    \n",
    "samplesheet = pd.read_csv('Input/samples.tsv',sep='\\t')\n",
    "#Retain only entries that have illumina files\n",
    "samplesheet=samplesheet[samplesheet['illuminafile'] ==samplesheet['illuminafile']]\n",
    "for idx,row in samplesheet.iterrows():\n",
    "    samplesheetDictPatient[row['illuminafile']] = row['patientid']\n",
    "    samplesheetDictTime[row['illuminafile']] = row['time']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process/annotate distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGH_CONFIDENCE_CUTOFF = 50\n",
    "output_folder = 'Output/StrainAnalysis/GutTrSnp_{}'.format(HIGH_CONFIDENCE_CUTOFF)\n",
    "\n",
    "guttrsnp_distances = pd.read_csv('Input/GutTrSnp/RealData_{}/distances.csv'.format(\n",
    "   HIGH_CONFIDENCE_CUTOFF \n",
    "),dtype={\n",
    "    'Destination ID':str,\n",
    "    'Source ID' :str,\n",
    "    'Taxon' : str\n",
    "})\n",
    "\n",
    "#Kick out the stuff where there was no overlap and thus no distance\n",
    "guttrsnp_distances = guttrsnp_distances.dropna()\n",
    "guttrsnp_distances=guttrsnp_distances[guttrsnp_distances['Share Of Overlap'] >= 0.01]\n",
    "\n",
    "guttrsnp_distances['Taxon Name'] = guttrsnp_distances['Taxon'].map(idtonames)\n",
    "guttrsnp_distances['Source'] = guttrsnp_distances['Source ID']+'/'+guttrsnp_distances['Source Time'].astype(str)\n",
    "guttrsnp_distances['Destination'] = guttrsnp_distances['Destination ID']+'/'+guttrsnp_distances['Destination Time'].astype(str)\n",
    "guttrsnp_distances['PrePost Pair'] = (\n",
    "    guttrsnp_distances['Source ID'] == guttrsnp_distances['Destination ID']\n",
    ")&(\n",
    "guttrsnp_distances['Source Time'] < 0\n",
    ")&(\n",
    "guttrsnp_distances['Destination Time'] > 0\n",
    ")\n",
    "guttrsnp_distances['Within Patient'] = guttrsnp_distances['Source ID'] == guttrsnp_distances['Destination ID']\n",
    "#DoT\n",
    "guttrsnp_distances['Elapsed Time'] = guttrsnp_distances['Destination Time'] - guttrsnp_distances['Source Time']\n",
    "guttrsnp_distances['Distance over Time'] = guttrsnp_distances['GutTrSnp Distance'] / guttrsnp_distances['Elapsed Time']\n",
    "\n",
    "guttrsnp_distances['Distance Name'] = guttrsnp_distances['Source Time'].astype(str) + '->' + guttrsnp_distances['Destination Time'].astype(str)\n",
    "\n",
    "coverages = pd.read_csv('Input/GutTrSnp/RealData_{}/coverages.csv'.format(\n",
    "    HIGH_CONFIDENCE_CUTOFF\n",
    "),dtype={\n",
    "    'Patient ID' : str,\n",
    "    'Taxon ID' : str\n",
    "})\n",
    "guttrsnp_distances=pd.merge(guttrsnp_distances,coverages,how='left',left_on=['Source ID','Source Time','Taxon'],right_on=['Patient ID','Time','Taxon ID'])\n",
    "guttrsnp_distances=pd.merge(guttrsnp_distances,coverages,how='left',left_on=['Destination ID','Destination Time','Taxon'],right_on=['Patient ID','Time','Taxon ID'])\n",
    "\n",
    "\n",
    "\n",
    "guttrsnp_distances = guttrsnp_distances.drop(columns=[\n",
    "    'Patient ID_x','Patient ID_y',\n",
    "    'Time_x','Time_y',\n",
    "    'Taxon ID_x','Taxon ID_y'\n",
    "])\n",
    "\n",
    "\n",
    "bins=[0,10,20,50,100,200,500,1000,2000]\n",
    "\n",
    "\n",
    "guttrsnp_distances['VCG Source'] = pd.cut(guttrsnp_distances['Average Vertical Coverage_x'],bins)\n",
    "guttrsnp_distances['VCG Destination'] = pd.cut(guttrsnp_distances['Average Vertical Coverage_y'],bins)\n",
    "guttrsnp_distances = guttrsnp_distances.dropna()\n",
    "guttrsnp_distances['VCG Source']=guttrsnp_distances['VCG Source'].astype(str)\n",
    "guttrsnp_distances['VCG Destination']=guttrsnp_distances['VCG Destination'].astype(str)\n",
    "\n",
    "timepoints = {}\n",
    "\n",
    "for taxon in guttrsnp_distances['Taxon'].unique():\n",
    "    for source_id in guttrsnp_distances['Source ID'].unique():\n",
    "        subtable = guttrsnp_distances[\n",
    "            (\n",
    "                guttrsnp_distances['Taxon'] == taxon\n",
    "            )&(\n",
    "                guttrsnp_distances['Source ID'] == source_id\n",
    "                \n",
    "            )\n",
    "        ]\n",
    "        timepoints[(taxon,source_id)] = sorted(subtable['Source Time'].unique().tolist())\n",
    "        \n",
    "def sequential_test(row):\n",
    "    if row['Within Patient']:\n",
    "        if (row['Taxon'],row['Source ID']) in timepoints:\n",
    "            sequence = timepoints[(row['Taxon'],row['Source ID'])]\n",
    "            if sequence.index(row['Destination Time'])-sequence.index(row['Source Time']) == 1:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "guttrsnp_distances['Sequential'] = guttrsnp_distances.apply(sequential_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_to_simulation={\n",
    "    'Bacteroides vulgatus ATCC 8482' : 'PVulgatus',\n",
    "    'Enterococcus faecium' : 'EFaecium',\n",
    "    'Lactobacillus gasseri ATCC 33323 = JCM 1131' : 'LGasseri',\n",
    "    'Shigella sonnei 53G' : 'EColi',\n",
    "    \n",
    "}\n",
    "\n",
    "guttrsnp_distances_simulation = pd.read_csv('Input/GutTrSnp/SubspeciesSimulation/aggregatedDistances.csv',dtype={'Taxon' : str}).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Checks / Generic Analysis\n",
    "\n",
    "\n",
    "os.makedirs(output_folder,exist_ok=True)\n",
    "\n",
    "plots = []\n",
    "curation = []\n",
    "for taxon in guttrsnp_distances['Taxon Name'].unique():#['Bacteroides vulgatus ATCC 8482','Flavonifractor plautii','Bacteroides uniformis','Parabacteroides distasonis ATCC 8503','Parabacteroides merdae']:\n",
    "    \n",
    "    taxontable = guttrsnp_distances[\n",
    "        (guttrsnp_distances['Taxon Name'] == taxon)&\n",
    "        (guttrsnp_distances['Sequential'])&\n",
    "        (guttrsnp_distances['Share Of Overlap'] >= 0.5)\n",
    "    ]\n",
    "       \n",
    "    taxontable['PrePost Pair'] = taxontable['PrePost Pair'].map(\n",
    "        {\n",
    "            True : 'Yes',\n",
    "            False : 'No'\n",
    "        }\n",
    "    )\n",
    "   \n",
    "    if len(taxontable) < 3:\n",
    "        continue    \n",
    " \n",
    "    min_dist = taxontable['GutTrSnp Distance'].min()\n",
    "    max_dist = taxontable['GutTrSnp Distance'].max()\n",
    "    step = (max_dist-min_dist)/50\n",
    "    \n",
    "    max_line = alt.Chart(\n",
    "        pd.DataFrame(\n",
    "        [(max_dist)],columns=['Distance']\n",
    "    )\n",
    "    ).mark_rule().encode(\n",
    "        x='Distance',\n",
    "        size=alt.value(3)\n",
    "    )\n",
    "    \n",
    "    if taxon in mapping_to_simulation:\n",
    "        \n",
    "        subdata = guttrsnp_distances_simulation[\n",
    "            (guttrsnp_distances_simulation['Simulated Taxon'] == mapping_to_simulation[taxon])&\n",
    "            (guttrsnp_distances_simulation['Taxon'] == taxontable['Taxon'].values[0])\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        min_dist = min(subdata['GutTrSnp Distance'].min(),min_dist)\n",
    "        max_dist = min(subdata['GutTrSnp Distance'].max(),max_dist)\n",
    "        step = (max_dist-min_dist)/50 \n",
    "             \n",
    "    \n",
    "    plot = alt.Chart(taxontable,height=300,title='Sequential Samples Same Patients').mark_bar().encode(\n",
    "            x=alt.X('GutTrSnp Distance',bin=alt.Bin(extent=[min_dist,max_dist],step=step),title='Distance'),\n",
    "            y=alt.Y('count()',axis=alt.Axis(tickMinStep=1),stack=True),\n",
    "            color=alt.Color('PrePost Pair',title='Across Transplantation')\n",
    "        )\n",
    "    \n",
    "        \n",
    "    plot = (plot |alt.Chart(taxontable,height=300,title='Sequential Samples Same Patients').mark_point().encode(\n",
    "            y=alt.Y('GutTrSnp Distance',title='Distance'),\n",
    "            x='Elapsed Time',\n",
    "            tooltip=['Source','Destination','Share Of Overlap'],\n",
    "            color=alt.Color('PrePost Pair',title='Across Transplantation')\n",
    "        )          \n",
    "    )\n",
    "    \n",
    "    curation.append(\n",
    "        taxontable\n",
    "    )\n",
    "                   \n",
    "                \n",
    "    prepairs = guttrsnp_distances[\n",
    "        (guttrsnp_distances['Taxon Name'] == taxon)&\n",
    "        (guttrsnp_distances['Source ID'] != guttrsnp_distances['Destination ID'])&\n",
    "        (guttrsnp_distances['Source Time'] < 0)&\n",
    "        (guttrsnp_distances['Destination Time'] < 0)\n",
    "    ]\n",
    "    \n",
    "        \n",
    "    plot =  (plot | alt.Chart(\n",
    "        prepairs,height=300,title='Pre-Samples Different Patients'\n",
    "    ).mark_bar().encode(\n",
    "        x=alt.X('GutTrSnp Distance',bin=alt.Bin(extent=[min_dist,max_dist],step=step),title='Distance'),\n",
    "            y=alt.Y('count()',axis=alt.Axis(tickMinStep=1))\n",
    "    ) \n",
    "    )\n",
    "    \n",
    "    all_distances = guttrsnp_distances[\n",
    "        (guttrsnp_distances['Taxon Name'] == taxon)&\n",
    "        (guttrsnp_distances['Share Of Overlap'] >= 0.5)\n",
    "    ]\n",
    "    \n",
    "    plot =  (plot | (alt.Chart(\n",
    "            all_distances,height=300,title='All Distances'\n",
    "        ).mark_bar().encode(\n",
    "            x=alt.X('GutTrSnp Distance',bin=alt.Bin(maxbins=50),title='Distance'),\n",
    "            y=alt.Y('count()',axis=alt.Axis(tickMinStep=1))\n",
    "        ) +max_line)\n",
    "    )\n",
    "\n",
    "    if taxon in mapping_to_simulation:\n",
    "        \n",
    "        subdata = guttrsnp_distances_simulation[\n",
    "            (guttrsnp_distances_simulation['Simulated Taxon'] == mapping_to_simulation[taxon])&\n",
    "            (guttrsnp_distances_simulation['Taxon'] == taxontable['Taxon'].values[0])\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        plot = (plot |(alt.Chart(\n",
    "            subdata[subdata['GutTrSnp Distance'] != 0],height=300,title='Simulated Data'\n",
    "        ).mark_bar().encode(\n",
    "            x=alt.X('GutTrSnp Distance',bin=alt.Bin(maxbins=50),title='Distance'),\n",
    "            y=alt.Y('count()',axis=alt.Axis(tickMinStep=1))\n",
    "        )+max_line))\n",
    "        \n",
    "    plots.append(\n",
    "        plot.properties(title=taxon)\n",
    "    )\n",
    "\n",
    "\n",
    "          \n",
    "chart = reduce(lambda x,y : x& y,plots)\n",
    "\n",
    "chart.save(output_folder+'/Overview.html')\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of manually curated shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = pd.read_excel('Input/Annotations/Patient_Statistics (2).xlsx',dtype={'Pat ID' : str})\n",
    "outcomes_reduced = outcomes[\n",
    "    ['Pat ID',\n",
    "     'Day relative to HSCT',\n",
    "     'Day relative to HSCT.1',\n",
    "     'Day relative to HSCT.2',\n",
    "     'Day relative to HSCT.3',\n",
    "     'Day relative to HSCT.4',\n",
    "     'Day relative to HSCT.5',\n",
    "     'Day relative to HSCT.6',\n",
    "     'Day relative to HSCT.7',\n",
    "    ]\n",
    "         ].rename(\n",
    "    columns={\n",
    "        'Day relative to HSCT' : '1st Relapse',\n",
    "        'Day relative to HSCT.1' : '2nd Relapse',\n",
    "        'Day relative to HSCT.2' : '2nd HSCT',\n",
    "        'Day relative to HSCT.3' : 'Acute GvHD Grade 1-2',\n",
    "        'Day relative to HSCT.4' : 'Acute GvHD Grade 3-4',\n",
    "        'Day relative to HSCT.5' : 'Moderate cGvHD',\n",
    "        'Day relative to HSCT.6' : 'Severe cGvHD',\n",
    "        'Day relative to HSCT.7' : 'Death',\n",
    "    }\n",
    ")\n",
    "outcomes_reduced\n",
    "\n",
    "outcomes_reduced = outcomes_reduced.melt(id_vars=['Pat ID'])\n",
    "outcomes_reduced = outcomes_reduced[(outcomes_reduced['value']==outcomes_reduced['value'])]\n",
    "\n",
    "outcomes_reduced.loc[\n",
    "    (outcomes_reduced['Pat ID']=='18.2'),'KI ID'\n",
    "] = '18.2'\n",
    "outcomes_reduced.loc[(outcomes_reduced['Pat ID']=='18.2'),'value'] -= OFFSET_PAT_18\n",
    "outcomes_reduced = outcomes_reduced[['Pat ID','variable','value']]\n",
    "outcomes_reduced = outcomes_reduced[outcomes_reduced['value'] != '?']\n",
    "\n",
    "outcomes_reduced\n",
    "\n",
    "manual_curation = pd.read_csv('Input/curation_reduced.csv')\n",
    "manual_curation['Distance Name'] = manual_curation['Taxon Name']+':'+manual_curation['Source']+'->'+manual_curation['Destination']\n",
    "manual_curation['Time X'] = manual_curation['Source'].str.split('/',expand=True)[1].astype(int)\n",
    "manual_curation['Time Y'] = manual_curation['Destination'].str.split('/',expand=True)[1].astype(int)\n",
    "manual_curation['Pat ID'] = manual_curation['Source'].str.split('/',expand=True)[0]\n",
    "manual_curation['Any Annotation'] = manual_curation['Clinical Annotation']!='No events tracked'\n",
    "\n",
    "sample_times = (manual_curation.groupby(['Pat ID','Taxon Name'])['Time X'].apply(list)+manual_curation.groupby(['Pat ID','Taxon Name'])['Time Y'].apply(list)\n",
    ").reset_index().explode(0).rename(columns={0:'Time'}).drop_duplicates()\n",
    "\n",
    "charts=[]\n",
    "\n",
    "missing_samples = pd.DataFrame([(x) for x in PAPER_SAMPLES],columns=['Sample'])\n",
    "missing_samples[['Patient','Time']] = missing_samples['Sample'].str.split('/',expand=True)\n",
    "missing_samples\n",
    "\n",
    "for taxon in manual_curation['Taxon Name'].unique():\n",
    "    st = manual_curation[manual_curation['Taxon Name'] == taxon]\n",
    "    orm = outcomes_reduced[outcomes_reduced['Pat ID'].isin(st['Pat ID'].unique())]\n",
    "    \n",
    "    lm = missing_samples[\n",
    "        (missing_samples['Patient'].isin(st['Pat ID'].unique()))&\n",
    "        (~missing_samples['Sample'].isin(st['Source'].unique()))\n",
    "    ]\n",
    "    \n",
    "    if len(lm) != 0:\n",
    "        print(taxon,lm)\n",
    "    \n",
    "    c = (alt.Chart(st).mark_line().encode(\n",
    "        x=alt.X('Time X',title=None,scale=alt.Scale(type='symlog')),\n",
    "        x2=alt.X2('Time Y',title=None),\n",
    "        y=alt.Y('Pat ID',title='Patient'),\n",
    "        color=alt.Color('Manual Curation',scale=alt.Scale(\n",
    "            domain=['Shift','Stable'],range=['Orange','Grey']\n",
    "        ))\n",
    "    )+alt.Chart(orm).mark_point(color='black',size=56).encode(\n",
    "        x=alt.X('value',title='Day'),\n",
    "        y=alt.Y('Pat ID',title='Patient'),\n",
    "        shape=alt.Shape('variable',\n",
    "                       scale=alt.Scale(\n",
    "    domain=['2nd HSCT','Acute GvHD Grade 1-2','Acute GvHD Grade 3-4','Moderate cGvHD','Severe cGvHD','1st Relapse','2nd Relapse','Death'],\n",
    "        range=['circle','square','square','diamond','diamond','triangle','triangle','cross'])\n",
    "                       )\n",
    "    )+alt.Chart(pd.DataFrame([(0)],columns=['x'])).mark_rule().encode(x='x')+alt.Chart(\n",
    "    sample_times[sample_times['Taxon Name'] == taxon]\n",
    "\n",
    "    ).mark_point(color='black',filled=True).encode(\n",
    "        y='Pat ID',\n",
    "        x=alt.X('Time',title=None)\n",
    "    )+alt.Chart(lm).mark_point(color='grey',filled=True).encode(\n",
    "        y='Patient:N',\n",
    "        x=alt.X('Time:Q',title=None)\n",
    "    )).resolve_scale(x='shared')\n",
    "    charts.append(c.properties(title=taxon,width=800))\n",
    "    \n",
    "reduce(lambda x,y : x&y, charts).resolve_scale(x='shared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    len(manual_curation),\n",
    "    len(manual_curation['Taxon Name'].unique()),\n",
    "    len(manual_curation['Pat ID'].unique())\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXIMITY = 0\n",
    "\n",
    "def decide_proximity(row):\n",
    "\n",
    "    st = outcomes_reduced[outcomes_reduced['Pat ID'] == row['Pat ID']]\n",
    "    for idx,orow in st.iterrows():\n",
    "        if (orow['variable'] in ['1st Relapse','2nd Relapse']):\n",
    "            if row['Time X']-PROXIMITY <= orow['value'] and row['Time Y']+PROXIMITY >= orow['value']:\n",
    "                return True\n",
    "\n",
    "    if row['Across TX'] in ['True','Yes']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "manual_curation['Proximity to Relapse/aHSCT'] = manual_curation.apply(\n",
    "    decide_proximity,axis=1\n",
    ")\n",
    "\n",
    "manual_curation.groupby(['Proximity to Relapse/aHSCT','Manual Curation'])['Pat ID'].count().rename('Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_curation.to_csv('manual_cur_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency, fisher_exact\n",
    "\n",
    "ct = pd.crosstab(manual_curation['Proximity to Relapse/aHSCT'],manual_curation['Manual Curation'])\n",
    "c, p, dof, expected = chi2_contingency(ct)\n",
    "print(p)\n",
    "oddsratio,p = fisher_exact(ct)\n",
    "print(oddsratio,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = 'G11_-100'\n",
    "TAXON = PLANTAE\n",
    "EXCLUDE = []\n",
    "\n",
    "subtable = validation_data[validation_data['Sample'] == SAMPLE]\n",
    "subtable = subtable[subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,TAXON))]\n",
    "for ftaxon in EXCLUDE:\n",
    "    subtable = subtable[~subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,ftaxon))]\n",
    "subtable['Validated'] = (subtable['Validation Rate'].astype(float) >= 0.2)\n",
    "weighted_validation_rate= (\n",
    "    subtable['Validated']*subtable['Reads']\n",
    ").sum()/subtable['Reads'].sum()\n",
    "print('Weighted Validation Rate: {} (>= 80% validated)'.format(weighted_validation_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(row):\n",
    "    \n",
    "    \n",
    "    validation_map = {\n",
    "        'Viruses' : (VIRUSES,[]),\n",
    "        'OtherEukaryota':(EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]),\n",
    "        'Fungi':(FUNGI,[]),\n",
    "        'Bacteria':(BACTERIA,[]),\n",
    "        'Plants':(PLANTAE,[]),\n",
    "        'Archaea':(ARCHAEA,[]),\n",
    "        'Mikrobiome' : ('1',[CHORDATA,PLANTAE]),\n",
    "        'Human' : (CHORDATA,[])\n",
    "\n",
    "    }\n",
    "\n",
    "        \n",
    "    SAMPLE = row['PatID']+'_'+str(row['time'])\n",
    "    \n",
    "    if row['domain'] == 'Unclassified':\n",
    "        return 'Cannot be validated'\n",
    "    \n",
    "    TAXON,EXCLUDE = validation_map[row['domain']]\n",
    "\n",
    "    \n",
    "    subtable = validation_data[validation_data['Sample'] == SAMPLE]\n",
    "    subtable = subtable[subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,TAXON))]\n",
    "    for ftaxon in EXCLUDE:\n",
    "        subtable = subtable[~subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,ftaxon))]\n",
    "    subtable['Validated'] = (subtable['Validation Rate'].astype(float) >= 0.2)\n",
    "    #print(subtable)\n",
    "    weighted_validation_rate= (\n",
    "        subtable['Validated']*subtable['Reads']\n",
    "    ).sum()/subtable['Reads'].sum()\n",
    "    #print('Gewichtete Validierungsrate: {} (>= 80% validiert)'.format(weighted_validation_rate))    \n",
    "    return weighted_validation_rate >= 0.8\n",
    "\n",
    "outlier['validated'] = outlier.apply(validate,axis=1)\n",
    "outlier.to_csv('Output/OutlierAndValidation.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Rate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('Output/ValidationRatePlots',exist_ok=True)\n",
    "\n",
    "GROUPS = [\n",
    "    (VIRUSES,[]),\n",
    "    (EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]),\n",
    "    (FUNGI,[]),\n",
    "    (BACTERIA,[]),\n",
    "    (PLANTAE,[]),\n",
    "    (ARCHAEA,[]),\n",
    "    (CHORDATA,[]),\n",
    "    (PLANTAE,[]),\n",
    "]\n",
    "\n",
    "for group in GROUPS:\n",
    "    taxon,excludes = group\n",
    "    subtable = validation_data[validation_data['Taxon ID'].apply(lambda x : is_below_or_equal(x,taxon))]\n",
    "    for ftaxon in excludes:\n",
    "        subtable = subtable[~subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,ftaxon))]\n",
    "    topxtaxa = subtable.groupby('Taxon Name')['Reads'].sum().sort_values()[-30:].keys()\n",
    "    subtable = subtable[subtable['Taxon Name'].isin(topxtaxa)]\n",
    "    subtable['Validated'] = (subtable['Validation Rate'].astype(float) >= 0.2)\n",
    "    subtable['Validated Reads'] = subtable['Validated'] * subtable['Reads']\n",
    "\n",
    "    charts = []\n",
    "    charts.append(\n",
    "        alt.Chart(\n",
    "            subtable\n",
    "        ).mark_boxplot().encode(\n",
    "            x=alt.X('Taxon Name',sort=subtable.groupby('Taxon Name')['Reads'].sum().sort_values().keys().tolist()[::-1]),\n",
    "            y=alt.Y('Validation Rate')\n",
    "        )\n",
    "    )\n",
    "    for genus in topxtaxa[::-1]:\n",
    "        charts.append(\n",
    "            alt.Chart(\n",
    "                subtable[subtable['Taxon Name']==genus],title=genus\n",
    "            ).mark_point().encode(\n",
    "                x=alt.X('Fraction Estimate',scale=alt.Scale(type='log')),\n",
    "                y=alt.Y('Validation Rate',scale=alt.Scale(domain=[0,1])),\n",
    "                tooltip=['Sample','Validation Rate']\n",
    "            )\n",
    "        )\n",
    "\n",
    "    combined_title = idtonames[taxon]\n",
    "    if excludes != []:\n",
    "        combined_title += ' without ' + ','.join(idtonames[x] for x in excludes)\n",
    "        \n",
    "\n",
    "    reduce(lambda x,y : x&y, charts).properties(title=combined_title).save('Output/ValidationRatePlots/{}.html'.format(combined_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data[validation_data['Taxon Name'] == 'Methanobrevibacter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('Output/ValidationRatePlots',exist_ok=True)\n",
    "\n",
    "GROUPS = [\n",
    "    (VIRUSES,[]),\n",
    "    (EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]),\n",
    "    (FUNGI,[]),\n",
    "    (BACTERIA,[]),\n",
    "    (PLANTAE,[]),\n",
    "    (ARCHAEA,[]),\n",
    "    (CHORDATA,[]),\n",
    "    (PLANTAE,[])\n",
    "]\n",
    "\n",
    "PAPER_SAMPLES_UNDERSCORE = [x.replace('/','_') for x in PAPER_SAMPLES]\n",
    "\n",
    "print('Continuous')\n",
    "for group in GROUPS:\n",
    "    taxon,excludes = group\n",
    "    subtable = validation_data[validation_data['Taxon ID'].apply(lambda x : is_below_or_equal(x,taxon))]\n",
    "    #reduce to paper samples only\n",
    "    subtable = subtable[subtable['Sample'].isin(PAPER_SAMPLES_UNDERSCORE)]\n",
    "    for ftaxon in excludes:\n",
    "        subtable = subtable[~subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,ftaxon))]\n",
    "    subtable['Validated Reads'] = subtable['Validation Rate'] * subtable['Reads']\n",
    "\n",
    "    print(idtonames[group[0]],\n",
    "        (subtable.groupby('Sample')['Validated Reads'].sum()/subtable.groupby('Sample')['Reads'].sum()).median()\n",
    "         )\n",
    "print('Binary') \n",
    "for group in GROUPS:\n",
    "    taxon,excludes = group\n",
    "    subtable = validation_data[validation_data['Taxon ID'].apply(lambda x : is_below_or_equal(x,taxon))]\n",
    "    #reduce to paper samples only\n",
    "    subtable = subtable[subtable['Sample'].isin(PAPER_SAMPLES_UNDERSCORE)]\n",
    "    for ftaxon in excludes:\n",
    "        subtable = subtable[~subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,ftaxon))]\n",
    "    subtable['Validated Reads'] = (subtable['Validation Rate']>=0.2) * subtable['Reads']\n",
    "\n",
    "    print(idtonames[group[0]],\n",
    "        (subtable.groupby('Sample')['Validated Reads'].sum()/subtable.groupby('Sample')['Reads'].sum()).median()\n",
    "         )    \n",
    "subtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Fungal Eukaryota Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_table = get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    level='G',\n",
    "    samples=PAPER_SAMPLES,\n",
    "    included_taxa_filter=None,\n",
    "    excluded_taxa_filter=None,\n",
    "    normalize=False\n",
    ")\n",
    "input_table['Read Fraction'] = input_table['readcount'] / input_table.groupby(['time','patientid'])['readcount'].transform('sum')\n",
    "\n",
    "taxon,excludes = EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]\n",
    "subtable = validation_data[validation_data['Taxon ID'].apply(lambda x : is_below_or_equal(x,taxon))]\n",
    "#reduce to paper samples only\n",
    "subtable = subtable[subtable['Sample'].isin(PAPER_SAMPLES_UNDERSCORE)]\n",
    "for ftaxon in excludes:\n",
    "    subtable = subtable[~subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,ftaxon))]\n",
    "subtable['Validated Reads'] = (subtable['Validation Rate']>=0.2) * subtable['Reads']\n",
    "subtable = (subtable[subtable['Validated Reads'] > 0])\n",
    "subtable = pd.merge(\n",
    "    subtable[['patientid','time','Validation Rate','Taxon ID']],\n",
    "    input_table,\n",
    "    left_on=['patientid','time','Taxon ID'],\n",
    "    right_on=['patientid','time','taxonid'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtable_sub20 = subtable[subtable['readcount'] >= 20]\n",
    "subtable_sub20['sample'].unique()\n",
    "#subtable_sub20['taxon'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(subtable['sample'].unique()))\n",
    "print(subtable['taxon'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Plots Per Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUPS = {\n",
    "    'Bacteria':(BACTERIA,[]),\n",
    "    'Fungi':(FUNGI,[]),\n",
    "    'OtherEukaryota':(EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]),\n",
    "    'Viruses':(VIRUSES,[]),\n",
    "    'Archaea':(ARCHAEA,[]),\n",
    "    'Homo':(HOMO,[]),\n",
    "    'Plants':(PLANTAE,[]),\n",
    "}\n",
    "\n",
    "Gruppen = list(GROUPS.keys())\n",
    "\n",
    "\n",
    "x=alt.X(\"domain:O\", title=None, axis=alt.Axis(labels=False, ticks=False), scale=alt.Scale(paddingInner=1), sort=Gruppen),    \n",
    "y=alt.Y(\"value:Q\",title='Fraction [%]',scale=alt.Scale(type='symlog',domain=[0,100]), axis=alt.Axis(grid=False,minExtent=40, values=[0.1,0.5,1,5,10,20,50,100])), \n",
    "\n",
    "\n",
    "tables = []\n",
    "sample_statistics['Sample'] = sample_statistics['PatID']+'_'+sample_statistics['time'].astype(str)\n",
    "\n",
    "for groupname,group in GROUPS.items():\n",
    "    taxon,excludes = group\n",
    "    subtable = validation_data[validation_data['Taxon ID'].apply(lambda x : is_below_or_equal(x,taxon))]\n",
    "    for ftaxon in excludes:\n",
    "        subtable = subtable[~subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,ftaxon))]\n",
    "    \n",
    "\n",
    "    \n",
    "    subtable['Validated'] = (subtable['Validation Rate'].astype(float) >= 0.2)\n",
    "    subtable['Validated Reads Binary'] = subtable['Validated'] * subtable['Reads']\n",
    "    subtable['Validated Reads Continuous'] = subtable['Validation Rate'] * subtable['Reads']\n",
    "    \n",
    "    subtable = subtable.groupby('Sample',as_index=False)[['Validated Reads Binary','Validated Reads Continuous','Reads']].sum()\n",
    "\n",
    "    subtable['Validated Read Fraction Binary'] = subtable['Validated Reads Binary'] / subtable['Reads']\n",
    "    subtable['Validated Read Fraction Continuous'] = subtable['Validated Reads Continuous'] / subtable['Reads']\n",
    "\n",
    "        \n",
    "    subtable['Domain'] = groupname\n",
    "\n",
    "    tables.append(subtable)\n",
    "    \n",
    "combined = pd.merge(\n",
    "    pd.concat(tables),\n",
    "    sample_statistics[['Sample','leukocytephase_cluster_2_kurz']],\n",
    "    on=['Sample'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "#filter null group\n",
    "combined = combined[combined['leukocytephase_cluster_2_kurz']==combined['leukocytephase_cluster_2_kurz']]\n",
    "\n",
    "\n",
    "(alt.Chart(combined,height=400,title='Binary').mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    x=alt.X('Domain',sort=list(GROUPS.keys())),\n",
    "    column=alt.Column(\n",
    "        'leukocytephase_cluster_2_kurz:O',\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None,\n",
    "    ),    y=alt.Y('Validated Read Fraction Binary',scale=alt.Scale(type='linear',domain=[0,1.1])),\n",
    "        color=alt.Color(\n",
    "        \"Domain:N\",\n",
    "        sort=Gruppen,\n",
    "        legend=alt.Legend(title=None,orient='top'),\n",
    "        scale=alt.Scale(domain=list(GROUPS.keys()),range=[ '#EE6677','#fecc5c','#228833', '#66CCEE', '#AA3377','lightgrey', 'darkgrey'])\n",
    "    )\n",
    ")&alt.Chart(combined,height=400,title='Continuous').mark_boxplot(ticks=True,median={'color':'black'}).encode(\n",
    "    x=alt.X('Domain',sort=list(GROUPS.keys())),\n",
    "    column=alt.Column(\n",
    "        'leukocytephase_cluster_2_kurz:O',\n",
    "        spacing=0,\n",
    "        sort=['Healthy', 'Pre TX','Leukozytopenia','Reconstitution'],\n",
    "        title=None\n",
    "    ),    y=alt.Y('Validated Read Fraction Continuous',scale=alt.Scale(type='linear',domain=[0,1.1])),\n",
    "    color=alt.Color(\n",
    "        \"Domain:N\",\n",
    "        sort=Gruppen,\n",
    "        legend=alt.Legend(title=None,orient='top'),\n",
    "        scale=alt.Scale(domain=list(GROUPS.keys()),range=[ '#EE6677','#fecc5c','#228833', '#66CCEE', '#AA3377','lightgrey', 'darkgrey'])\n",
    "    )\n",
    ")).save('Output/ValidatedReadsPerDomainOverview.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Validated Taxa Per Domain Stratified By PCoA Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_X = 20\n",
    "\n",
    "GROUPS = [\n",
    "    (VIRUSES,[]),\n",
    "    (EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]),\n",
    "    (FUNGI,[]),\n",
    "    (BACTERIA,[]),\n",
    "    (PLANTAE,[]),\n",
    "    (ARCHAEA,[]),\n",
    "    (CHORDATA,[]),\n",
    "    (PLANTAE,[]),\n",
    "    ('1',[CHORDATA,PLANTAE])\n",
    "]\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "os.makedirs('top_validated_taxa',exist_ok=True)\n",
    "\n",
    "tables = []\n",
    "\n",
    "for group in GROUPS:\n",
    "    taxon,excludes = group\n",
    "    subtable = validation_data[validation_data['Taxon ID'].apply(lambda x : is_not_below(x,taxon))]\n",
    "    for ftaxon in excludes:\n",
    "        subtable = subtable[~subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,ftaxon))]\n",
    "    subtable['Validated'] = (subtable['Validation Rate'].astype(float) >= 0.2)\n",
    "    subtable['Validated Reads Binary'] = subtable['Validated'] * subtable['Reads']\n",
    "    subtable['Validated Reads Continuous'] = subtable['Validation Rate'] * subtable['Reads']\n",
    "\n",
    "    subtable = subtable.groupby(['Sample','Taxon ID'],as_index=False)[['Validated Reads Binary','Validated Reads Continuous','Reads']].sum()\n",
    "\n",
    "    subtable['Validated Read Fraction Binary'] = subtable['Validated Reads Binary'] / subtable['Reads']\n",
    "    subtable['Validated Read Fraction Continuous'] = subtable['Validated Reads Continuous'] / subtable['Reads']\n",
    "    \n",
    "\n",
    "    combined_title = idtonames[taxon]\n",
    "    if excludes != []:\n",
    "        combined_title += ' without ' + ','.join(idtonames[x] for x in excludes)\n",
    "        \n",
    "    subtable['Domain'] = combined_title\n",
    "\n",
    "    tables.append(subtable)\n",
    "    \n",
    "combined = pd.concat(tables)\n",
    "\n",
    "sample_statistics['Sample'] = sample_statistics['PatID']+'_'+sample_statistics['time'].astype(str)\n",
    "combined = pd.merge(\n",
    "    combined,\n",
    "    sample_statistics[['Sample','leukocytephase_cluster_kurz']],\n",
    "    on=['Sample'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "for domain in combined['Domain'].unique():\n",
    "    subtable = combined[combined['Domain'] == domain]\n",
    "    subtable['Taxon Name'] = subtable['Taxon ID'].map(idtonames)\n",
    "    \n",
    "    taxa_we_look_at = list(subtable.groupby('Taxon Name')['Validated Read Fraction Binary'].sum().sort_values(ascending=False)[:(TOP_X+1)].keys())\n",
    "\n",
    "    taxatable = subtable[subtable['Taxon Name'].isin(taxa_we_look_at)]\n",
    "    c1=alt.Chart(taxatable,title='Binary').mark_rect().encode(\n",
    "        x='Taxon Name',\n",
    "        y='leukocytephase_cluster_kurz',\n",
    "        color='mean(Validated Read Fraction Binary)'\n",
    "    )\n",
    "\n",
    "    taxa_we_look_at = list(subtable.groupby('Taxon Name')['Validated Read Fraction Continuous'].sum().sort_values(ascending=False)[:(TOP_X+1)].keys())\n",
    "\n",
    "    taxatable = subtable[subtable['Taxon Name'].isin(taxa_we_look_at)]\n",
    "    c2=alt.Chart(taxatable,title='Continuous').mark_rect().encode(\n",
    "        x='Taxon Name',\n",
    "        y='leukocytephase_cluster_kurz',\n",
    "        color='mean(Validated Read Fraction Continuous)'\n",
    "    )\n",
    "    \n",
    "    (c1|c2).resolve_scale(color='independent').save('top_validated_taxa/{}_top_{}.png'.format(domain,TOP_X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Top Genera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GROUPS = [\n",
    "    (BACTERIA,[]),\n",
    "    (FUNGI,[]),\n",
    "    (EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]),\n",
    "    (VIRUSES,[]),\n",
    "    (ARCHAEA,[]),\n",
    "    (PLANTAE,[]),\n",
    "    (CHORDATA,[]),\n",
    "    (PLANTAE,[]),\n",
    "    ('1',[CHORDATA,FUNGI,PLANTAE])\n",
    "]\n",
    "\n",
    "PAPER_SAMPLES_UNDERSCORE = [x.replace('/','_') for x in PAPER_SAMPLES]\n",
    "\n",
    "os.makedirs('top_validated_taxa',exist_ok=True)\n",
    "sample_statistics['Sample'] = sample_statistics['PatID']+'_'+sample_statistics['time'].astype(str)\n",
    "\n",
    "tables = []\n",
    "\n",
    "total_reads = validation_data.groupby('Sample')['Reads'].sum()\n",
    "all_charts = []\n",
    "\n",
    "\n",
    "for group in GROUPS:\n",
    "    taxon,excludes = group\n",
    "    subtable = validation_data[validation_data['Taxon ID'].apply(lambda x : is_below_or_equal(x,taxon))]\n",
    "    for ftaxon in excludes:\n",
    "        subtable = subtable[~subtable['Taxon ID'].apply(lambda x : is_below_or_equal(x,ftaxon))]\n",
    "    subtable['Validated'] = (subtable['Validation Rate'].astype(float) >= 0.2)\n",
    "    subtable['Validated Reads Binary'] = subtable['Validated'] * subtable['Reads']\n",
    "    #subtable['Validated Reads Continuous'] = subtable['Validation Rate'] * subtable['Reads']\n",
    "\n",
    "    subtable = subtable.groupby(['Sample','Taxon ID'],as_index=False)[['Validated Reads Binary',\n",
    "                                                                       #'Validated Reads Continuous',\n",
    "                                                                       'Reads','Validated']].sum()\n",
    "\n",
    "    local_reads = subtable.groupby('Sample')['Reads'].sum()\n",
    "    \n",
    "    subtable['Validated Local Read Fraction Binary'] = subtable.apply(\n",
    "        lambda x:x['Validated Reads Binary']/local_reads[x['Sample']],axis=1\n",
    "    )\n",
    "    #subtable['Validated Local Read Fraction Continuous'] = subtable['Validated Reads Continuous'] / subtable['Reads']\n",
    "    \n",
    "    subtable['Validated Global Read Fraction Binary'] = subtable.apply(\n",
    "        lambda x:x['Validated Reads Binary']/total_reads[x['Sample']],axis=1\n",
    "    )\n",
    "    #subtable['Validated Global Read Fraction Continuous'] = subtable.apply(\n",
    "    #    lambda x:x['Validated Reads Continuous']/total_reads[x['Sample']],axis=1\n",
    "    #)        \n",
    "    \n",
    "    \n",
    "    combined_title = idtonames[taxon]\n",
    "    if excludes != []:\n",
    "        combined_title += ' without ' + ','.join(idtonames[x] for x in excludes)\n",
    "    \n",
    "    subtable['Validated Above 5% Binary Local'] = subtable['Validated Local Read Fraction Binary'] > 0.05\n",
    "    subtable['Validated Above 5% Binary Global'] = subtable['Validated Global Read Fraction Binary'] > 0.05\n",
    "    #subtable['Validated Above 5% Continuous Local'] = subtable['Validated Local Read Fraction Continuous'] > 0.05\n",
    "    #subtable['Validated Above 5% Continuous Global'] = subtable['Validated Global Read Fraction Continuous'] > 0.05\n",
    "    subtable = pd.merge(\n",
    "        subtable,\n",
    "        sample_statistics[['Sample','leukocytephase_cluster_2_kurz','Startcluster']],\n",
    "        on=['Sample'],\n",
    "        how='left'\n",
    "    )\n",
    "    subtable['Startcluster'] = subtable['Startcluster'].replace({'Gesund':'Control'})\n",
    "    subtable = subtable.rename(columns={'Validated' : 'Detected'})\n",
    "    \n",
    "    totals =subtable.groupby(['Taxon ID'])[[\n",
    "    'Detected',\n",
    "    ]].sum().rename(columns={'Detected' : 'Absolute Detected'})\n",
    "\n",
    "    short =(subtable.groupby(['Taxon ID'])[[\n",
    "    'Detected',\n",
    "    'Validated Above 5% Binary Local',\n",
    "    'Validated Above 5% Binary Global',\n",
    "    #'Validated Above 5% Continuous Local',\n",
    "    #'Validated Above 5% Continuous Global',\n",
    "    ]].sum()/len(validation_data['Sample'].unique())).add_prefix('Total ').fillna(0)\n",
    "\n",
    "    short = pd.concat([totals,short],axis=1)\n",
    "\n",
    "    long = subtable.groupby(['leukocytephase_cluster_2_kurz','Taxon ID'])[[\n",
    "    'Detected',\n",
    "    'Validated Above 5% Binary Local',\n",
    "    'Validated Above 5% Binary Global',\n",
    "    #'Validated Above 5% Continuous Local',\n",
    "    #'Validated Above 5% Continuous Global'\n",
    "    ]].sum().reset_index()\n",
    "\n",
    "    samples_per_group = sample_statistics.groupby(['leukocytephase_cluster_2_kurz'])[\n",
    "    'Sample'\n",
    "    ].count().reset_index()\n",
    "\n",
    "    long = pd.merge(long,samples_per_group,how='left',on='leukocytephase_cluster_2_kurz')\n",
    "\n",
    "    long['Detected'] = long['Detected'] / long['Sample']\n",
    "    long['Validated Above 5% Binary Local'] = long['Validated Above 5% Binary Local'] / long['Sample']\n",
    "    long['Validated Above 5% Binary Global'] = long['Validated Above 5% Binary Global'] / long['Sample']\n",
    "    #long['Validated Above 5% Continuous Local'] = long['Validated Above 5% Continuous Local'] / long['Sample']\n",
    "    #long['Validated Above 5% Continuous Global'] = long['Validated Above 5% Continuous Global'] / long['Sample']\n",
    "    long = long.drop(columns='Sample')\n",
    "\n",
    "    long = long.pivot(index='Taxon ID',columns='leukocytephase_cluster_2_kurz').fillna(0).swaplevel(0,1,1)\n",
    "\n",
    "    \n",
    "    long_2 = subtable.groupby(['Startcluster','Taxon ID'])[[\n",
    "    'Detected',\n",
    "    'Validated Above 5% Binary Local',\n",
    "    'Validated Above 5% Binary Global',\n",
    "    #'Validated Above 5% Continuous Local',\n",
    "    #'Validated Above 5% Continuous Global'\n",
    "    ]].sum().reset_index()\n",
    "\n",
    "    samples_per_group = sample_statistics.groupby(['Startcluster'])[\n",
    "    'Sample'\n",
    "    ].count().reset_index()\n",
    "\n",
    "    long_2 = pd.merge(long_2,samples_per_group,how='left',on='Startcluster')\n",
    "\n",
    "    long_2['Detected'] = long_2['Detected'] / long_2['Sample']\n",
    "    long_2['Validated Above 5% Binary Local'] = long_2['Validated Above 5% Binary Local'] / long_2['Sample']\n",
    "    long_2['Validated Above 5% Binary Global'] = long_2['Validated Above 5% Binary Global'] / long_2['Sample']\n",
    "    #long['Validated Above 5% Continuous Local'] = long['Validated Above 5% Continuous Local'] / long['Sample']\n",
    "    #long['Validated Above 5% Continuous Global'] = long['Validated Above 5% Continuous Global'] / long['Sample']\n",
    "    long_2 = long_2.drop(columns='Sample')\n",
    "\n",
    "    long_2 = long_2.pivot(index='Taxon ID',columns='Startcluster').fillna(0).swaplevel(0,1,1)\n",
    "\n",
    "    long=pd.merge(long,long_2,left_index=True,right_index=True,how='outer')\n",
    "        \n",
    "    combined = pd.merge(\n",
    "        short,long,left_index=True,right_index=True,how='outer'\n",
    "    ).sort_values(\n",
    "        by='Total Detected',ascending=False\n",
    "    )\n",
    "    combined = combined[[                    'Absolute Detected',\n",
    "                                                 'Total Detected',\n",
    "                          'Total Validated Above 5% Binary Local',\n",
    "                         'Total Validated Above 5% Binary Global',\n",
    "                                          ('Healthy', 'Detected'),\n",
    "                                        ('Healthy', 'Validated Above 5% Binary Local'),\n",
    "                  ('Healthy', 'Validated Above 5% Binary Global'),\n",
    "                                           ('Pre TX', 'Detected'),\n",
    "                    ('Pre TX', 'Validated Above 5% Binary Local'),\n",
    "                   ('Pre TX', 'Validated Above 5% Binary Global'),\n",
    "\n",
    "                                   ('Leukozytopenia', 'Detected'),\n",
    "                                 ('Leukozytopenia', 'Validated Above 5% Binary Local'),\n",
    "           ('Leukozytopenia', 'Validated Above 5% Binary Global'),\n",
    "\n",
    "                                   ('Reconstitution', 'Detected'),\n",
    "            ('Reconstitution', 'Validated Above 5% Binary Local'),\n",
    "           ('Reconstitution', 'Validated Above 5% Binary Global'),\n",
    "                                                  (1, 'Detected'),\n",
    "\n",
    "                           (1, 'Validated Above 5% Binary Local'),\n",
    "                                               (1, 'Validated Above 5% Binary Global'),\n",
    "\n",
    "                                                                       (2, 'Detected'),\n",
    "                                                (2, 'Validated Above 5% Binary Local'),\n",
    "                          (2, 'Validated Above 5% Binary Global'),\n",
    "\n",
    "                                                  (3, 'Detected'),\n",
    "                           (3, 'Validated Above 5% Binary Local'),\n",
    "                          (3, 'Validated Above 5% Binary Global')]\n",
    "    ]\n",
    "    for column in combined.columns:\n",
    "        if column != 'Absolute Detected':\n",
    "            combined[column] = combined[column].astype(float).map(\"{:.2%}\".format)\n",
    "    combined = combined[combined['Absolute Detected'] > 0]\n",
    "            \n",
    "    excel = combined[:]\n",
    "    excel.index = excel.index.map(idtonames)\n",
    "    excel.to_excel('top_validated_taxa/{}.xlsx'.format(combined_title))\n",
    "    \n",
    "    #Chart\n",
    "    \n",
    "    marker_data = validation_data\n",
    "    marker_data = marker_data[marker_data['Sample'].isin(PAPER_SAMPLES_UNDERSCORE)]\n",
    "    marker_data['Validated'] = marker_data['Validation Rate'] >= 0.2\n",
    "\n",
    "    os.makedirs('marker_genera_overview',exist_ok=True)\n",
    "\n",
    "\n",
    "    def SORTFUNCTION(x):\n",
    "        return (\n",
    "            x[0] == 'G', #Sort by G or regular patient first\n",
    "            float(x.split('G')[-1])\n",
    "        )\n",
    "\n",
    "    patientlist_sorted = sorted(\n",
    "        marker_data['patientid'].unique(),key=SORTFUNCTION\n",
    "    )\n",
    "\n",
    "\n",
    "    tables = []\n",
    "\n",
    "    for taxon in combined.index[:10]:\n",
    "        \n",
    "        subtable = marker_data[\n",
    "            marker_data['Taxon ID'] == taxon\n",
    "        ]\n",
    "\n",
    "\n",
    "        substitutes = []\n",
    "        for sample in PAPER_SAMPLES_UNDERSCORE:\n",
    "            if sample not in subtable['Sample'].unique():\n",
    "                patientid,time = sample.rsplit('_',1)\n",
    "                substitutes.append(\n",
    "                    (patientid,int(time),'?')\n",
    "                )\n",
    "        subtable = pd.concat([subtable,pd.DataFrame(substitutes,columns=['patientid','time','Validated'])]) \n",
    "\n",
    "\n",
    "        occurences = subtable[subtable['Validated']==True].groupby('patientid')['Validated'].count()\n",
    "        interesting_patients = occurences[occurences >= 1].keys()\n",
    "        subtable = subtable[subtable['patientid'].isin(interesting_patients)]\n",
    "\n",
    "        subtable['Validated'] = subtable['Validated'].map(\n",
    "            {\n",
    "                '?' : 'Not Validated',\n",
    "                False : 'Not Validated',\n",
    "                True : 'Validated'\n",
    "            }\n",
    "        )\n",
    "\n",
    "        subtable['Genus'] = idtonames[taxon] if taxon in idtonames else str(taxon) \n",
    "\n",
    "        #Filter patients without any presence\n",
    "\n",
    "        tables.append(subtable)\n",
    "    charttable = pd.concat(tables)\n",
    "    charts=[]\n",
    "    for genus in list(combined.index[:10]):\n",
    "        genustable = charttable[charttable['Taxon ID'] == genus]\n",
    "        if len(genustable) == 0:\n",
    "            continue\n",
    "        chart = (alt.Chart(genustable[~genustable['patientid'].str.startswith('G')],title=idtonames[genus]).mark_point().encode(\n",
    "            x=alt.X('time',scale=alt.Scale(type='symlog'),axis=alt.Axis(grid=False),title='Day'),\n",
    "                color=alt.Color(\n",
    "                    'Validated',scale=alt.Scale(domain=['Not Validated','Validated'],range=['lightgrey','orange'])),\n",
    "            y=alt.Y('patientid:N',sort=patientlist_sorted,axis=alt.Axis(grid=True),title=None)\n",
    "            )|\n",
    "\n",
    "            alt.Chart(genustable[genustable['patientid'].str.startswith('G')]).mark_point().encode(            color=alt.Color(\n",
    "                    'Validated',scale=alt.Scale(domain=['Not Validated','Validated'],range=['lightgrey','orange'])),\n",
    "            y=alt.Y('patientid:N',sort=patientlist_sorted,title=None)\n",
    "            )).resolve_scale(y='independent')   \n",
    "        \n",
    "        charts.append(chart)\n",
    "        if group in [\n",
    "            (BACTERIA,[]),\n",
    "            (FUNGI,[]),\n",
    "            (EUKARYOTA,[CHORDATA,FUNGI,PLANTAE]),\n",
    "            (VIRUSES,[]),\n",
    "            (ARCHAEA,[]),\n",
    "            (PLANTAE,[]),\n",
    "            (CHORDATA,[]),\n",
    "            (PLANTAE,[])\n",
    "        ]:\n",
    "            all_charts.append((chart,len(genustable['patientid'].unique())))\n",
    "    reduce(lambda x,y : x&y,charts).save('top_validated_taxa/{}.svg'.format(combined_title))\n",
    "    reduce(lambda x,y : x&y,charts).save('top_validated_taxa/{}.png'.format(combined_title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENT_MAX = 200\n",
    "FIXED_SPACE_PER_PLOT = 10\n",
    "\n",
    "cur = 0\n",
    "idx = 0\n",
    "\n",
    "column_charts = []\n",
    "column_chart = []\n",
    "\n",
    "while(idx < len(all_charts)):\n",
    "    chart,patients = all_charts[idx]\n",
    "    \n",
    "    column_chart.append(chart)\n",
    "    cur += patients+FIXED_SPACE_PER_PLOT\n",
    "    idx += 1\n",
    "    if cur >= PATIENT_MAX:\n",
    "        cur = 0\n",
    "        column_charts.append(\n",
    "            reduce(lambda x,y : x&y ,column_chart)\n",
    "        )\n",
    "        column_chart = []\n",
    "reduce(lambda x,y : x|y, column_charts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlation val rate abundance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_table = get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    level='G',\n",
    "    samples=PAPER_SAMPLES,\n",
    "    included_taxa_filter=None,\n",
    "    excluded_taxa_filter=None,\n",
    "    normalize=False\n",
    ")\n",
    "\n",
    "input_table['Read Fraction (%)'] = input_table['readcount'] / input_table.groupby(['time','patientid'])['readcount'].transform('sum')\n",
    "\n",
    "input_table['Sample'] = input_table['patientid']+'_'+input_table['time'].astype(str)\n",
    "\n",
    "\n",
    "DISCARD_CUTOFF = 20\n",
    "\n",
    "with_validation = pd.merge(\n",
    "    input_table,\n",
    "    validation_data[['Sample','Taxon ID','Validation Rate']],\n",
    "    left_on=['Sample','taxonid'],\n",
    "    right_on=['Sample','Taxon ID'],\n",
    "    how='left'\n",
    ").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_validation['Read Fraction Bin'] = pd.cut(with_validation['Read Fraction (%)'],[x*0.05 for x in range(round(1/0.05))]+[1]).astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.disable_max_rows()\n",
    "alt.Chart(\n",
    "    with_validation.dropna(),width=500\n",
    ").mark_boxplot(ticks=True).encode(\n",
    "    x=alt.X('Read Fraction Bin:O'),\n",
    "    y=alt.Y('Validation Rate')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_statistics['Sample'] = sample_statistics['PatID']+'_'+sample_statistics['time'].astype(str)\n",
    "\n",
    "t3 = pd.merge(\n",
    "    with_validation[with_validation['taxon'] == 'Candida'],\n",
    "    sample_statistics[['Sample','leukocytephase_cluster_2_kurz']],\n",
    "    how='left',\n",
    "    on='Sample'\n",
    ")\n",
    "\n",
    "t3['Validated'] = (t3['Validation Rate']>= 0.2)\n",
    "\n",
    "t3.groupby(['leukocytephase_cluster_2_kurz'])['Validated'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3.groupby(['leukocytephase_cluster_2_kurz'])['Validated'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation to clinical outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCARD_CUTOFF = 20\n",
    "\n",
    "table = get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    samples=set(PAPER_SAMPLES)-set(DUPLICATES),\n",
    "    level='G',\n",
    "    normalize=False,\n",
    "    excluded_taxa_filter=[CHORDATA])\n",
    "\n",
    "table['Sample'] = table['patientid']+'_'+table['time'].astype(str)\n",
    "\n",
    "table = pd.merge(\n",
    "    table,\n",
    "    validation_data[['Sample','Taxon ID','Validation Rate']],\n",
    "    left_on=['Sample','taxonid'],\n",
    "    right_on=['Sample','Taxon ID'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "#Phase 1: Kick out low abundance groups, assign to \"Not enough reads\"\n",
    "table.loc[table['readcount']<DISCARD_CUTOFF,'taxon'] = 'Not enough reads'\n",
    "\n",
    "#Phase 2: Check for rest if validates, if not assign to \"Not validated\"\n",
    "table['Validated'] = (table['Validation Rate'] > 0.2)\n",
    "table.loc[(table['Validated']!=True)&~(table['taxon'] == 'Not enough reads'), 'taxon'] = 'Not validated'\n",
    "#Readjust sum (group multiple \"Not enough reads\" entries together)\n",
    "table = table.groupby(['taxon','time','patientid'],as_index=False).sum()\n",
    "table['Read Fraction (%)'] = table['readcount']*100 / table.groupby(['time','patientid'])['readcount'].transform('sum')\n",
    "table['Sample'] = table['patientid']+'/'+table['time'].astype(str)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bac = input_table[input_table['Taxon'].isin(['Phocaeicola','Bacteroides'])]\n",
    "bac['Healthy'] = bac['Sample ID'].str.startswith('G').map(\n",
    "    {True : 'Control',\n",
    "    False : 'Lifelines'}\n",
    ")\n",
    "bac = bac.groupby(['Healthy','Sample ID'])['Read Fraction (%)'].sum().reset_index()\n",
    "alt.Chart(bac).mark_boxplot().encode(\n",
    "    x=alt.X('Healthy',sort=['Control','Lifelines']),\n",
    "    y=alt.Y('Read Fraction (%)',title='Estimated Abundance Bacteroides+Phocaeicola')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXA_OF_INTEREST=[\n",
    "    'Bacteroides',\n",
    "    'Candida',\n",
    "    'Enterococcus',\n",
    "    'Saccharomyces',\n",
    "    'Lactobacillus',\n",
    "    'Methanosarcina',\n",
    "    'Pseudomonas',\n",
    "    'Methanobrevibacter',\n",
    "    'Blautia'\n",
    "]\n",
    "\n",
    "outer_charts = []\n",
    "\n",
    "for TAXON_OF_INTEREST in TAXA_OF_INTEREST:\n",
    "\n",
    "\n",
    "    taxon_table = table[table['taxon'] == TAXON_OF_INTEREST]\n",
    "    for sample in table['Sample'].unique():\n",
    "        #check if sample does not have the taxon\n",
    "        if sample not in taxon_table['Sample'].unique():\n",
    "            #print('Sample {} does not have the taxon, creating a dummy entry ...'.format(sample))\n",
    "            patientid,time = sample.split('/')\n",
    "            time = int(time)\n",
    "            taxon_table = pd.concat([\n",
    "                taxon_table,\n",
    "                pd.DataFrame(\n",
    "                    [(0,patientid,time,TAXON_OF_INTEREST,'???','???',sample,0)],\n",
    "                    columns=['readcount','patientid','time','taxon','taxonid','level','Sample','Read Fraction (%)']\n",
    "                )\n",
    "            ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    taxon_table = pd.merge(\n",
    "        taxon_table,\n",
    "        sample_statistics[['PatID','time','Startcluster','leukocytephase_cluster_2_kurz']],\n",
    "        how='left',\n",
    "        left_on=['patientid','time'],\n",
    "        right_on=['PatID','time']\n",
    "    ).drop(columns=['PatID'])\n",
    "    \n",
    "    taxon_table = taxon_table[\n",
    "        (taxon_table['leukocytephase_cluster_2_kurz'] == 'Leukozytopenia')|\n",
    "        (taxon_table['patientid'].str.startswith('G'))\n",
    "    ]\n",
    "\n",
    "    outcomes['Any GvHD'] = (\n",
    "        (outcomes['aGVHD Grade 1-2']==1) |\n",
    "        (outcomes['aGvHD Grade 3 - 4']==1) | \n",
    "        (outcomes['moderate cGVHD']==1) |\n",
    "        (outcomes['severe cGvHD']==1)\n",
    "    )\n",
    "\n",
    "    outcomes['Relapse'] = (\n",
    "        (outcomes['Replase_1']==1) |\n",
    "        (outcomes['Replase_2']==1)\n",
    "        )\n",
    "\n",
    "    taxon_table = pd.merge(\n",
    "        taxon_table,\n",
    "        outcomes[['Pat ID','Outcomes (non (0), adverse event (1))','Any GvHD','Death', 'Relapse']],\n",
    "        how='left',\n",
    "        left_on=['patientid'],\n",
    "        right_on=['Pat ID']\n",
    "    ).rename(columns={\n",
    "            'Outcomes (non (0), adverse event (1))' : 'Adverse Event',\n",
    "        'Any GvHD' : 'GvHD',\n",
    "        'Death' : 'Death',\n",
    "        'Relapse' : 'Relapse'\n",
    "    })\n",
    "\n",
    "    charts = []\n",
    "    \n",
    "    taxon_table.to_csv('{}_MARKER_GENERA_ANNA.csv'.format(TAXON_OF_INTEREST))\n",
    "\n",
    "    for category in ['Adverse Event','GvHD','Death', 'Relapse']:\n",
    "        \n",
    "        \n",
    "\n",
    "        yes_distrib = taxon_table[(taxon_table[category] == True)]['Read Fraction (%)']\n",
    "        no_distrib = taxon_table[(taxon_table[category] == False)]['Read Fraction (%)']\n",
    "        U1, p = mannwhitneyu(yes_distrib, no_distrib, method=\"exact\")    \n",
    "        '''\n",
    "        print('Comparing {} ({} samples) to no {} ({} samples), p-value: {:.2}'.format(\n",
    "            category,len(yes_distrib),\n",
    "                                                                                    category,\n",
    "                                                                                    len(no_distrib),\n",
    "                                                                                    p))\n",
    "        '''\n",
    "        chart = alt.Chart(taxon_table[~taxon_table['patientid'].str.startswith('G')],width=80, height=400, title='{} (p-Val: {:.2})'.format(\n",
    "            category,p\n",
    "        )).mark_boxplot(ticks=True).encode(\n",
    "            x=alt.X(category+':N',title=None),\n",
    "            y=alt.Y('Read Fraction (%)', axis=alt.Axis(format='.2f')),\n",
    "            tooltip=['Pat ID', 'time', 'Read Fraction (%)'],\n",
    "        )+alt.Chart(taxon_table[taxon_table['patientid'].str.startswith('G')]).mark_rule(color='red').encode(\n",
    "            y='mean(Read Fraction (%))'\n",
    "        )\n",
    "        charts.append(chart)\n",
    "\n",
    "    outer_charts.append(\n",
    "        reduce(lambda x,y : x|y , charts).resolve_scale(y='shared').properties(title=TAXON_OF_INTEREST)\n",
    "    )\n",
    "reduce (lambda x,y : x&y, outer_charts).configure_tick(thickness=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxon_table[taxon_table['Adverse Event']!=taxon_table['Adverse Event']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Calibration with Zymo Std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data[validation_data['Sample'].str.startswith('Zymo')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_validation = validation_data\n",
    "\n",
    "stats_validation['Validated'] = (stats_validation['Validation Rate'].astype(float) >= 0.2)\n",
    "stats_validation['Validated Reads Binary'] = stats_validation['Validated'] * stats_validation['Reads']\n",
    "stats_validation['Validated Reads Continuous'] = stats_validation['Validation Rate'] * stats_validation['Reads']\n",
    "\n",
    "stats_validation = stats_validation.groupby('Sample',as_index=False)[['Validated Reads Binary','Validated Reads Continuous','Reads']].sum()\n",
    "\n",
    "stats_validation['Validated Read Fraction Binary'] = stats_validation['Validated Reads Binary'] / stats_validation['Reads']\n",
    "stats_validation['Validated Read Fraction Continuous'] = stats_validation['Validated Reads Continuous'] / stats_validation['Reads']\n",
    "stats_validation[\n",
    "    stats_validation['Sample'].isin(\n",
    "        x.replace('/','_') for x in PAPER_SAMPLES\n",
    "    )\n",
    "                ].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVEL = 'G'\n",
    "\n",
    "#Filters\n",
    "INCLUDE = None\n",
    "EXCLUDE = [CHORDATA,PLANTAE]\n",
    "\n",
    "SAMPLES = PAPER_SAMPLES\n",
    "\n",
    "\n",
    "def SORTFUNCTION(x):\n",
    "    #return x\n",
    "    return (\n",
    "        x[0] == 'G', #Sort by G or regular patient first\n",
    "        float(x.split('G')[-1].split('/')[0]), #Then Patient ID\n",
    "        int(x.split('G')[-1].split('/')[1]), #Then Time\n",
    "    )\n",
    "\n",
    "GROUPING = 'leukocytephase_cluster_kurz'\n",
    "SORTING = 'Sample ID'\n",
    "#'leukocytephase_cluster_kurz'#'leukocytephase_cluster_kurz' # None if you don't want any Grouping, otherwise a column to group by\n",
    "\n",
    "NORMALIZE = False\n",
    "\n",
    "#################\n",
    "\n",
    "os.makedirs('Output/Composition',exist_ok=True)\n",
    "\n",
    "input_table = get_normalized_abundances(\n",
    "    kraken_dataframe,\n",
    "    level=LEVEL,\n",
    "    samples=SAMPLES,\n",
    "    included_taxa_filter=INCLUDE,\n",
    "    excluded_taxa_filter=EXCLUDE,\n",
    "    normalize=NORMALIZE\n",
    ")\n",
    "\n",
    "input_table['Read Fraction (%)'] = input_table['readcount']*100 / input_table.groupby(['time','patientid'])['readcount'].transform('sum')\n",
    "\n",
    "input_table['Sample'] = input_table['patientid']+'_'+input_table['time'].astype(str)\n",
    "\n",
    "\n",
    "DISCARD_CUTOFF = 20\n",
    "\n",
    "with_validation = pd.merge(\n",
    "    input_table,\n",
    "    validation_data[['Sample','Taxon ID','Validation Rate']],\n",
    "    left_on=['Sample','taxonid'],\n",
    "    right_on=['Sample','Taxon ID'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "total_reads = with_validation.groupby('sample',as_index=False)['readcount'].sum()\n",
    "sample_statistics['sample']= sample_statistics['PatID']+'/'+sample_statistics['time'].astype(str)\n",
    "total_reads = pd.merge(total_reads,sample_statistics[['sample','Classified Reads']],on='sample',how='left')\n",
    "total_reads['Total Fraction (%)'] = total_reads['readcount']/total_reads['Classified Reads']\n",
    "total_reads['Sample ID'] = total_reads['sample']\n",
    "\n",
    "#Phase 1: Kick out low abundance groups, assign to \"Not enough reads\"\n",
    "with_validation.loc[with_validation['readcount']<DISCARD_CUTOFF,'taxon'] = 'Not enough reads'\n",
    "\n",
    "#Phase 2: Check for rest if validates, if not assign to \"Not validated\"\n",
    "with_validation['Validated'] = (with_validation['Validation Rate'] > 0.2)\n",
    "with_validation.loc[(with_validation['Validated']!=True)&~(with_validation['taxon'] == 'Not enough reads'), 'taxonid'] = '-3'\n",
    "#Readjust sum (group multiple \"Not enough reads\" entries together)\n",
    "with_validation = with_validation.groupby(['taxonid','sample'],as_index=False).sum()\n",
    "\n",
    "with_validation['Ingroup'] = with_validation['taxonid'].apply(lambda x : is_below_or_equal(x,'1224'))\n",
    "\n",
    "sample_statistics['sample']= sample_statistics['PatID']+'/'+sample_statistics['time'].astype(str)\n",
    "table = pd.merge(\n",
    "    with_validation[with_validation['Ingroup']].groupby(['sample'],as_index=False)['Read Fraction (%)'].sum(),\n",
    "    sample_statistics[['sample','leukocytephase_cluster_kurz']],on='sample',how='left'\n",
    ")\n",
    "\n",
    "print(table.groupby('leukocytephase_cluster_kurz')['Read Fraction (%)'].median())\n",
    "alt.Chart(table).mark_boxplot().encode(\n",
    "    x='leukocytephase_cluster_kurz',\n",
    "    y='Read Fraction (%)'\n",
    ")\n",
    "\n",
    "sample_statistics.groupby('leukocytephase_cluster_2_kurz')['Total Reads'].median()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
